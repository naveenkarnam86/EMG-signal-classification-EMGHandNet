{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session =tf.compat.v1.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "# for device in gpu_devices:\n",
    "#     tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * 2 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 2)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt\n",
    "# get_ipython().magic(u'matplotlib auto')\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "# import torch\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,Activation\n",
    "from tensorflow import reshape\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.layers import Conv1D,Conv2D, MaxPooling1D,AveragePooling1D\n",
    "from tensorflow.keras.layers import SeparableConv1D, Bidirectional\n",
    "from tensorflow.keras.layers import ZeroPadding2D,ZeroPadding1D, MaxPooling2D, Bidirectional\n",
    "from tensorflow.keras.regularizers import l2,l1\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras.layers import Flatten, Activation, RepeatVector, Permute, Multiply, Lambda, Dense, merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "# import coremltools\n",
    "# from torch import nn, optim\n",
    "# import torch.nn.functional as F\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "#from IPython.display import display, HTML\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def historyVisualization(history):\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['mean_squared_error'])\n",
    "    plt.plot(history.history['mean_absolute_error'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['MSE', 'MAE'], loc='upper left')\n",
    "    plt.show()\n",
    "def plotPredict(y_pred, y_test):\n",
    "    plt.title('Predicted vs Actual')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('value')\n",
    "    plt.plot(y_pred, label='Predicted')\n",
    "    plt.plot(y_test, label='Actual ')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(validations, predictions):\n",
    "\n",
    "    matrix = metrics.confusion_matrix(validations, predictions)\n",
    "    plt.figure(figsize=(20, 14))\n",
    "    sns.heatmap(matrix,\n",
    "                cmap='coolwarm',\n",
    "                linecolor='white',\n",
    "                linewidths=1,\n",
    "                xticklabels=LABELS,\n",
    "                yticklabels=LABELS,\n",
    "                annot=True,\n",
    "                fmt='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat1=scipy.io.loadmat('/media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/nina_sEMG_preprocess_data.mat')\n",
    "# nina_raw_train=mat1['Training_process_data']\n",
    "# # # nina_raw_valid=mat1['Valid_raw_data']\n",
    "# nina_raw_test=mat1['Testing_process_data'] \n",
    "# print('raw_Train_data',nina_raw_train)\n",
    "# # # print('raw_valid_data',nina_raw_valid)\n",
    "# print('raw_Test_data',nina_raw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(\"/media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/nina_pro_Train.csv\", nina_raw_train, delimiter=\",\")\n",
    "# np.savetxt(\"/media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/nina_pro_Test.csv\", nina_raw_test, delimiter=\",\")\n",
    "# # np.savetxt(\"/content/drive/My Drive/codes/nina_prep_python/rawdata/nina_pro_Valid.csv\", nina_raw_valid, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras version  2.3.0-tf\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]\n"
     ]
    }
   ],
   "source": [
    "file_path_Train= \"/media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/nina_pro_Train.csv\"\n",
    "file_path_Test= \"/media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/nina_pro_Test.csv\"\n",
    "# file_path_Valid= \"/content/drive/My Drive/codes/nina_prep_python/rawdata/nina_pro_Valid.csv\"\n",
    "# file_path_Train= \"./nina_pro_Train_E123.csv\"\n",
    "# file_path_Test= \"./nina_pro_Test_E123.csv\"\n",
    "# Set some standard parameters upfront\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "sns.set() # Default seaborn look and feel\n",
    "#plt.style.use('ggplot')\n",
    "print('keras version ', keras.__version__)\n",
    "# Same labels will be reused throughout the program\n",
    "LABELS=[]\n",
    "for i in range(1,53,1):\n",
    "  LABELS.append (i)\n",
    "print(LABELS)\n",
    "# LABELS = ['1','2', '3', '4', '5', '6', '7', '8', '9', '10', '11','12','13',\\\n",
    "# '14','15','16','17','18','19','20','21','22','23','24','25','26','27','28',\\\n",
    "# '29','30','31','32','33','34','35','36','37','38','39','40','41','42','43',\\\n",
    "# '44','45','46','47','48','49','50','51','52']\n",
    "#LABELS = np.asarray(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of steps within one time segment\n",
    "TIME_PERIODS = 20\n",
    "# The steps to take from one segment to the next; if this value is equal to\n",
    "# TIME_PERIODS, then there is no overlap between the segments\n",
    "STEP_DISTANCE = 10\n",
    "n_features = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_Train(file_path_Train):\n",
    "    column_names = ['C1', 'C2','C3','C4','C5','C6','C7','C8','C9','C10','Class_label']\n",
    "    df_Train = pd.read_csv(file_path_Train,\n",
    "                     header=None,\n",
    "                     names=column_names)\n",
    "    # Last column has a \";\" character which must be removed ...\n",
    "    df_Train['Class_label'].replace(regex=True,\n",
    "      inplace=True,\n",
    "      to_replace=r';',\n",
    "      value=r'')\n",
    "    # ... and then this column must be transformed to float explicitly\n",
    "    df_Train['Class_label'] = df_Train['Class_label'].apply(convert_to_float)\n",
    "    # This is very important otherwise the model will not fit and loss\n",
    "    # will show up as NAN\n",
    "    df_Train.dropna(axis=0, how='any', inplace=True)\n",
    "    return df_Train\n",
    "def convert_to_float(x):\n",
    "    try:\n",
    "        return np.float(x)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_basic_dataframe_info(dataframe):\n",
    "    # Shape and how many rows and columns\n",
    "    print('Number of columns in the dataframe: %i' % (dataframe.shape[1]))\n",
    "    print('Number of rows in the dataframe: %i\\n' % (dataframe.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_Test(file_path_Test):\n",
    "    column_names = ['C1', 'C2','C3','C4','C5','C6','C7','C8','C9','C10','Class_label']\n",
    "    df_Test = pd.read_csv(file_path_Test,\n",
    "                     header=None,\n",
    "                     names=column_names)\n",
    "    # Last column has a \";\" character which must be removed ...\n",
    "    df_Test['Class_label'].replace(regex=True,\n",
    "      inplace=True,\n",
    "      to_replace=r';',\n",
    "      value=r'')\n",
    "    # ... and then this column must be transformed to float explicitly\n",
    "    df_Test['Class_label'] = df_Test['Class_label'].apply(convert_to_float)\n",
    "    # This is very important otherwise the model will not fit and loss\n",
    "    # will show up as NAN\n",
    "    df_Test.dropna(axis=0, how='any', inplace=True)\n",
    "    return df_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_float(x):\n",
    "    try:\n",
    "        return np.float(x)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_basic_dataframe_info(dataframe):\n",
    "    # Shape and how many rows and columns\n",
    "    print('Number of columns in the dataframe: %i' % (dataframe.shape[1]))\n",
    "    print('Number of rows in the dataframe: %i\\n' % (dataframe.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 5031936\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>Class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    C1  C2  C3  C4  C5  C6  C7  C8  C9  C10  Class_label\n",
       "0  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "1  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "2  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "3  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "4  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "5  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "6  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "7  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "8  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "9  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "10 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "11 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "12 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "13 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "14 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "15 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "16 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "17 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "18 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "19 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Train = read_data_Train('/media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/nina_pro_Train.csv')\n",
    "# Describe the data\n",
    "show_basic_dataframe_info(df_Train)\n",
    "df_Train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_Train['Class_label'].value_counts().plot(kind='bar',\n",
    "                                   title='Training Samples count for each class')\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 20\n",
    "fig_size[1] = 10\n",
    "#plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "plt.ylabel('Total Samples')\n",
    "plt.xlabel('classes')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x',tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 2156544\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>Class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    C1  C2  C3  C4  C5  C6  C7  C8  C9  C10  Class_label\n",
       "0  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "1  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "2  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "3  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "4  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "5  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "6  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "7  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "8  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "9  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "10 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "11 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "12 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "13 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "14 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "15 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "16 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "17 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "18 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0\n",
       "19 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0  0.0          1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Test = read_data_Test('/media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/nina_pro_Test.csv')\n",
    "# Describe the data\n",
    "show_basic_dataframe_info(df_Test)\n",
    "df_Test.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_Test['Class_label'].value_counts().plot(kind='bar',\n",
    "                                   title='Testing Samples count for each class')\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 20\n",
    "fig_size[1] = 10\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "plt.ylabel('Total Samples')\n",
    "plt.xlabel('classes')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x',tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>Class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00720</td>\n",
       "      <td>0.00242</td>\n",
       "      <td>0.00493</td>\n",
       "      <td>0.00242</td>\n",
       "      <td>0.00245</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>0.01634</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.00247</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00836</td>\n",
       "      <td>0.00243</td>\n",
       "      <td>0.00478</td>\n",
       "      <td>0.00242</td>\n",
       "      <td>0.00245</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.00312</td>\n",
       "      <td>0.01699</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.00247</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00949</td>\n",
       "      <td>0.00243</td>\n",
       "      <td>0.00464</td>\n",
       "      <td>0.00242</td>\n",
       "      <td>0.00246</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.00317</td>\n",
       "      <td>0.01762</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.00248</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01052</td>\n",
       "      <td>0.00243</td>\n",
       "      <td>0.00451</td>\n",
       "      <td>0.00242</td>\n",
       "      <td>0.00246</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.00322</td>\n",
       "      <td>0.01822</td>\n",
       "      <td>0.00242</td>\n",
       "      <td>0.00248</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01142</td>\n",
       "      <td>0.00243</td>\n",
       "      <td>0.00439</td>\n",
       "      <td>0.00242</td>\n",
       "      <td>0.00246</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>0.00327</td>\n",
       "      <td>0.01877</td>\n",
       "      <td>0.00242</td>\n",
       "      <td>0.00248</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       C1      C2      C3      C4      C5      C6      C7      C8      C9  \\\n",
       "0 0.00720 0.00242 0.00493 0.00242 0.00245 0.00241 0.00308 0.01634 0.00241   \n",
       "1 0.00836 0.00243 0.00478 0.00242 0.00245 0.00241 0.00312 0.01699 0.00241   \n",
       "2 0.00949 0.00243 0.00464 0.00242 0.00246 0.00241 0.00317 0.01762 0.00241   \n",
       "3 0.01052 0.00243 0.00451 0.00242 0.00246 0.00241 0.00322 0.01822 0.00242   \n",
       "4 0.01142 0.00243 0.00439 0.00242 0.00246 0.00241 0.00327 0.01877 0.00242   \n",
       "\n",
       "      C10  Class_label  \n",
       "0 0.00247      1.00000  \n",
       "1 0.00247      1.00000  \n",
       "2 0.00248      1.00000  \n",
       "3 0.00248      1.00000  \n",
       "4 0.00248      1.00000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = \"{:,.5f}\".format\n",
    "df_Train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(df_Train.iloc[:,0:n_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train.iloc[:,0:n_features]=scaler.transform(df_Train.iloc[:,0:n_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>Class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.67229</td>\n",
       "      <td>-0.63485</td>\n",
       "      <td>-0.51161</td>\n",
       "      <td>-0.43213</td>\n",
       "      <td>-0.36882</td>\n",
       "      <td>-0.45281</td>\n",
       "      <td>-0.75398</td>\n",
       "      <td>-0.90170</td>\n",
       "      <td>-0.51352</td>\n",
       "      <td>-0.77192</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.66990</td>\n",
       "      <td>-0.63485</td>\n",
       "      <td>-0.51198</td>\n",
       "      <td>-0.43213</td>\n",
       "      <td>-0.36879</td>\n",
       "      <td>-0.45280</td>\n",
       "      <td>-0.75392</td>\n",
       "      <td>-0.90043</td>\n",
       "      <td>-0.51352</td>\n",
       "      <td>-0.77191</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.66759</td>\n",
       "      <td>-0.63485</td>\n",
       "      <td>-0.51234</td>\n",
       "      <td>-0.43212</td>\n",
       "      <td>-0.36875</td>\n",
       "      <td>-0.45280</td>\n",
       "      <td>-0.75384</td>\n",
       "      <td>-0.89918</td>\n",
       "      <td>-0.51352</td>\n",
       "      <td>-0.77190</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.66546</td>\n",
       "      <td>-0.63484</td>\n",
       "      <td>-0.51268</td>\n",
       "      <td>-0.43212</td>\n",
       "      <td>-0.36871</td>\n",
       "      <td>-0.45280</td>\n",
       "      <td>-0.75376</td>\n",
       "      <td>-0.89799</td>\n",
       "      <td>-0.51352</td>\n",
       "      <td>-0.77189</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.66362</td>\n",
       "      <td>-0.63483</td>\n",
       "      <td>-0.51299</td>\n",
       "      <td>-0.43211</td>\n",
       "      <td>-0.36867</td>\n",
       "      <td>-0.45279</td>\n",
       "      <td>-0.75369</td>\n",
       "      <td>-0.89691</td>\n",
       "      <td>-0.51351</td>\n",
       "      <td>-0.77188</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        C1       C2       C3       C4       C5       C6       C7       C8  \\\n",
       "0 -0.67229 -0.63485 -0.51161 -0.43213 -0.36882 -0.45281 -0.75398 -0.90170   \n",
       "1 -0.66990 -0.63485 -0.51198 -0.43213 -0.36879 -0.45280 -0.75392 -0.90043   \n",
       "2 -0.66759 -0.63485 -0.51234 -0.43212 -0.36875 -0.45280 -0.75384 -0.89918   \n",
       "3 -0.66546 -0.63484 -0.51268 -0.43212 -0.36871 -0.45280 -0.75376 -0.89799   \n",
       "4 -0.66362 -0.63483 -0.51299 -0.43211 -0.36867 -0.45279 -0.75369 -0.89691   \n",
       "\n",
       "        C9      C10  Class_label  \n",
       "0 -0.51352 -0.77192      1.00000  \n",
       "1 -0.51352 -0.77191      1.00000  \n",
       "2 -0.51352 -0.77190      1.00000  \n",
       "3 -0.51352 -0.77189      1.00000  \n",
       "4 -0.51351 -0.77188      1.00000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = \"{:,.5f}\".format\n",
    "df_Train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>Class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.03176</td>\n",
       "      <td>0.00247</td>\n",
       "      <td>0.00244</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.00244</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.00258</td>\n",
       "      <td>0.03562</td>\n",
       "      <td>0.00244</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.03205</td>\n",
       "      <td>0.00248</td>\n",
       "      <td>0.00244</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.00245</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.00258</td>\n",
       "      <td>0.03567</td>\n",
       "      <td>0.00244</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03234</td>\n",
       "      <td>0.00248</td>\n",
       "      <td>0.00245</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.00246</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.00259</td>\n",
       "      <td>0.03569</td>\n",
       "      <td>0.00244</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03261</td>\n",
       "      <td>0.00249</td>\n",
       "      <td>0.00246</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.00246</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.00260</td>\n",
       "      <td>0.03570</td>\n",
       "      <td>0.00244</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.03285</td>\n",
       "      <td>0.00249</td>\n",
       "      <td>0.00246</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.00246</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>0.00261</td>\n",
       "      <td>0.03566</td>\n",
       "      <td>0.00245</td>\n",
       "      <td>0.00241</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       C1      C2      C3      C4      C5      C6      C7      C8      C9  \\\n",
       "0 0.03176 0.00247 0.00244 0.00240 0.00244 0.00240 0.00258 0.03562 0.00244   \n",
       "1 0.03205 0.00248 0.00244 0.00240 0.00245 0.00240 0.00258 0.03567 0.00244   \n",
       "2 0.03234 0.00248 0.00245 0.00240 0.00246 0.00240 0.00259 0.03569 0.00244   \n",
       "3 0.03261 0.00249 0.00246 0.00240 0.00246 0.00240 0.00260 0.03570 0.00244   \n",
       "4 0.03285 0.00249 0.00246 0.00240 0.00246 0.00240 0.00261 0.03566 0.00245   \n",
       "\n",
       "      C10  Class_label  \n",
       "0 0.00241      1.00000  \n",
       "1 0.00241      1.00000  \n",
       "2 0.00241      1.00000  \n",
       "3 0.00241      1.00000  \n",
       "4 0.00241      1.00000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = \"{:,.5f}\".format\n",
    "df_Test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test.iloc[:,0:n_features]=scaler.transform(df_Test.iloc[:,0:n_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "      <th>C9</th>\n",
       "      <th>C10</th>\n",
       "      <th>Class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.62181</td>\n",
       "      <td>-0.63470</td>\n",
       "      <td>-0.51795</td>\n",
       "      <td>-0.43219</td>\n",
       "      <td>-0.36893</td>\n",
       "      <td>-0.45286</td>\n",
       "      <td>-0.75480</td>\n",
       "      <td>-0.86372</td>\n",
       "      <td>-0.51346</td>\n",
       "      <td>-0.77206</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.62121</td>\n",
       "      <td>-0.63469</td>\n",
       "      <td>-0.51792</td>\n",
       "      <td>-0.43219</td>\n",
       "      <td>-0.36881</td>\n",
       "      <td>-0.45286</td>\n",
       "      <td>-0.75479</td>\n",
       "      <td>-0.86364</td>\n",
       "      <td>-0.51345</td>\n",
       "      <td>-0.77206</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.62061</td>\n",
       "      <td>-0.63468</td>\n",
       "      <td>-0.51790</td>\n",
       "      <td>-0.43219</td>\n",
       "      <td>-0.36872</td>\n",
       "      <td>-0.45286</td>\n",
       "      <td>-0.75477</td>\n",
       "      <td>-0.86358</td>\n",
       "      <td>-0.51345</td>\n",
       "      <td>-0.77206</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.62006</td>\n",
       "      <td>-0.63467</td>\n",
       "      <td>-0.51789</td>\n",
       "      <td>-0.43219</td>\n",
       "      <td>-0.36867</td>\n",
       "      <td>-0.45286</td>\n",
       "      <td>-0.75476</td>\n",
       "      <td>-0.86357</td>\n",
       "      <td>-0.51344</td>\n",
       "      <td>-0.77205</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.61957</td>\n",
       "      <td>-0.63465</td>\n",
       "      <td>-0.51789</td>\n",
       "      <td>-0.43219</td>\n",
       "      <td>-0.36865</td>\n",
       "      <td>-0.45286</td>\n",
       "      <td>-0.75474</td>\n",
       "      <td>-0.86364</td>\n",
       "      <td>-0.51344</td>\n",
       "      <td>-0.77205</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        C1       C2       C3       C4       C5       C6       C7       C8  \\\n",
       "0 -0.62181 -0.63470 -0.51795 -0.43219 -0.36893 -0.45286 -0.75480 -0.86372   \n",
       "1 -0.62121 -0.63469 -0.51792 -0.43219 -0.36881 -0.45286 -0.75479 -0.86364   \n",
       "2 -0.62061 -0.63468 -0.51790 -0.43219 -0.36872 -0.45286 -0.75477 -0.86358   \n",
       "3 -0.62006 -0.63467 -0.51789 -0.43219 -0.36867 -0.45286 -0.75476 -0.86357   \n",
       "4 -0.61957 -0.63465 -0.51789 -0.43219 -0.36865 -0.45286 -0.75474 -0.86364   \n",
       "\n",
       "        C9      C10  Class_label  \n",
       "0 -0.51346 -0.77206      1.00000  \n",
       "1 -0.51345 -0.77206      1.00000  \n",
       "2 -0.51345 -0.77206      1.00000  \n",
       "3 -0.51344 -0.77205      1.00000  \n",
       "4 -0.51344 -0.77205      1.00000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = \"{:,.5f}\".format\n",
    "df_Test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_size               C1       C2       C3       C4       C5       C6       C7  \\\n",
      "0       -0.67229 -0.63485 -0.51161 -0.43213 -0.36882 -0.45281 -0.75398   \n",
      "1       -0.66990 -0.63485 -0.51198 -0.43213 -0.36879 -0.45280 -0.75392   \n",
      "2       -0.66759 -0.63485 -0.51234 -0.43212 -0.36875 -0.45280 -0.75384   \n",
      "3       -0.66546 -0.63484 -0.51268 -0.43212 -0.36871 -0.45280 -0.75376   \n",
      "4       -0.66362 -0.63483 -0.51299 -0.43211 -0.36867 -0.45279 -0.75369   \n",
      "...          ...      ...      ...      ...      ...      ...      ...   \n",
      "5031931 -0.67906 -0.63424 -0.51714 -0.43021 -0.35403  0.13654  0.12757   \n",
      "5031932 -0.67903 -0.63426 -0.51716 -0.43025 -0.35431  0.13208  0.11671   \n",
      "5031933 -0.67898 -0.63427 -0.51718 -0.43028 -0.35465  0.12755  0.10702   \n",
      "5031934 -0.67894 -0.63427 -0.51719 -0.43031 -0.35495  0.12339  0.09885   \n",
      "5031935 -0.67890 -0.63428 -0.51720 -0.43032 -0.35514  0.11980  0.09225   \n",
      "\n",
      "              C8       C9      C10  Class_label  ActivityEncoded  \n",
      "0       -0.90170 -0.51352 -0.77192      1.00000                0  \n",
      "1       -0.90043 -0.51352 -0.77191      1.00000                0  \n",
      "2       -0.89918 -0.51352 -0.77190      1.00000                0  \n",
      "3       -0.89799 -0.51352 -0.77189      1.00000                0  \n",
      "4       -0.89691 -0.51351 -0.77188      1.00000                0  \n",
      "...          ...      ...      ...          ...              ...  \n",
      "5031931 -0.14295 -0.51057 -0.63128     52.00000               51  \n",
      "5031932 -0.14375 -0.51064 -0.63332     52.00000               51  \n",
      "5031933 -0.14539 -0.51070 -0.63524     52.00000               51  \n",
      "5031934 -0.14709 -0.51074 -0.63689     52.00000               51  \n",
      "5031935 -0.14877 -0.51076 -0.63823     52.00000               51  \n",
      "\n",
      "[5031936 rows x 12 columns]\n",
      "df_test_size               C1       C2       C3       C4       C5       C6       C7  \\\n",
      "0       -0.62181 -0.63470 -0.51795 -0.43219 -0.36893 -0.45286 -0.75480   \n",
      "1       -0.62121 -0.63469 -0.51792 -0.43219 -0.36881 -0.45286 -0.75479   \n",
      "2       -0.62061 -0.63468 -0.51790 -0.43219 -0.36872 -0.45286 -0.75477   \n",
      "3       -0.62006 -0.63467 -0.51789 -0.43219 -0.36867 -0.45286 -0.75476   \n",
      "4       -0.61957 -0.63465 -0.51789 -0.43219 -0.36865 -0.45286 -0.75474   \n",
      "...          ...      ...      ...      ...      ...      ...      ...   \n",
      "2156539 -0.68193 -0.63448 -0.51749 -0.43057 -0.35825  0.23042 -0.44465   \n",
      "2156540 -0.68193 -0.63449 -0.51751 -0.43061 -0.35853  0.22799 -0.44471   \n",
      "2156541 -0.68194 -0.63450 -0.51752 -0.43064 -0.35875  0.22538 -0.44467   \n",
      "2156542 -0.68194 -0.63450 -0.51753 -0.43066 -0.35891  0.22290 -0.44460   \n",
      "2156543 -0.68194 -0.63451 -0.51754 -0.43067 -0.35900  0.22067 -0.44457   \n",
      "\n",
      "              C8       C9      C10  Class_label  ActivityEncoded  \n",
      "0       -0.86372 -0.51346 -0.77206      1.00000                0  \n",
      "1       -0.86364 -0.51345 -0.77206      1.00000                0  \n",
      "2       -0.86358 -0.51345 -0.77206      1.00000                0  \n",
      "3       -0.86357 -0.51344 -0.77205      1.00000                0  \n",
      "4       -0.86364 -0.51344 -0.77205      1.00000                0  \n",
      "...          ...      ...      ...          ...              ...  \n",
      "2156539 -0.54290 -0.51309 -0.53333     52.00000               51  \n",
      "2156540 -0.54278 -0.51310 -0.53488     52.00000               51  \n",
      "2156541 -0.54273 -0.51311 -0.53638     52.00000               51  \n",
      "2156542 -0.54273 -0.51311 -0.53768     52.00000               51  \n",
      "2156543 -0.54273 -0.51311 -0.53877     52.00000               51  \n",
      "\n",
      "[2156544 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "LABEL = 'ActivityEncoded'\n",
    "# Transform the labels from String to Integer via LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Add a new column to the existing DataFrame with the encoded values\n",
    "df_Train[LABEL] = le.fit_transform(df_Train['Class_label'].values.ravel())\n",
    "# df_Valid[LABEL] = le.fit_transform(df_Valid['Class_label'].values.ravel())\n",
    "df_Test[LABEL] = le.fit_transform(df_Test['Class_label'].values.ravel())\n",
    "print('df_train_size',df_Train)\n",
    "# print('df_valid_size',df_Valid)\n",
    "print('df_test_size',df_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segments_and_labels(df, time_steps,step, label_name):\n",
    "    # x, y, z acceleration as features\n",
    "    # N_FEATURES = 10\n",
    "    # Number of steps to advance in each iteration (for me, it should always\n",
    "    # be equal to the time_steps in order to have no overlap between segments)\n",
    "    # step = time_steps\n",
    "\t#step = 200\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(0, len(df) - time_steps, step):\n",
    "        c1s = df['C1'].values[i: i + time_steps]\n",
    "        c2s = df['C2'].values[i: i + time_steps]\n",
    "        c3s = df['C3'].values[i: i + time_steps]\n",
    "        c4s = df['C4'].values[i: i + time_steps]\n",
    "        c5s = df['C5'].values[i: i + time_steps]\n",
    "        c6s = df['C6'].values[i: i + time_steps]\n",
    "        c7s = df['C7'].values[i: i + time_steps]\n",
    "        c8s = df['C8'].values[i: i + time_steps]\n",
    "        c9s = df['C9'].values[i: i + time_steps]\n",
    "        c10s = df['C10'].values[i: i + time_steps]\n",
    "        # Retrieve the most often used label in this segment\n",
    "      #  label_name - label_name.astype(int)\n",
    "        label = stats.mode(df[label_name][i: i + time_steps])[0][0]\n",
    "        segments.append([c1s, c2s, c3s,c4s, c5s, c6s,c7s, c8s, c9s,c10s])\n",
    "        # segments = pd.concat([c1s, c2s, c3s,c4s, c5s, c6s,c7s, c8s, c9s, c10s], axis=1)\n",
    "        labels.append(label)\n",
    "    # Bring the segments into a better shape\n",
    "    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, time_steps, n_features)\n",
    "    labels = np.asarray(labels)\n",
    "    return reshaped_segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (503192, 20, 10)\n",
      "503192 training samples\n",
      "y_train shape:  (503192,)\n",
      "20\n",
      "10\n",
      "[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (503192, 52)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = create_segments_and_labels(df_Train,TIME_PERIODS,STEP_DISTANCE,LABEL)\n",
    "print('x_train shape: ', x_train.shape)\n",
    "# print(x_train)\n",
    "print(x_train.shape[0], 'training samples')\n",
    "print('y_train shape: ', y_train.shape)\n",
    "# Set input & output dimensions\n",
    "num_time_periods, num_sensors = x_train.shape[1], x_train.shape[2]\n",
    "print(num_time_periods)\n",
    "print(num_sensors)\n",
    "num_classes = le.classes_.size\n",
    "print(list(le.classes_))\n",
    "# input_shape = (num_time_periods,num_sensors)\n",
    "# print(input_shape)\n",
    "input_shape = (num_time_periods,num_sensors)\n",
    "#x_train = x_train.reshape(x_train.shape[0], input_shape)\n",
    "print('x_train shape:', x_train[0].shape)\n",
    "print('input_shape:', input_shape)\n",
    "x_train = x_train.astype('float32')\n",
    "# x_train = [torch.tensor(arr, dtype=torch.float32) for arr in x_train]\n",
    "# y_train = y_train.astype('float32')\n",
    "# print(y_train)\n",
    "y_train_hot = np_utils.to_categorical(y_train, num_classes)\n",
    "print(y_train_hot)\n",
    "# y_train_hot= [torch.tensor(arr, dtype=torch.uint8) for arr in y_train_hot]\n",
    "print('New y_train shape: ', y_train_hot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (215653, 20, 10)\n",
      "215653 testing samples\n",
      "y_test shape:  (215653,)\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = create_segments_and_labels(df_Test,\n",
    "                                            TIME_PERIODS,\n",
    "                                            STEP_DISTANCE,\n",
    "                                            LABEL)\n",
    "print('x_test shape: ', x_test.shape)\n",
    "# print(x_train)\n",
    "print(x_test.shape[0], 'testing samples')\n",
    "print('y_test shape: ', y_test.shape)\n",
    "# Set input_shape / reshape for Keras\n",
    "#x_test = x_test.reshape(x_test.shape[0], input_shape)\n",
    "x_test = x_test.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "y_test_hot = np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (503192, 20, 10, 1)\n",
      "x_test shape:  (215653, 20, 10, 1)\n",
      "n_outputs 52\n"
     ]
    }
   ],
   "source": [
    "n_steps, n_length = 20, 10\n",
    "n_depth=1\n",
    "x_train = x_train.reshape(x_train.shape[0], n_steps, n_length,n_depth)\n",
    "print('x_train shape: ', x_train.shape)\n",
    "# x_valid = x_valid.reshape(x_valid.shape[0], n_steps, n_length, n_depth)\n",
    "# print('x_valid shape: ', x_valid.shape)\n",
    "x_test = x_test.reshape(x_test.shape[0], n_steps, n_length,n_depth)\n",
    "print('x_test shape: ', x_test.shape)\n",
    "n_outputs = y_train_hot.shape[1]\n",
    "print('n_outputs',n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose, epochs, batch_size = 0, 500, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "  inputs = keras.Input(shape=(n_steps,n_length,n_depth))\n",
    "  x = Conv2D(filters=64, kernel_size=(7,1), padding='same', kernel_initializer=\"he_normal\",strides=(2,1),kernel_regularizer=l1(1e-04))(inputs)\n",
    "  x = MaxPooling2D(pool_size=(8,1),strides=(2,1))(x)\n",
    "  x = Activation('tanh')(x)\n",
    "  x = Conv2D(filters=64, kernel_size=(5,1),padding=\"same\",kernel_initializer=\"he_normal\",strides=(2,1),kernel_regularizer=l1(1e-04))(x)\n",
    "  x = Activation('tanh')(x)\n",
    "  x = Dropout(0.2093)(x)\n",
    "  x = Activation('relu')(x)\n",
    "  x = Flatten()(x)\n",
    "  x = Dense(512, activation='tanh')(x)\n",
    "  x = BatchNormalization(epsilon=1e-05, momentum=0.9, weights=None)(x)\n",
    "  outputs = Dense(n_outputs, activation='softmax')(x)\n",
    "  CNN_olsson_model = keras.Model(inputs, outputs)\n",
    "  CNN_olsson_model.summary()\n",
    "  return CNN_olsson_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 20, 10, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 10, 10, 64)        512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 2, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 10, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               328192    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 52)                26676     \n",
      "=================================================================\n",
      "Total params: 377,972\n",
      "Trainable params: 376,948\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 23 model summary \n",
    "if __name__ == \"__main__\":\n",
    "    model = generate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 20, 10, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 10, 10, 64)        512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 2, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 1, 10, 64)         20544     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               328192    \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 52)                26676     \n",
      "=================================================================\n",
      "Total params: 377,972\n",
      "Trainable params: 376,948\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "adam=optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "checkpoint_filepath = '/media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5'\n",
    "# model.load_weights(checkpoint_filepath) \n",
    "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath,verbose=1, monitor='val_accuracy',save_weights_only=True,save_best_only=True)\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# model.build(inputs)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, to_file='/media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/Model1.png',show_shapes=True,show_layer_names=True,dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "125794/125798 [============================>.] - ETA: 0s - loss: 3.7939 - accuracy: 0.0814\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.13227, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 888s 7ms/step - loss: 3.7939 - accuracy: 0.0814 - val_loss: 3.5011 - val_accuracy: 0.1323\n",
      "Epoch 2/500\n",
      "125798/125798 [==============================] - ETA: 0s - loss: 3.5520 - accuracy: 0.1269\n",
      "Epoch 00002: val_accuracy improved from 0.13227 to 0.15910, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 867s 7ms/step - loss: 3.5520 - accuracy: 0.1269 - val_loss: 3.3397 - val_accuracy: 0.1591\n",
      "Epoch 3/500\n",
      "125796/125798 [============================>.] - ETA: 0s - loss: 3.4286 - accuracy: 0.1555\n",
      "Epoch 00003: val_accuracy improved from 0.15910 to 0.18818, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 931s 7ms/step - loss: 3.4286 - accuracy: 0.1555 - val_loss: 3.2102 - val_accuracy: 0.1882\n",
      "Epoch 4/500\n",
      "125796/125798 [============================>.] - ETA: 0s - loss: 3.3393 - accuracy: 0.1781\n",
      "Epoch 00004: val_accuracy improved from 0.18818 to 0.21513, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 889s 7ms/step - loss: 3.3393 - accuracy: 0.1781 - val_loss: 3.1063 - val_accuracy: 0.2151\n",
      "Epoch 5/500\n",
      "125788/125798 [============================>.] - ETA: 0s - loss: 3.2722 - accuracy: 0.1958\n",
      "Epoch 00005: val_accuracy improved from 0.21513 to 0.23254, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 871s 7ms/step - loss: 3.2722 - accuracy: 0.1958 - val_loss: 3.0493 - val_accuracy: 0.2325\n",
      "Epoch 6/500\n",
      "125789/125798 [============================>.] - ETA: 0s - loss: 3.2172 - accuracy: 0.2117\n",
      "Epoch 00006: val_accuracy improved from 0.23254 to 0.24574, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 950s 8ms/step - loss: 3.2172 - accuracy: 0.2117 - val_loss: 2.9664 - val_accuracy: 0.2457\n",
      "Epoch 7/500\n",
      "125789/125798 [============================>.] - ETA: 0s - loss: 3.1736 - accuracy: 0.2233\n",
      "Epoch 00007: val_accuracy improved from 0.24574 to 0.26061, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 921s 7ms/step - loss: 3.1736 - accuracy: 0.2233 - val_loss: 2.9263 - val_accuracy: 0.2606\n",
      "Epoch 8/500\n",
      "125792/125798 [============================>.] - ETA: 0s - loss: 3.1401 - accuracy: 0.2332\n",
      "Epoch 00008: val_accuracy improved from 0.26061 to 0.26525, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 928s 7ms/step - loss: 3.1401 - accuracy: 0.2332 - val_loss: 2.9036 - val_accuracy: 0.2653\n",
      "Epoch 9/500\n",
      "125793/125798 [============================>.] - ETA: 0s - loss: 3.1117 - accuracy: 0.2417\n",
      "Epoch 00009: val_accuracy improved from 0.26525 to 0.27464, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 707s 6ms/step - loss: 3.1117 - accuracy: 0.2417 - val_loss: 2.8642 - val_accuracy: 0.2746\n",
      "Epoch 10/500\n",
      "125790/125798 [============================>.] - ETA: 0s - loss: 3.0862 - accuracy: 0.2484\n",
      "Epoch 00010: val_accuracy improved from 0.27464 to 0.28520, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 670s 5ms/step - loss: 3.0861 - accuracy: 0.2484 - val_loss: 2.8637 - val_accuracy: 0.2852\n",
      "Epoch 11/500\n",
      "125793/125798 [============================>.] - ETA: 0s - loss: 3.0664 - accuracy: 0.2543\n",
      "Epoch 00011: val_accuracy improved from 0.28520 to 0.28525, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 665s 5ms/step - loss: 3.0665 - accuracy: 0.2543 - val_loss: 2.8897 - val_accuracy: 0.2852\n",
      "Epoch 12/500\n",
      "125796/125798 [============================>.] - ETA: 0s - loss: 3.0476 - accuracy: 0.2586\n",
      "Epoch 00012: val_accuracy improved from 0.28525 to 0.29109, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 663s 5ms/step - loss: 3.0476 - accuracy: 0.2586 - val_loss: 2.8566 - val_accuracy: 0.2911\n",
      "Epoch 13/500\n",
      "125786/125798 [============================>.] - ETA: 0s - loss: 3.0309 - accuracy: 0.2630\n",
      "Epoch 00013: val_accuracy improved from 0.29109 to 0.29647, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 662s 5ms/step - loss: 3.0309 - accuracy: 0.2630 - val_loss: 2.8221 - val_accuracy: 0.2965\n",
      "Epoch 14/500\n",
      "125790/125798 [============================>.] - ETA: 0s - loss: 3.0159 - accuracy: 0.2666\n",
      "Epoch 00014: val_accuracy improved from 0.29647 to 0.30073, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 665s 5ms/step - loss: 3.0159 - accuracy: 0.2666 - val_loss: 2.8009 - val_accuracy: 0.3007\n",
      "Epoch 15/500\n",
      "125790/125798 [============================>.] - ETA: 0s - loss: 3.0027 - accuracy: 0.2706\n",
      "Epoch 00015: val_accuracy improved from 0.30073 to 0.30582, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 667s 5ms/step - loss: 3.0027 - accuracy: 0.2706 - val_loss: 2.7854 - val_accuracy: 0.3058\n",
      "Epoch 16/500\n",
      "125790/125798 [============================>.] - ETA: 0s - loss: 2.9919 - accuracy: 0.2743\n",
      "Epoch 00016: val_accuracy improved from 0.30582 to 0.30585, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 667s 5ms/step - loss: 2.9919 - accuracy: 0.2743 - val_loss: 2.7832 - val_accuracy: 0.3059\n",
      "Epoch 17/500\n",
      "125796/125798 [============================>.] - ETA: 0s - loss: 2.9829 - accuracy: 0.2763\n",
      "Epoch 00017: val_accuracy improved from 0.30585 to 0.30845, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 667s 5ms/step - loss: 2.9829 - accuracy: 0.2763 - val_loss: 2.7951 - val_accuracy: 0.3085\n",
      "Epoch 18/500\n",
      "125793/125798 [============================>.] - ETA: 0s - loss: 2.9704 - accuracy: 0.2790\n",
      "Epoch 00018: val_accuracy improved from 0.30845 to 0.31041, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 665s 5ms/step - loss: 2.9704 - accuracy: 0.2790 - val_loss: 2.8182 - val_accuracy: 0.3104\n",
      "Epoch 19/500\n",
      "125798/125798 [==============================] - ETA: 0s - loss: 2.9623 - accuracy: 0.2818\n",
      "Epoch 00019: val_accuracy did not improve from 0.31041\n",
      "125798/125798 [==============================] - 681s 5ms/step - loss: 2.9623 - accuracy: 0.2818 - val_loss: 2.9162 - val_accuracy: 0.3072\n",
      "Epoch 20/500\n",
      "125795/125798 [============================>.] - ETA: 0s - loss: 2.9540 - accuracy: 0.2839\n",
      "Epoch 00020: val_accuracy improved from 0.31041 to 0.31109, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 681s 5ms/step - loss: 2.9541 - accuracy: 0.2839 - val_loss: 2.7787 - val_accuracy: 0.3111\n",
      "Epoch 21/500\n",
      "125798/125798 [==============================] - ETA: 0s - loss: 2.9454 - accuracy: 0.2859\n",
      "Epoch 00021: val_accuracy improved from 0.31109 to 0.31560, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 683s 5ms/step - loss: 2.9454 - accuracy: 0.2859 - val_loss: 2.8150 - val_accuracy: 0.3156\n",
      "Epoch 22/500\n",
      "125793/125798 [============================>.] - ETA: 0s - loss: 2.9430 - accuracy: 0.2874\n",
      "Epoch 00022: val_accuracy improved from 0.31560 to 0.31754, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 676s 5ms/step - loss: 2.9430 - accuracy: 0.2874 - val_loss: 2.7666 - val_accuracy: 0.3175\n",
      "Epoch 23/500\n",
      "125787/125798 [============================>.] - ETA: 0s - loss: 2.9362 - accuracy: 0.2883\n",
      "Epoch 00023: val_accuracy did not improve from 0.31754\n",
      "125798/125798 [==============================] - 677s 5ms/step - loss: 2.9361 - accuracy: 0.2883 - val_loss: 2.7798 - val_accuracy: 0.3157\n",
      "Epoch 24/500\n",
      "125786/125798 [============================>.] - ETA: 0s - loss: 2.9270 - accuracy: 0.2910\n",
      "Epoch 00024: val_accuracy improved from 0.31754 to 0.31779, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 680s 5ms/step - loss: 2.9270 - accuracy: 0.2910 - val_loss: 2.8635 - val_accuracy: 0.3178\n",
      "Epoch 25/500\n",
      "125788/125798 [============================>.] - ETA: 0s - loss: 2.9207 - accuracy: 0.2918\n",
      "Epoch 00025: val_accuracy improved from 0.31779 to 0.32095, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 681s 5ms/step - loss: 2.9207 - accuracy: 0.2918 - val_loss: 2.7742 - val_accuracy: 0.3209\n",
      "Epoch 26/500\n",
      "125788/125798 [============================>.] - ETA: 0s - loss: 2.9168 - accuracy: 0.2939\n",
      "Epoch 00026: val_accuracy did not improve from 0.32095\n",
      "125798/125798 [==============================] - 679s 5ms/step - loss: 2.9168 - accuracy: 0.2939 - val_loss: 2.9729 - val_accuracy: 0.3159\n",
      "Epoch 27/500\n",
      "125791/125798 [============================>.] - ETA: 0s - loss: 2.9118 - accuracy: 0.2942\n",
      "Epoch 00027: val_accuracy did not improve from 0.32095\n",
      "125798/125798 [==============================] - 683s 5ms/step - loss: 2.9118 - accuracy: 0.2942 - val_loss: 2.8149 - val_accuracy: 0.3193\n",
      "Epoch 28/500\n",
      "125796/125798 [============================>.] - ETA: 0s - loss: 2.9050 - accuracy: 0.2959\n",
      "Epoch 00028: val_accuracy did not improve from 0.32095\n",
      "125798/125798 [==============================] - 680s 5ms/step - loss: 2.9050 - accuracy: 0.2959 - val_loss: 3.0075 - val_accuracy: 0.3189\n",
      "Epoch 29/500\n",
      "125793/125798 [============================>.] - ETA: 0s - loss: 2.9038 - accuracy: 0.2978\n",
      "Epoch 00029: val_accuracy improved from 0.32095 to 0.32513, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 682s 5ms/step - loss: 2.9038 - accuracy: 0.2978 - val_loss: 2.7456 - val_accuracy: 0.3251\n",
      "Epoch 30/500\n",
      "125797/125798 [============================>.] - ETA: 0s - loss: 2.8990 - accuracy: 0.2988\n",
      "Epoch 00030: val_accuracy did not improve from 0.32513\n",
      "125798/125798 [==============================] - 683s 5ms/step - loss: 2.8990 - accuracy: 0.2988 - val_loss: 2.9232 - val_accuracy: 0.3223\n",
      "Epoch 31/500\n",
      "125785/125798 [============================>.] - ETA: 0s - loss: 2.8931 - accuracy: 0.3006\n",
      "Epoch 00031: val_accuracy improved from 0.32513 to 0.32678, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 680s 5ms/step - loss: 2.8931 - accuracy: 0.3006 - val_loss: 2.7334 - val_accuracy: 0.3268\n",
      "Epoch 32/500\n",
      "125793/125798 [============================>.] - ETA: 0s - loss: 2.8875 - accuracy: 0.3011\n",
      "Epoch 00032: val_accuracy improved from 0.32678 to 0.33100, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 683s 5ms/step - loss: 2.8875 - accuracy: 0.3011 - val_loss: 2.7537 - val_accuracy: 0.3310\n",
      "Epoch 33/500\n",
      "125790/125798 [============================>.] - ETA: 0s - loss: 2.8881 - accuracy: 0.3010\n",
      "Epoch 00033: val_accuracy did not improve from 0.33100\n",
      "125798/125798 [==============================] - 683s 5ms/step - loss: 2.8881 - accuracy: 0.3010 - val_loss: 2.8183 - val_accuracy: 0.3280\n",
      "Epoch 34/500\n",
      "125795/125798 [============================>.] - ETA: 0s - loss: 2.8799 - accuracy: 0.3026\n",
      "Epoch 00034: val_accuracy did not improve from 0.33100\n",
      "125798/125798 [==============================] - 685s 5ms/step - loss: 2.8799 - accuracy: 0.3026 - val_loss: 2.7847 - val_accuracy: 0.3249\n",
      "Epoch 35/500\n",
      "125795/125798 [============================>.] - ETA: 0s - loss: 2.8782 - accuracy: 0.3037\n",
      "Epoch 00035: val_accuracy did not improve from 0.33100\n",
      "125798/125798 [==============================] - 685s 5ms/step - loss: 2.8782 - accuracy: 0.3037 - val_loss: 2.7681 - val_accuracy: 0.3302\n",
      "Epoch 36/500\n",
      "125789/125798 [============================>.] - ETA: 0s - loss: 2.8775 - accuracy: 0.3040\n",
      "Epoch 00036: val_accuracy did not improve from 0.33100\n",
      "125798/125798 [==============================] - 685s 5ms/step - loss: 2.8775 - accuracy: 0.3040 - val_loss: 2.8179 - val_accuracy: 0.3275\n",
      "Epoch 37/500\n",
      "125797/125798 [============================>.] - ETA: 0s - loss: 2.8744 - accuracy: 0.3049\n",
      "Epoch 00037: val_accuracy did not improve from 0.33100\n",
      "125798/125798 [==============================] - 683s 5ms/step - loss: 2.8744 - accuracy: 0.3049 - val_loss: 2.9568 - val_accuracy: 0.3260\n",
      "Epoch 38/500\n",
      "125796/125798 [============================>.] - ETA: 0s - loss: 2.8702 - accuracy: 0.3053\n",
      "Epoch 00038: val_accuracy did not improve from 0.33100\n",
      "125798/125798 [==============================] - 686s 5ms/step - loss: 2.8702 - accuracy: 0.3053 - val_loss: 3.0041 - val_accuracy: 0.3241\n",
      "Epoch 39/500\n",
      "125791/125798 [============================>.] - ETA: 0s - loss: 2.8669 - accuracy: 0.3065\n",
      "Epoch 00039: val_accuracy did not improve from 0.33100\n",
      "125798/125798 [==============================] - 686s 5ms/step - loss: 2.8669 - accuracy: 0.3065 - val_loss: 2.8202 - val_accuracy: 0.3276\n",
      "Epoch 40/500\n",
      "125792/125798 [============================>.] - ETA: 0s - loss: 2.8629 - accuracy: 0.3074\n",
      "Epoch 00040: val_accuracy did not improve from 0.33100\n",
      "125798/125798 [==============================] - 687s 5ms/step - loss: 2.8629 - accuracy: 0.3074 - val_loss: 2.9044 - val_accuracy: 0.3284\n",
      "Epoch 41/500\n",
      "125787/125798 [============================>.] - ETA: 0s - loss: 2.8613 - accuracy: 0.3080\n",
      "Epoch 00041: val_accuracy improved from 0.33100 to 0.33266, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 683s 5ms/step - loss: 2.8613 - accuracy: 0.3080 - val_loss: 2.7832 - val_accuracy: 0.3327\n",
      "Epoch 42/500\n",
      "125795/125798 [============================>.] - ETA: 0s - loss: 2.8590 - accuracy: 0.3087\n",
      "Epoch 00042: val_accuracy improved from 0.33266 to 0.33302, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 688s 5ms/step - loss: 2.8590 - accuracy: 0.3087 - val_loss: 2.7611 - val_accuracy: 0.3330\n",
      "Epoch 43/500\n",
      "125788/125798 [============================>.] - ETA: 0s - loss: 2.8578 - accuracy: 0.3093\n",
      "Epoch 00043: val_accuracy did not improve from 0.33302\n",
      "125798/125798 [==============================] - 688s 5ms/step - loss: 2.8578 - accuracy: 0.3093 - val_loss: 3.1817 - val_accuracy: 0.3319\n",
      "Epoch 44/500\n",
      "125790/125798 [============================>.] - ETA: 0s - loss: 2.8566 - accuracy: 0.3095\n",
      "Epoch 00044: val_accuracy did not improve from 0.33302\n",
      "125798/125798 [==============================] - 685s 5ms/step - loss: 2.8566 - accuracy: 0.3095 - val_loss: 2.8360 - val_accuracy: 0.3308\n",
      "Epoch 45/500\n",
      "125796/125798 [============================>.] - ETA: 0s - loss: 2.8530 - accuracy: 0.3106\n",
      "Epoch 00045: val_accuracy did not improve from 0.33302\n",
      "125798/125798 [==============================] - 686s 5ms/step - loss: 2.8530 - accuracy: 0.3106 - val_loss: 3.1512 - val_accuracy: 0.3280\n",
      "Epoch 46/500\n",
      "125786/125798 [============================>.] - ETA: 0s - loss: 2.8501 - accuracy: 0.3107\n",
      "Epoch 00046: val_accuracy did not improve from 0.33302\n",
      "125798/125798 [==============================] - 685s 5ms/step - loss: 2.8501 - accuracy: 0.3107 - val_loss: 3.2204 - val_accuracy: 0.3293\n",
      "Epoch 47/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125796/125798 [============================>.] - ETA: 0s - loss: 2.8482 - accuracy: 0.3109\n",
      "Epoch 00047: val_accuracy did not improve from 0.33302\n",
      "125798/125798 [==============================] - 680s 5ms/step - loss: 2.8482 - accuracy: 0.3109 - val_loss: 2.7851 - val_accuracy: 0.3326\n",
      "Epoch 48/500\n",
      "125790/125798 [============================>.] - ETA: 0s - loss: 2.8482 - accuracy: 0.3112\n",
      "Epoch 00048: val_accuracy did not improve from 0.33302\n",
      "125798/125798 [==============================] - 681s 5ms/step - loss: 2.8482 - accuracy: 0.3112 - val_loss: 2.9748 - val_accuracy: 0.3327\n",
      "Epoch 49/500\n",
      "125794/125798 [============================>.] - ETA: 0s - loss: 2.8446 - accuracy: 0.3114\n",
      "Epoch 00049: val_accuracy did not improve from 0.33302\n",
      "125798/125798 [==============================] - 681s 5ms/step - loss: 2.8446 - accuracy: 0.3114 - val_loss: 2.9361 - val_accuracy: 0.3316\n",
      "Epoch 50/500\n",
      "125787/125798 [============================>.] - ETA: 0s - loss: 2.8451 - accuracy: 0.3124\n",
      "Epoch 00050: val_accuracy improved from 0.33302 to 0.33672, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 676s 5ms/step - loss: 2.8451 - accuracy: 0.3124 - val_loss: 2.7413 - val_accuracy: 0.3367\n",
      "Epoch 51/500\n",
      "125788/125798 [============================>.] - ETA: 0s - loss: 2.8386 - accuracy: 0.3140\n",
      "Epoch 00051: val_accuracy did not improve from 0.33672\n",
      "125798/125798 [==============================] - 679s 5ms/step - loss: 2.8386 - accuracy: 0.3140 - val_loss: 2.8158 - val_accuracy: 0.3361\n",
      "Epoch 52/500\n",
      "125791/125798 [============================>.] - ETA: 0s - loss: 2.8385 - accuracy: 0.3133\n",
      "Epoch 00052: val_accuracy did not improve from 0.33672\n",
      "125798/125798 [==============================] - 680s 5ms/step - loss: 2.8385 - accuracy: 0.3133 - val_loss: 2.7298 - val_accuracy: 0.3351\n",
      "Epoch 53/500\n",
      "125787/125798 [============================>.] - ETA: 0s - loss: 2.8364 - accuracy: 0.3144\n",
      "Epoch 00053: val_accuracy improved from 0.33672 to 0.33744, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 682s 5ms/step - loss: 2.8364 - accuracy: 0.3144 - val_loss: 2.9080 - val_accuracy: 0.3374\n",
      "Epoch 54/500\n",
      "125796/125798 [============================>.] - ETA: 0s - loss: 2.8363 - accuracy: 0.3149\n",
      "Epoch 00054: val_accuracy did not improve from 0.33744\n",
      "125798/125798 [==============================] - 681s 5ms/step - loss: 2.8363 - accuracy: 0.3149 - val_loss: 2.7578 - val_accuracy: 0.3354\n",
      "Epoch 55/500\n",
      "125790/125798 [============================>.] - ETA: 0s - loss: 2.8309 - accuracy: 0.3150\n",
      "Epoch 00055: val_accuracy did not improve from 0.33744\n",
      "125798/125798 [==============================] - 682s 5ms/step - loss: 2.8309 - accuracy: 0.3150 - val_loss: 3.0808 - val_accuracy: 0.3351\n",
      "Epoch 56/500\n",
      "125792/125798 [============================>.] - ETA: 0s - loss: 2.8303 - accuracy: 0.3155\n",
      "Epoch 00056: val_accuracy did not improve from 0.33744\n",
      "125798/125798 [==============================] - 681s 5ms/step - loss: 2.8303 - accuracy: 0.3155 - val_loss: 2.8055 - val_accuracy: 0.3358\n",
      "Epoch 57/500\n",
      "125795/125798 [============================>.] - ETA: 0s - loss: 2.8275 - accuracy: 0.3160\n",
      "Epoch 00057: val_accuracy did not improve from 0.33744\n",
      "125798/125798 [==============================] - 685s 5ms/step - loss: 2.8275 - accuracy: 0.3160 - val_loss: 3.0144 - val_accuracy: 0.3362\n",
      "Epoch 58/500\n",
      "125786/125798 [============================>.] - ETA: 0s - loss: 2.8297 - accuracy: 0.3167\n",
      "Epoch 00058: val_accuracy did not improve from 0.33744\n",
      "125798/125798 [==============================] - 682s 5ms/step - loss: 2.8297 - accuracy: 0.3167 - val_loss: 3.0235 - val_accuracy: 0.3359\n",
      "Epoch 59/500\n",
      "125795/125798 [============================>.] - ETA: 0s - loss: 2.8259 - accuracy: 0.3157\n",
      "Epoch 00059: val_accuracy improved from 0.33744 to 0.33747, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 682s 5ms/step - loss: 2.8258 - accuracy: 0.3157 - val_loss: 3.0685 - val_accuracy: 0.3375\n",
      "Epoch 60/500\n",
      "125788/125798 [============================>.] - ETA: 0s - loss: 2.8250 - accuracy: 0.3163\n",
      "Epoch 00060: val_accuracy did not improve from 0.33747\n",
      "125798/125798 [==============================] - 685s 5ms/step - loss: 2.8250 - accuracy: 0.3163 - val_loss: 2.9519 - val_accuracy: 0.3316\n",
      "Epoch 61/500\n",
      "125794/125798 [============================>.] - ETA: 0s - loss: 2.8253 - accuracy: 0.3162\n",
      "Epoch 00061: val_accuracy improved from 0.33747 to 0.34379, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 683s 5ms/step - loss: 2.8253 - accuracy: 0.3162 - val_loss: 2.7794 - val_accuracy: 0.3438\n",
      "Epoch 62/500\n",
      "125786/125798 [============================>.] - ETA: 0s - loss: 2.8245 - accuracy: 0.3175\n",
      "Epoch 00062: val_accuracy did not improve from 0.34379\n",
      "125798/125798 [==============================] - 684s 5ms/step - loss: 2.8246 - accuracy: 0.3175 - val_loss: 2.7618 - val_accuracy: 0.3400\n",
      "Epoch 63/500\n",
      "125794/125798 [============================>.] - ETA: 0s - loss: 2.8217 - accuracy: 0.3172\n",
      "Epoch 00063: val_accuracy did not improve from 0.34379\n",
      "125798/125798 [==============================] - 685s 5ms/step - loss: 2.8217 - accuracy: 0.3172 - val_loss: 2.8860 - val_accuracy: 0.3406\n",
      "Epoch 64/500\n",
      "125789/125798 [============================>.] - ETA: 0s - loss: 2.8214 - accuracy: 0.3180\n",
      "Epoch 00064: val_accuracy did not improve from 0.34379\n",
      "125798/125798 [==============================] - 689s 5ms/step - loss: 2.8214 - accuracy: 0.3180 - val_loss: 2.9886 - val_accuracy: 0.3371\n",
      "Epoch 65/500\n",
      "125786/125798 [============================>.] - ETA: 0s - loss: 2.8223 - accuracy: 0.3183\n",
      "Epoch 00065: val_accuracy did not improve from 0.34379\n",
      "125798/125798 [==============================] - 689s 5ms/step - loss: 2.8223 - accuracy: 0.3183 - val_loss: 2.7507 - val_accuracy: 0.3404\n",
      "Epoch 66/500\n",
      "125789/125798 [============================>.] - ETA: 0s - loss: 2.8168 - accuracy: 0.3191\n",
      "Epoch 00066: val_accuracy did not improve from 0.34379\n",
      "125798/125798 [==============================] - 686s 5ms/step - loss: 2.8168 - accuracy: 0.3191 - val_loss: 2.9159 - val_accuracy: 0.3385\n",
      "Epoch 67/500\n",
      "125786/125798 [============================>.] - ETA: 0s - loss: 2.8194 - accuracy: 0.3191\n",
      "Epoch 00067: val_accuracy did not improve from 0.34379\n",
      "125798/125798 [==============================] - 686s 5ms/step - loss: 2.8195 - accuracy: 0.3191 - val_loss: 2.8401 - val_accuracy: 0.3363\n",
      "Epoch 68/500\n",
      "125796/125798 [============================>.] - ETA: 0s - loss: 2.8171 - accuracy: 0.3194\n",
      "Epoch 00068: val_accuracy did not improve from 0.34379\n",
      "125798/125798 [==============================] - 687s 5ms/step - loss: 2.8171 - accuracy: 0.3194 - val_loss: 2.8840 - val_accuracy: 0.3392\n",
      "Epoch 69/500\n",
      "125793/125798 [============================>.] - ETA: 0s - loss: 2.8150 - accuracy: 0.3187\n",
      "Epoch 00069: val_accuracy did not improve from 0.34379\n",
      "125798/125798 [==============================] - 686s 5ms/step - loss: 2.8150 - accuracy: 0.3187 - val_loss: 3.0120 - val_accuracy: 0.3385\n",
      "Epoch 70/500\n",
      "125797/125798 [============================>.] - ETA: 0s - loss: 2.8153 - accuracy: 0.3196\n",
      "Epoch 00070: val_accuracy did not improve from 0.34379\n",
      "125798/125798 [==============================] - 687s 5ms/step - loss: 2.8153 - accuracy: 0.3196 - val_loss: 3.2318 - val_accuracy: 0.3422\n",
      "Epoch 71/500\n",
      "125797/125798 [============================>.] - ETA: 0s - loss: 2.8110 - accuracy: 0.3201\n",
      "Epoch 00071: val_accuracy did not improve from 0.34379\n",
      "125798/125798 [==============================] - 685s 5ms/step - loss: 2.8110 - accuracy: 0.3201 - val_loss: 2.8391 - val_accuracy: 0.3417\n",
      "Epoch 72/500\n",
      "125789/125798 [============================>.] - ETA: 0s - loss: 2.8107 - accuracy: 0.3201\n",
      "Epoch 00072: val_accuracy did not improve from 0.34379\n",
      "125798/125798 [==============================] - 686s 5ms/step - loss: 2.8107 - accuracy: 0.3201 - val_loss: 2.8588 - val_accuracy: 0.3414\n",
      "Epoch 73/500\n",
      "125785/125798 [============================>.] - ETA: 0s - loss: 2.8101 - accuracy: 0.3210\n",
      "Epoch 00073: val_accuracy improved from 0.34379 to 0.34569, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 680s 5ms/step - loss: 2.8101 - accuracy: 0.3210 - val_loss: 2.8109 - val_accuracy: 0.3457\n",
      "Epoch 74/500\n",
      "125794/125798 [============================>.] - ETA: 0s - loss: 2.8109 - accuracy: 0.3204\n",
      "Epoch 00074: val_accuracy did not improve from 0.34569\n",
      "125798/125798 [==============================] - 680s 5ms/step - loss: 2.8109 - accuracy: 0.3204 - val_loss: 3.1879 - val_accuracy: 0.3381\n",
      "Epoch 75/500\n",
      "125798/125798 [==============================] - ETA: 0s - loss: 2.8096 - accuracy: 0.3216\n",
      "Epoch 00075: val_accuracy did not improve from 0.34569\n",
      "125798/125798 [==============================] - 681s 5ms/step - loss: 2.8096 - accuracy: 0.3216 - val_loss: 2.7247 - val_accuracy: 0.3456\n",
      "Epoch 76/500\n",
      "125793/125798 [============================>.] - ETA: 0s - loss: 2.8080 - accuracy: 0.3217\n",
      "Epoch 00076: val_accuracy did not improve from 0.34569\n",
      "125798/125798 [==============================] - 680s 5ms/step - loss: 2.8080 - accuracy: 0.3217 - val_loss: 2.7891 - val_accuracy: 0.3372\n",
      "Epoch 77/500\n",
      "125790/125798 [============================>.] - ETA: 0s - loss: 2.8077 - accuracy: 0.3213\n",
      "Epoch 00077: val_accuracy did not improve from 0.34569\n",
      "125798/125798 [==============================] - 679s 5ms/step - loss: 2.8077 - accuracy: 0.3213 - val_loss: 2.7450 - val_accuracy: 0.3448\n",
      "Epoch 78/500\n",
      "125793/125798 [============================>.] - ETA: 0s - loss: 2.8065 - accuracy: 0.3215\n",
      "Epoch 00078: val_accuracy did not improve from 0.34569\n",
      "125798/125798 [==============================] - 681s 5ms/step - loss: 2.8066 - accuracy: 0.3215 - val_loss: 2.8340 - val_accuracy: 0.3431\n",
      "Epoch 79/500\n",
      "125797/125798 [============================>.] - ETA: 0s - loss: 2.8057 - accuracy: 0.3218\n",
      "Epoch 00079: val_accuracy did not improve from 0.34569\n",
      "125798/125798 [==============================] - 682s 5ms/step - loss: 2.8057 - accuracy: 0.3218 - val_loss: 2.9511 - val_accuracy: 0.3399\n",
      "Epoch 80/500\n",
      "125797/125798 [============================>.] - ETA: 0s - loss: 2.8044 - accuracy: 0.3223\n",
      "Epoch 00080: val_accuracy did not improve from 0.34569\n",
      "125798/125798 [==============================] - 682s 5ms/step - loss: 2.8044 - accuracy: 0.3223 - val_loss: 2.8322 - val_accuracy: 0.3444\n",
      "Epoch 81/500\n",
      "125792/125798 [============================>.] - ETA: 0s - loss: 2.8020 - accuracy: 0.3218\n",
      "Epoch 00081: val_accuracy did not improve from 0.34569\n",
      "125798/125798 [==============================] - 682s 5ms/step - loss: 2.8020 - accuracy: 0.3218 - val_loss: 2.8012 - val_accuracy: 0.3434\n",
      "Epoch 82/500\n",
      "125793/125798 [============================>.] - ETA: 0s - loss: 2.8036 - accuracy: 0.3228\n",
      "Epoch 00082: val_accuracy did not improve from 0.34569\n",
      "125798/125798 [==============================] - 683s 5ms/step - loss: 2.8036 - accuracy: 0.3228 - val_loss: 2.8543 - val_accuracy: 0.3394\n",
      "Epoch 83/500\n",
      "125787/125798 [============================>.] - ETA: 0s - loss: 2.8025 - accuracy: 0.3222\n",
      "Epoch 00083: val_accuracy did not improve from 0.34569\n",
      "125798/125798 [==============================] - 683s 5ms/step - loss: 2.8025 - accuracy: 0.3222 - val_loss: 2.9070 - val_accuracy: 0.3435\n",
      "Epoch 84/500\n",
      "125798/125798 [==============================] - ETA: 0s - loss: 2.7989 - accuracy: 0.3234\n",
      "Epoch 00084: val_accuracy did not improve from 0.34569\n",
      "125798/125798 [==============================] - 684s 5ms/step - loss: 2.7989 - accuracy: 0.3234 - val_loss: 2.9215 - val_accuracy: 0.3388\n",
      "Epoch 85/500\n",
      "125787/125798 [============================>.] - ETA: 0s - loss: 2.8007 - accuracy: 0.3226\n",
      "Epoch 00085: val_accuracy improved from 0.34569 to 0.34610, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 686s 5ms/step - loss: 2.8008 - accuracy: 0.3226 - val_loss: 2.7583 - val_accuracy: 0.3461\n",
      "Epoch 86/500\n",
      "125785/125798 [============================>.] - ETA: 0s - loss: 2.7999 - accuracy: 0.3235\n",
      "Epoch 00086: val_accuracy did not improve from 0.34610\n",
      "125798/125798 [==============================] - 684s 5ms/step - loss: 2.7999 - accuracy: 0.3235 - val_loss: 3.0941 - val_accuracy: 0.3412\n",
      "Epoch 87/500\n",
      "125794/125798 [============================>.] - ETA: 0s - loss: 2.7984 - accuracy: 0.3243\n",
      "Epoch 00087: val_accuracy did not improve from 0.34610\n",
      "125798/125798 [==============================] - 682s 5ms/step - loss: 2.7984 - accuracy: 0.3243 - val_loss: 3.1643 - val_accuracy: 0.3430\n",
      "Epoch 88/500\n",
      "125798/125798 [==============================] - ETA: 0s - loss: 2.7990 - accuracy: 0.3236\n",
      "Epoch 00088: val_accuracy did not improve from 0.34610\n",
      "125798/125798 [==============================] - 684s 5ms/step - loss: 2.7990 - accuracy: 0.3236 - val_loss: 2.8969 - val_accuracy: 0.3425\n",
      "Epoch 89/500\n",
      "125789/125798 [============================>.] - ETA: 0s - loss: 2.7951 - accuracy: 0.3244\n",
      "Epoch 00089: val_accuracy did not improve from 0.34610\n",
      "125798/125798 [==============================] - 687s 5ms/step - loss: 2.7951 - accuracy: 0.3244 - val_loss: 3.2922 - val_accuracy: 0.3388\n",
      "Epoch 90/500\n",
      "125789/125798 [============================>.] - ETA: 0s - loss: 2.7954 - accuracy: 0.3247\n",
      "Epoch 00090: val_accuracy did not improve from 0.34610\n",
      "125798/125798 [==============================] - 685s 5ms/step - loss: 2.7954 - accuracy: 0.3247 - val_loss: 3.2889 - val_accuracy: 0.3393\n",
      "Epoch 91/500\n",
      "125791/125798 [============================>.] - ETA: 0s - loss: 2.7987 - accuracy: 0.3236\n",
      "Epoch 00091: val_accuracy did not improve from 0.34610\n",
      "125798/125798 [==============================] - 686s 5ms/step - loss: 2.7987 - accuracy: 0.3236 - val_loss: 2.8814 - val_accuracy: 0.3430\n",
      "Epoch 92/500\n",
      "125791/125798 [============================>.] - ETA: 0s - loss: 2.7946 - accuracy: 0.3251\n",
      "Epoch 00092: val_accuracy improved from 0.34610 to 0.34625, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 685s 5ms/step - loss: 2.7946 - accuracy: 0.3251 - val_loss: 2.8024 - val_accuracy: 0.3462\n",
      "Epoch 93/500\n",
      "125787/125798 [============================>.] - ETA: 0s - loss: 2.7949 - accuracy: 0.3249\n",
      "Epoch 00093: val_accuracy improved from 0.34625 to 0.34660, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 687s 5ms/step - loss: 2.7949 - accuracy: 0.3249 - val_loss: 2.7135 - val_accuracy: 0.3466\n",
      "Epoch 94/500\n",
      "125786/125798 [============================>.] - ETA: 0s - loss: 2.7919 - accuracy: 0.3247\n",
      "Epoch 00094: val_accuracy did not improve from 0.34660\n",
      "125798/125798 [==============================] - 685s 5ms/step - loss: 2.7919 - accuracy: 0.3247 - val_loss: 2.7865 - val_accuracy: 0.3446\n",
      "Epoch 95/500\n",
      "125795/125798 [============================>.] - ETA: 0s - loss: 2.7939 - accuracy: 0.3249\n",
      "Epoch 00095: val_accuracy did not improve from 0.34660\n",
      "125798/125798 [==============================] - 686s 5ms/step - loss: 2.7939 - accuracy: 0.3249 - val_loss: 2.8469 - val_accuracy: 0.3462\n",
      "Epoch 96/500\n",
      "125788/125798 [============================>.] - ETA: 0s - loss: 2.7898 - accuracy: 0.3260\n",
      "Epoch 00096: val_accuracy did not improve from 0.34660\n",
      "125798/125798 [==============================] - 686s 5ms/step - loss: 2.7899 - accuracy: 0.3260 - val_loss: 2.8115 - val_accuracy: 0.3455\n",
      "Epoch 97/500\n",
      "125787/125798 [============================>.] - ETA: 0s - loss: 2.7925 - accuracy: 0.3255\n",
      "Epoch 00097: val_accuracy did not improve from 0.34660\n",
      "125798/125798 [==============================] - 695s 6ms/step - loss: 2.7925 - accuracy: 0.3255 - val_loss: 2.8689 - val_accuracy: 0.3463\n",
      "Epoch 98/500\n",
      "125787/125798 [============================>.] - ETA: 0s - loss: 2.7913 - accuracy: 0.3262\n",
      "Epoch 00098: val_accuracy improved from 0.34660 to 0.34810, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 734s 6ms/step - loss: 2.7913 - accuracy: 0.3263 - val_loss: 3.0072 - val_accuracy: 0.3481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/500\n",
      "125795/125798 [============================>.] - ETA: 0s - loss: 2.7906 - accuracy: 0.3260\n",
      "Epoch 00099: val_accuracy did not improve from 0.34810\n",
      "125798/125798 [==============================] - 705s 6ms/step - loss: 2.7907 - accuracy: 0.3260 - val_loss: 2.7579 - val_accuracy: 0.3479\n",
      "Epoch 100/500\n",
      "125791/125798 [============================>.] - ETA: 0s - loss: 2.7902 - accuracy: 0.3258\n",
      "Epoch 00100: val_accuracy did not improve from 0.34810\n",
      "125798/125798 [==============================] - 697s 6ms/step - loss: 2.7903 - accuracy: 0.3258 - val_loss: 3.2371 - val_accuracy: 0.3438\n",
      "Epoch 101/500\n",
      "125786/125798 [============================>.] - ETA: 0s - loss: 2.7877 - accuracy: 0.3261\n",
      "Epoch 00101: val_accuracy did not improve from 0.34810\n",
      "125798/125798 [==============================] - 696s 6ms/step - loss: 2.7877 - accuracy: 0.3261 - val_loss: 3.0378 - val_accuracy: 0.3451\n",
      "Epoch 102/500\n",
      "125789/125798 [============================>.] - ETA: 0s - loss: 2.7888 - accuracy: 0.3264\n",
      "Epoch 00102: val_accuracy did not improve from 0.34810\n",
      "125798/125798 [==============================] - 694s 6ms/step - loss: 2.7888 - accuracy: 0.3264 - val_loss: 3.0120 - val_accuracy: 0.3426\n",
      "Epoch 103/500\n",
      "125790/125798 [============================>.] - ETA: 0s - loss: 2.7880 - accuracy: 0.3263\n",
      "Epoch 00103: val_accuracy did not improve from 0.34810\n",
      "125798/125798 [==============================] - 689s 5ms/step - loss: 2.7880 - accuracy: 0.3263 - val_loss: 3.3465 - val_accuracy: 0.3426\n",
      "Epoch 104/500\n",
      "125787/125798 [============================>.] - ETA: 0s - loss: 2.7861 - accuracy: 0.3266\n",
      "Epoch 00104: val_accuracy did not improve from 0.34810\n",
      "125798/125798 [==============================] - 698s 6ms/step - loss: 2.7862 - accuracy: 0.3266 - val_loss: 2.8145 - val_accuracy: 0.3456\n",
      "Epoch 105/500\n",
      "125793/125798 [============================>.] - ETA: 0s - loss: 2.7846 - accuracy: 0.3274\n",
      "Epoch 00105: val_accuracy did not improve from 0.34810\n",
      "125798/125798 [==============================] - 704s 6ms/step - loss: 2.7846 - accuracy: 0.3274 - val_loss: 2.7246 - val_accuracy: 0.3457\n",
      "Epoch 106/500\n",
      "125789/125798 [============================>.] - ETA: 0s - loss: 2.7873 - accuracy: 0.3268\n",
      "Epoch 00106: val_accuracy did not improve from 0.34810\n",
      "125798/125798 [==============================] - 685s 5ms/step - loss: 2.7873 - accuracy: 0.3268 - val_loss: 3.0815 - val_accuracy: 0.3418\n",
      "Epoch 107/500\n",
      "125798/125798 [==============================] - ETA: 0s - loss: 2.7856 - accuracy: 0.3273\n",
      "Epoch 00107: val_accuracy did not improve from 0.34810\n",
      "125798/125798 [==============================] - 696s 6ms/step - loss: 2.7856 - accuracy: 0.3273 - val_loss: 2.8290 - val_accuracy: 0.3472\n",
      "Epoch 108/500\n",
      "125791/125798 [============================>.] - ETA: 0s - loss: 2.7861 - accuracy: 0.3269\n",
      "Epoch 00108: val_accuracy did not improve from 0.34810\n",
      "125798/125798 [==============================] - 910s 7ms/step - loss: 2.7861 - accuracy: 0.3269 - val_loss: 2.7862 - val_accuracy: 0.3425\n",
      "Epoch 109/500\n",
      "125790/125798 [============================>.] - ETA: 0s - loss: 2.7840 - accuracy: 0.3272\n",
      "Epoch 00109: val_accuracy improved from 0.34810 to 0.34825, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 920s 7ms/step - loss: 2.7840 - accuracy: 0.3272 - val_loss: 2.7826 - val_accuracy: 0.3483\n",
      "Epoch 110/500\n",
      "125797/125798 [============================>.] - ETA: 0s - loss: 2.7816 - accuracy: 0.3273\n",
      "Epoch 00110: val_accuracy improved from 0.34825 to 0.34969, saving model to /media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/checkpoint.hdf5\n",
      "125798/125798 [==============================] - 919s 7ms/step - loss: 2.7816 - accuracy: 0.3273 - val_loss: 2.7669 - val_accuracy: 0.3497\n",
      "Epoch 111/500\n",
      "125798/125798 [==============================] - ETA: 0s - loss: 2.7849 - accuracy: 0.3270\n",
      "Epoch 00111: val_accuracy did not improve from 0.34969\n",
      "125798/125798 [==============================] - 908s 7ms/step - loss: 2.7849 - accuracy: 0.3270 - val_loss: 3.1407 - val_accuracy: 0.3433\n",
      "Epoch 112/500\n",
      "125792/125798 [============================>.] - ETA: 0s - loss: 2.7817 - accuracy: 0.3275\n",
      "Epoch 00112: val_accuracy did not improve from 0.34969\n",
      "125798/125798 [==============================] - 872s 7ms/step - loss: 2.7817 - accuracy: 0.3275 - val_loss: 2.7773 - val_accuracy: 0.3462\n",
      "Epoch 113/500\n",
      "125793/125798 [============================>.] - ETA: 0s - loss: 2.7806 - accuracy: 0.3284\n",
      "Epoch 00113: val_accuracy did not improve from 0.34969\n",
      "125798/125798 [==============================] - 862s 7ms/step - loss: 2.7806 - accuracy: 0.3284 - val_loss: 2.8230 - val_accuracy: 0.3466\n",
      "Epoch 114/500\n",
      "125789/125798 [============================>.] - ETA: 0s - loss: 2.7815 - accuracy: 0.3278\n",
      "Epoch 00114: val_accuracy did not improve from 0.34969\n",
      "125798/125798 [==============================] - 862s 7ms/step - loss: 2.7815 - accuracy: 0.3278 - val_loss: 2.8695 - val_accuracy: 0.3478\n",
      "Epoch 115/500\n",
      "125792/125798 [============================>.] - ETA: 0s - loss: 2.7827 - accuracy: 0.3276\n",
      "Epoch 00115: val_accuracy did not improve from 0.34969\n",
      "125798/125798 [==============================] - 694s 6ms/step - loss: 2.7827 - accuracy: 0.3276 - val_loss: 2.7715 - val_accuracy: 0.3485\n",
      "Epoch 116/500\n",
      "125792/125798 [============================>.] - ETA: 0s - loss: 2.7832 - accuracy: 0.3275\n",
      "Epoch 00116: val_accuracy did not improve from 0.34969\n",
      "125798/125798 [==============================] - 686s 5ms/step - loss: 2.7832 - accuracy: 0.3275 - val_loss: 2.9306 - val_accuracy: 0.3442\n",
      "Epoch 117/500\n",
      "125792/125798 [============================>.] - ETA: 0s - loss: 2.7820 - accuracy: 0.3279\n",
      "Epoch 00117: val_accuracy did not improve from 0.34969\n",
      "125798/125798 [==============================] - 711s 6ms/step - loss: 2.7820 - accuracy: 0.3279 - val_loss: 3.3602 - val_accuracy: 0.3424\n",
      "Epoch 118/500\n",
      "125793/125798 [============================>.] - ETA: 0s - loss: 2.7820 - accuracy: 0.3279\n",
      "Epoch 00118: val_accuracy did not improve from 0.34969\n",
      "125798/125798 [==============================] - 688s 5ms/step - loss: 2.7820 - accuracy: 0.3279 - val_loss: 2.8866 - val_accuracy: 0.3454\n",
      "Epoch 119/500\n",
      "125791/125798 [============================>.] - ETA: 0s - loss: 2.7792 - accuracy: 0.3290\n",
      "Epoch 00119: val_accuracy did not improve from 0.34969\n",
      "125798/125798 [==============================] - 693s 6ms/step - loss: 2.7792 - accuracy: 0.3290 - val_loss: 3.0535 - val_accuracy: 0.3453\n",
      "Epoch 120/500\n",
      "125795/125798 [============================>.] - ETA: 0s - loss: 2.7820 - accuracy: 0.3277\n",
      "Epoch 00120: val_accuracy did not improve from 0.34969\n",
      "125798/125798 [==============================] - 720s 6ms/step - loss: 2.7820 - accuracy: 0.3277 - val_loss: 3.3810 - val_accuracy: 0.3450\n",
      "Epoch 121/500\n",
      "125797/125798 [============================>.] - ETA: 0s - loss: 2.7793 - accuracy: 0.3284\n",
      "Epoch 00121: val_accuracy did not improve from 0.34969\n",
      "125798/125798 [==============================] - 716s 6ms/step - loss: 2.7793 - accuracy: 0.3284 - val_loss: 3.1103 - val_accuracy: 0.3464\n",
      "Epoch 122/500\n",
      "125789/125798 [============================>.] - ETA: 0s - loss: 2.7787 - accuracy: 0.3282\n",
      "Epoch 00122: val_accuracy did not improve from 0.34969\n",
      "125798/125798 [==============================] - 705s 6ms/step - loss: 2.7787 - accuracy: 0.3282 - val_loss: 2.7452 - val_accuracy: 0.3468\n",
      "Epoch 123/500\n",
      "125787/125798 [============================>.] - ETA: 0s - loss: 2.7776 - accuracy: 0.3284\n",
      "Epoch 00123: val_accuracy did not improve from 0.34969\n",
      "125798/125798 [==============================] - 718s 6ms/step - loss: 2.7776 - accuracy: 0.3284 - val_loss: 2.9414 - val_accuracy: 0.3472\n"
     ]
    }
   ],
   "source": [
    "csv_logger = CSVLogger('/media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/LSTM_nina_20X100.csv', append=True, separator=';')\n",
    "history = model.fit(x_train, y_train_hot, epochs=epochs, batch_size=batch_size, callbacks=[csv_logger,checkpoint_callback,early],validation_data=(x_test, y_test_hot), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = history.history['val_accuracy'].index(max(history.history['val_accuracy']))\n",
    "print('epoch_number',best_index+1)\n",
    "print('train accuracy and validation accuracy', history.history['accuracy'][best_index], history.history['val_accuracy'][best_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/media/naveen/71A3-5054/sEMG_codes/ninapro_DB1/CNN20X10/CNN_LSTM_nina_5X97')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in history.history:\n",
    "    print(\"history\",i)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(history.history['accuracy'], 'r', label='Accuracy of training data')\n",
    "plt.plot(history.history['val_accuracy'], 'b', label='Accuracy of validation data')\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylim(0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in history.history:\n",
    "    print(\"history\",i)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(history.history['loss'], 'r--', label='Loss of training data')\n",
    "plt.plot(history.history['val_loss'], 'b--', label='Loss of validation data')\n",
    "plt.title('Model  Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylim(0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath) \n",
    "_, testaccuracy = model.evaluate(x_test, y_test_hot, batch_size=batch_size, verbose=1)\n",
    "print('test_accuracy',testaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(x_train)\n",
    "# Take the class with the highest probability from the train predictions\n",
    "max_y_pred_train = np.argmax(y_pred_train, axis=1)\n",
    "#max_y_train = np.argmax(y_train, axis=1)\n",
    "show_confusion_matrix(y_train, max_y_pred_train)\n",
    "print(classification_report(y_train, max_y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(x_test)\n",
    "# Take the class with the highest probability from the test predictions\n",
    "max_y_pred_test = np.argmax(y_pred_test, axis=1)\n",
    "max_y_test = np.argmax(y_test_hot, axis=1)\n",
    "show_confusion_matrix(max_y_test, max_y_pred_test)\n",
    "\n",
    "print(classification_report(max_y_test, max_y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
