{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from matplotlib import pyplot as plt\n",
    "# get_ipython().magic(u'matplotlib auto')\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "# import torch\n",
    "from tensorflow import keras\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,Activation\n",
    "from tensorflow import reshape\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.layers import Conv1D,Conv2D, MaxPooling1D,AveragePooling1D\n",
    "from tensorflow.keras.layers import Input, LocallyConnected1D\n",
    "from tensorflow.keras.layers import SeparableConv1D, Bidirectional\n",
    "from tensorflow.keras.layers import LocallyConnected2D\n",
    "from tensorflow.keras.layers import ZeroPadding2D,ZeroPadding1D, MaxPooling2D, Bidirectional\n",
    "from tensorflow.keras.regularizers import l2,l1\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import CSVLogger,LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "# import coremltools\n",
    "# from torch import nn, optim\n",
    "# import torch.nn.functional as F\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "#from IPython.display import display, HTML\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of steps within one time segment\n",
    "TIME_PERIODS = 20\n",
    "# The steps to take from one segment to the next; if this value is equal to\n",
    "# TIME_PERIODS, then there is no overlap between the segments\n",
    "STEP_DISTANCE = 20\n",
    "N_FEATURES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'Class_label']\n"
     ]
    }
   ],
   "source": [
    "column_names = ['C'+str(j) for j in range(1, N_FEATURES+1)]\n",
    "lst = ['Class_label']\n",
    "column_names = column_names+lst\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_Train(file_path_Train):\n",
    "    df_Train = pd.read_csv(file_path_Train,header=None,names=column_names)\n",
    "    # Last column has a \";\" character which must be removed ...\n",
    "    df_Train['Class_label'].replace(regex=True,inplace=True,to_replace=r';',value=r'')\n",
    "    # ... and then this column must be transformed to float explicitly\n",
    "    df_Train['Class_label'] = df_Train['Class_label'].apply(convert_to_float)\n",
    "    # This is very important otherwise the model will not fit and loss\n",
    "    # will show up as NAN\n",
    "    df_Train.dropna(axis=0, how='any', inplace=True)\n",
    "    return df_Train\n",
    "def convert_to_float(x):\n",
    "    try:\n",
    "        return np.float(x)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_basic_dataframe_info(dataframe):\n",
    "    # Shape and how many rows and columns\n",
    "    print('Number of columns in the dataframe: %i' % (dataframe.shape[1]))\n",
    "    print('Number of rows in the dataframe: %i\\n' % (dataframe.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_Test(file_path_Test):\n",
    "    df_Test = pd.read_csv(file_path_Test,header=None,names=column_names)\n",
    "    # Last column has a \";\" character which must be removed ...\n",
    "    df_Test['Class_label'].replace(regex=True,inplace=True,to_replace=r';',value=r'')\n",
    "    # ... and then this column must be transformed to float explicitly\n",
    "    df_Test['Class_label'] = df_Test['Class_label'].apply(convert_to_float)\n",
    "    # This is very important otherwise the model will not fit and loss\n",
    "    # will show up as NAN\n",
    "    df_Test.dropna(axis=0, how='any', inplace=True)\n",
    "    return df_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " pd.options.display.float_format = \"{:,.5f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 function to segment data into trial lengths (trial length =513 samples in this dataset)\n",
    "def create_segments_and_labels(df, time_steps,step,n_features, label_name):\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(0, len(df) - time_steps, step):\n",
    "      for j in range(1, n_features+1):\n",
    "        L = ('C'+str(j)) \n",
    "        segments.append(df[str(L)].values[i: i + time_steps])\n",
    "      label = stats.mode(df[label_name][i: i + time_steps])[0][0]\n",
    "      labels.append(label)\n",
    "    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, time_steps, n_features)\n",
    "    labels = np.asarray(labels)\n",
    "    return reshaped_segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose, epochs, batch_size = 0, 120, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "   initial_lrate = 1e-3\n",
    "   drop = 0.1\n",
    "   epochs_drop = 40.0\n",
    "   lrate = initial_lrate * tf.math.pow(drop,  \n",
    "           tf.math.floor((1+epoch)/epochs_drop))\n",
    "   return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrate = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
    "test_acc=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path='/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_data,n_channel):\n",
    "    inputs=input_data\n",
    "    print(inputs.shape)\n",
    "    l_y=[]\n",
    "    for i in range(0,n_channel):\n",
    "        ip=inputs[:,:,i]\n",
    "        print('ip_shape',ip.shape)\n",
    "#         ip1=tf.convert_to_tensor(ip, dtype=tf.float32)\n",
    "        ip=tf.reshape(ip,(tf.shape(ip)[0],20,1))\n",
    "        print('ip_shape',ip.shape)\n",
    "        x = Conv1D(64, kernel_size=3,kernel_initializer=\"he_normal\",kernel_regularizer=l1(1e-04), \\\n",
    "               padding='same',input_shape=ip.shape)(ip)\n",
    "        print('conv1_o',x.shape)\n",
    "        x = BatchNormalization(epsilon=1e-06, momentum=0.9, weights=None)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x= Conv1D(filters=64, kernel_size=3,padding=\"same\", kernel_regularizer=l2(5e-04),kernel_initializer=\"he_normal\",strides=1)(x)\n",
    "        x= BatchNormalization(epsilon=1e-06, momentum=0.9, weights=None)(x)\n",
    "        x= Activation('relu')(x)\n",
    "        x= LocallyConnected1D(64, kernel_size=1, kernel_regularizer=l2(5e-04),kernel_initializer=\"he_normal\",strides=1,padding='valid')(x)\n",
    "        x= BatchNormalization(epsilon=1e-06, momentum=0.9, weights=None)(x)\n",
    "        x= Activation('relu')(x)\n",
    "        x= LocallyConnected1D(64, kernel_size=1, kernel_regularizer=l2(5e-04),kernel_initializer=\"he_normal\",strides=1,padding='valid')(x)\n",
    "        x= BatchNormalization(epsilon=1e-06, momentum=0.9, weights=None)(x)\n",
    "        x= Activation('relu')(x)\n",
    "        y= Dropout(0.5)(x)\n",
    "        l_y.append(y)\n",
    "    y_stack=tf.stack(l_y, axis=2)   \n",
    "    return y_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_length =  20\n",
    "n_depth=10\n",
    "n_channel=10\n",
    "n_outputs=52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    inputs = Input(shape=(n_length,n_depth),name=\"main\")\n",
    "    p=conv_block(inputs,10)\n",
    "    print(p.shape)\n",
    "    print(type(p))\n",
    "#     print(y.shape)\n",
    "    y=Flatten()(p)\n",
    "    print(y.shape)\n",
    "    y= Dense(512, kernel_initializer=\"he_normal\")(y)\n",
    "    y= BatchNormalization(epsilon=1e-06, momentum=0.9, weights=None)(y)\n",
    "    y= Activation('relu')(y)\n",
    "    y= Dropout(0.5)(y)\n",
    "    y= Dense(512, kernel_initializer=\"he_normal\")(y)\n",
    "    y= BatchNormalization(epsilon=1e-06, momentum=0.9, weights=None)(y)\n",
    "    y= Activation('relu')(y)\n",
    "    y= Dropout(0.5)(y)\n",
    "    y= Dense(128, kernel_initializer=\"he_normal\")(y)\n",
    "    y= BatchNormalization(epsilon=1e-06, momentum=0.9, weights=None)(y)\n",
    "    y= Activation('relu')(y)\n",
    "    outputs=Dense(n_outputs, activation='softmax')(y)\n",
    "    print(outputs.shape)\n",
    "#     wei_mscnn_model = Model(inputs,sub_inputs], outputs)\n",
    "    print('inputs_shape',inputs.shape)\n",
    "    final_model = Model(inputs,outputs,name=\"wei_mscnn_model\")\n",
    "    return final_model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 20, 10)\n",
      "ip_shape (None, 20)\n",
      "ip_shape (None, 20, 1)\n",
      "conv1_o (None, 20, 64)\n",
      "ip_shape (None, 20)\n",
      "ip_shape (None, 20, 1)\n",
      "conv1_o (None, 20, 64)\n",
      "ip_shape (None, 20)\n",
      "ip_shape (None, 20, 1)\n",
      "conv1_o (None, 20, 64)\n",
      "ip_shape (None, 20)\n",
      "ip_shape (None, 20, 1)\n",
      "conv1_o (None, 20, 64)\n",
      "ip_shape (None, 20)\n",
      "ip_shape (None, 20, 1)\n",
      "conv1_o (None, 20, 64)\n",
      "ip_shape (None, 20)\n",
      "ip_shape (None, 20, 1)\n",
      "conv1_o (None, 20, 64)\n",
      "ip_shape (None, 20)\n",
      "ip_shape (None, 20, 1)\n",
      "conv1_o (None, 20, 64)\n",
      "ip_shape (None, 20)\n",
      "ip_shape (None, 20, 1)\n",
      "conv1_o (None, 20, 64)\n",
      "ip_shape (None, 20)\n",
      "ip_shape (None, 20, 1)\n",
      "conv1_o (None, 20, 64)\n",
      "ip_shape (None, 20)\n",
      "ip_shape (None, 20, 1)\n",
      "conv1_o (None, 20, 64)\n",
      "(None, 20, 10, 64)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(None, 12800)\n",
      "(None, 52)\n",
      "inputs_shape (None, 20, 10)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = generate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S1_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S1_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 5.8801 - accuracy: 0.0635\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.09797, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 16s 2s/step - loss: 5.8801 - accuracy: 0.0635 - val_loss: 5.7292 - val_accuracy: 0.0980 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 5.2919 - accuracy: 0.1527\n",
      "Epoch 00002: val_accuracy improved from 0.09797 to 0.20800, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 5.2883 - accuracy: 0.1533 - val_loss: 5.3295 - val_accuracy: 0.2080 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.7886 - accuracy: 0.2374\n",
      "Epoch 00003: val_accuracy improved from 0.20800 to 0.32060, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 4.7854 - accuracy: 0.2379 - val_loss: 4.7068 - val_accuracy: 0.3206 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.3178 - accuracy: 0.3240\n",
      "Epoch 00004: val_accuracy improved from 0.32060 to 0.41241, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 4.3164 - accuracy: 0.3236 - val_loss: 4.0608 - val_accuracy: 0.4124 - lr: 0.0010\n",
      "Epoch 5/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 3.9205 - accuracy: 0.3804\n",
      "Epoch 00005: val_accuracy improved from 0.41241 to 0.47320, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 3.9205 - accuracy: 0.3804 - val_loss: 3.6186 - val_accuracy: 0.4732 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.6133 - accuracy: 0.4239\n",
      "Epoch 00006: val_accuracy improved from 0.47320 to 0.51346, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 3.6113 - accuracy: 0.4247 - val_loss: 3.2884 - val_accuracy: 0.5135 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.3235 - accuracy: 0.4730\n",
      "Epoch 00007: val_accuracy improved from 0.51346 to 0.53783, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 3.3216 - accuracy: 0.4735 - val_loss: 3.0399 - val_accuracy: 0.5378 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.0874 - accuracy: 0.5104\n",
      "Epoch 00008: val_accuracy improved from 0.53783 to 0.57092, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 3.0858 - accuracy: 0.5101 - val_loss: 2.8396 - val_accuracy: 0.5709 - lr: 0.0010\n",
      "Epoch 9/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.8940 - accuracy: 0.5332\n",
      "Epoch 00009: val_accuracy improved from 0.57092 to 0.59400, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 2.8940 - accuracy: 0.5332 - val_loss: 2.6572 - val_accuracy: 0.5940 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.7302 - accuracy: 0.5562\n",
      "Epoch 00010: val_accuracy improved from 0.59400 to 0.60682, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 2.7316 - accuracy: 0.5556 - val_loss: 2.4960 - val_accuracy: 0.6068 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.6045 - accuracy: 0.5699\n",
      "Epoch 00011: val_accuracy improved from 0.60682 to 0.61990, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 2.6054 - accuracy: 0.5697 - val_loss: 2.3829 - val_accuracy: 0.6199 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4605 - accuracy: 0.5924\n",
      "Epoch 00012: val_accuracy did not improve from 0.61990\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 2.4595 - accuracy: 0.5916 - val_loss: 2.3035 - val_accuracy: 0.6199 - lr: 0.0010\n",
      "Epoch 13/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.3529 - accuracy: 0.6063\n",
      "Epoch 00013: val_accuracy improved from 0.61990 to 0.65094, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 2.3529 - accuracy: 0.6063 - val_loss: 2.1838 - val_accuracy: 0.6509 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2661 - accuracy: 0.6162\n",
      "Epoch 00014: val_accuracy improved from 0.65094 to 0.66376, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 2.2648 - accuracy: 0.6168 - val_loss: 2.0688 - val_accuracy: 0.6638 - lr: 0.0010\n",
      "Epoch 15/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.1587 - accuracy: 0.6328\n",
      "Epoch 00015: val_accuracy did not improve from 0.66376\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 2.1587 - accuracy: 0.6328 - val_loss: 2.0430 - val_accuracy: 0.6463 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1122 - accuracy: 0.6360\n",
      "Epoch 00016: val_accuracy improved from 0.66376 to 0.66556, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 2.1122 - accuracy: 0.6361 - val_loss: 1.9471 - val_accuracy: 0.6656 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0406 - accuracy: 0.6482\n",
      "Epoch 00017: val_accuracy improved from 0.66556 to 0.67325, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 2.0406 - accuracy: 0.6477 - val_loss: 1.8855 - val_accuracy: 0.6732 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9546 - accuracy: 0.6597\n",
      "Epoch 00018: val_accuracy improved from 0.67325 to 0.69043, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 1.9551 - accuracy: 0.6596 - val_loss: 1.8148 - val_accuracy: 0.6904 - lr: 0.0010\n",
      "Epoch 19/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.9060 - accuracy: 0.6671\n",
      "Epoch 00019: val_accuracy improved from 0.69043 to 0.70044, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 1.9060 - accuracy: 0.6671 - val_loss: 1.7689 - val_accuracy: 0.7004 - lr: 0.0010\n",
      "Epoch 20/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.8606 - accuracy: 0.6763\n",
      "Epoch 00020: val_accuracy did not improve from 0.70044\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1.8606 - accuracy: 0.6763 - val_loss: 1.7413 - val_accuracy: 0.6961 - lr: 0.0010\n",
      "Epoch 21/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.8247 - accuracy: 0.6766\n",
      "Epoch 00021: val_accuracy improved from 0.70044 to 0.70659, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 1.8247 - accuracy: 0.6766 - val_loss: 1.7027 - val_accuracy: 0.7066 - lr: 0.0010\n",
      "Epoch 22/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7699 - accuracy: 0.6877\n",
      "Epoch 00022: val_accuracy did not improve from 0.70659\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.7712 - accuracy: 0.6868 - val_loss: 1.6949 - val_accuracy: 0.6976 - lr: 0.0010\n",
      "Epoch 23/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.7306 - accuracy: 0.6873\n",
      "Epoch 00023: val_accuracy did not improve from 0.70659\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.7306 - accuracy: 0.6873 - val_loss: 1.7352 - val_accuracy: 0.6863 - lr: 0.0010\n",
      "Epoch 24/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.6891 - accuracy: 0.6950\n",
      "Epoch 00024: val_accuracy improved from 0.70659 to 0.71711, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.6891 - accuracy: 0.6950 - val_loss: 1.6156 - val_accuracy: 0.7171 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6432 - accuracy: 0.7057\n",
      "Epoch 00025: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1.6424 - accuracy: 0.7058 - val_loss: 1.5693 - val_accuracy: 0.7117 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6163 - accuracy: 0.7078\n",
      "Epoch 00026: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.6179 - accuracy: 0.7071 - val_loss: 1.5924 - val_accuracy: 0.7171 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6056 - accuracy: 0.7100\n",
      "Epoch 00027: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.6090 - accuracy: 0.7091 - val_loss: 1.5875 - val_accuracy: 0.7004 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5971 - accuracy: 0.7022\n",
      "Epoch 00028: val_accuracy improved from 0.71711 to 0.72018, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 1.5970 - accuracy: 0.7024 - val_loss: 1.5494 - val_accuracy: 0.7202 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5357 - accuracy: 0.7216\n",
      "Epoch 00029: val_accuracy improved from 0.72018 to 0.73326, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 1.5364 - accuracy: 0.7215 - val_loss: 1.4845 - val_accuracy: 0.7333 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5142 - accuracy: 0.7201\n",
      "Epoch 00030: val_accuracy improved from 0.73326 to 0.73583, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 133ms/step - loss: 1.5140 - accuracy: 0.7204 - val_loss: 1.4545 - val_accuracy: 0.7358 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4730 - accuracy: 0.7292\n",
      "Epoch 00031: val_accuracy improved from 0.73583 to 0.73609, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 1.4758 - accuracy: 0.7274 - val_loss: 1.4291 - val_accuracy: 0.7361 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4511 - accuracy: 0.7376\n",
      "Epoch 00032: val_accuracy improved from 0.73609 to 0.74378, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.4530 - accuracy: 0.7368 - val_loss: 1.4146 - val_accuracy: 0.7438 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4431 - accuracy: 0.7317\n",
      "Epoch 00033: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1.4437 - accuracy: 0.7315 - val_loss: 1.3970 - val_accuracy: 0.7351 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4369 - accuracy: 0.7313\n",
      "Epoch 00034: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.4376 - accuracy: 0.7308 - val_loss: 1.4385 - val_accuracy: 0.7276 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4252 - accuracy: 0.7302\n",
      "Epoch 00035: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.4259 - accuracy: 0.7305 - val_loss: 1.3840 - val_accuracy: 0.7394 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3790 - accuracy: 0.7479\n",
      "Epoch 00036: val_accuracy improved from 0.74378 to 0.74711, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.3796 - accuracy: 0.7478 - val_loss: 1.3840 - val_accuracy: 0.7471 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3409 - accuracy: 0.7478\n",
      "Epoch 00037: val_accuracy did not improve from 0.74711\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.3416 - accuracy: 0.7474 - val_loss: 1.4018 - val_accuracy: 0.7348 - lr: 0.0010\n",
      "Epoch 38/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3206 - accuracy: 0.7599\n",
      "Epoch 00038: val_accuracy did not improve from 0.74711\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1.3206 - accuracy: 0.7599 - val_loss: 1.3643 - val_accuracy: 0.7399 - lr: 0.0010\n",
      "Epoch 39/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3166 - accuracy: 0.7585\n",
      "Epoch 00039: val_accuracy improved from 0.74711 to 0.75199, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.3166 - accuracy: 0.7585 - val_loss: 1.3259 - val_accuracy: 0.7520 - lr: 0.0010\n",
      "Epoch 40/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2660 - accuracy: 0.7725\n",
      "Epoch 00040: val_accuracy improved from 0.75199 to 0.75866, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 1.2660 - accuracy: 0.7725 - val_loss: 1.3068 - val_accuracy: 0.7587 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2373 - accuracy: 0.7765\n",
      "Epoch 00041: val_accuracy improved from 0.75866 to 0.75994, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 1.2373 - accuracy: 0.7765 - val_loss: 1.2954 - val_accuracy: 0.7599 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2073 - accuracy: 0.7856\n",
      "Epoch 00042: val_accuracy improved from 0.75994 to 0.76532, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 1.2075 - accuracy: 0.7858 - val_loss: 1.2810 - val_accuracy: 0.7653 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2061 - accuracy: 0.7912\n",
      "Epoch 00043: val_accuracy did not improve from 0.76532\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.2061 - accuracy: 0.7912 - val_loss: 1.2829 - val_accuracy: 0.7648 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1894 - accuracy: 0.7983\n",
      "Epoch 00044: val_accuracy improved from 0.76532 to 0.76917, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 1.1894 - accuracy: 0.7983 - val_loss: 1.2704 - val_accuracy: 0.7692 - lr: 1.0000e-04\n",
      "Epoch 45/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1830 - accuracy: 0.7963\n",
      "Epoch 00045: val_accuracy improved from 0.76917 to 0.76968, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.1846 - accuracy: 0.7957 - val_loss: 1.2658 - val_accuracy: 0.7697 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1803 - accuracy: 0.7937\n",
      "Epoch 00046: val_accuracy did not improve from 0.76968\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1.1803 - accuracy: 0.7937 - val_loss: 1.2656 - val_accuracy: 0.7671 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1734 - accuracy: 0.7996\n",
      "Epoch 00047: val_accuracy improved from 0.76968 to 0.77097, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 1.1734 - accuracy: 0.7996 - val_loss: 1.2476 - val_accuracy: 0.7710 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1601 - accuracy: 0.8017\n",
      "Epoch 00048: val_accuracy improved from 0.77097 to 0.77302, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 1.1637 - accuracy: 0.8005 - val_loss: 1.2507 - val_accuracy: 0.7730 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1646 - accuracy: 0.7995\n",
      "Epoch 00049: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 1.1646 - accuracy: 0.7995 - val_loss: 1.2413 - val_accuracy: 0.7707 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1544 - accuracy: 0.8023\n",
      "Epoch 00050: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 1.1554 - accuracy: 0.8017 - val_loss: 1.2400 - val_accuracy: 0.7694 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1467 - accuracy: 0.8047\n",
      "Epoch 00051: val_accuracy improved from 0.77302 to 0.77379, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.1501 - accuracy: 0.8034 - val_loss: 1.2357 - val_accuracy: 0.7738 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1459 - accuracy: 0.8030\n",
      "Epoch 00052: val_accuracy did not improve from 0.77379\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.1462 - accuracy: 0.8032 - val_loss: 1.2415 - val_accuracy: 0.7738 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1405 - accuracy: 0.8072\n",
      "Epoch 00053: val_accuracy improved from 0.77379 to 0.77635, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 1.1405 - accuracy: 0.8072 - val_loss: 1.2259 - val_accuracy: 0.7764 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1354 - accuracy: 0.8071\n",
      "Epoch 00054: val_accuracy did not improve from 0.77635\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1.1354 - accuracy: 0.8071 - val_loss: 1.2237 - val_accuracy: 0.7758 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1241 - accuracy: 0.8126\n",
      "Epoch 00055: val_accuracy improved from 0.77635 to 0.77969, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.1243 - accuracy: 0.8124 - val_loss: 1.2208 - val_accuracy: 0.7797 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1180 - accuracy: 0.8114\n",
      "Epoch 00056: val_accuracy did not improve from 0.77969\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.1180 - accuracy: 0.8114 - val_loss: 1.2189 - val_accuracy: 0.7748 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1132 - accuracy: 0.8141\n",
      "Epoch 00057: val_accuracy did not improve from 0.77969\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.1180 - accuracy: 0.8126 - val_loss: 1.2136 - val_accuracy: 0.7728 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1118 - accuracy: 0.8146\n",
      "Epoch 00058: val_accuracy did not improve from 0.77969\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1.1123 - accuracy: 0.8143 - val_loss: 1.2130 - val_accuracy: 0.7728 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1110 - accuracy: 0.8089\n",
      "Epoch 00059: val_accuracy did not improve from 0.77969\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 1.1125 - accuracy: 0.8084 - val_loss: 1.2077 - val_accuracy: 0.7756 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0965 - accuracy: 0.8204\n",
      "Epoch 00060: val_accuracy did not improve from 0.77969\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 1.0977 - accuracy: 0.8197 - val_loss: 1.2068 - val_accuracy: 0.7758 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0935 - accuracy: 0.8159\n",
      "Epoch 00061: val_accuracy did not improve from 0.77969\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1.0940 - accuracy: 0.8157 - val_loss: 1.2050 - val_accuracy: 0.7743 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0890 - accuracy: 0.8172\n",
      "Epoch 00062: val_accuracy did not improve from 0.77969\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0908 - accuracy: 0.8166 - val_loss: 1.2133 - val_accuracy: 0.7705 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0873 - accuracy: 0.8202\n",
      "Epoch 00063: val_accuracy did not improve from 0.77969\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1.0895 - accuracy: 0.8190 - val_loss: 1.1997 - val_accuracy: 0.7753 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0874 - accuracy: 0.8202\n",
      "Epoch 00064: val_accuracy did not improve from 0.77969\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 1.0873 - accuracy: 0.8202 - val_loss: 1.1967 - val_accuracy: 0.7797 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0773 - accuracy: 0.8261\n",
      "Epoch 00065: val_accuracy did not improve from 0.77969\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0800 - accuracy: 0.8254 - val_loss: 1.1971 - val_accuracy: 0.7738 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0688 - accuracy: 0.8243\n",
      "Epoch 00066: val_accuracy did not improve from 0.77969\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 1.0684 - accuracy: 0.8247 - val_loss: 1.1947 - val_accuracy: 0.7776 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0689 - accuracy: 0.8254\n",
      "Epoch 00067: val_accuracy did not improve from 0.77969\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0720 - accuracy: 0.8243 - val_loss: 1.1947 - val_accuracy: 0.7779 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0705 - accuracy: 0.8228\n",
      "Epoch 00068: val_accuracy improved from 0.77969 to 0.78123, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.0706 - accuracy: 0.8227 - val_loss: 1.1834 - val_accuracy: 0.7812 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0659 - accuracy: 0.8227\n",
      "Epoch 00069: val_accuracy did not improve from 0.78123\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.0659 - accuracy: 0.8227 - val_loss: 1.1954 - val_accuracy: 0.7787 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0672 - accuracy: 0.8225\n",
      "Epoch 00070: val_accuracy did not improve from 0.78123\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0672 - accuracy: 0.8225 - val_loss: 1.1851 - val_accuracy: 0.7797 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0470 - accuracy: 0.8310\n",
      "Epoch 00071: val_accuracy did not improve from 0.78123\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.0478 - accuracy: 0.8310 - val_loss: 1.1861 - val_accuracy: 0.7779 - lr: 1.0000e-04\n",
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0560 - accuracy: 0.8273\n",
      "Epoch 00072: val_accuracy did not improve from 0.78123\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.0580 - accuracy: 0.8272 - val_loss: 1.1880 - val_accuracy: 0.7776 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0544 - accuracy: 0.8257\n",
      "Epoch 00073: val_accuracy improved from 0.78123 to 0.78200, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 1.0544 - accuracy: 0.8257 - val_loss: 1.1786 - val_accuracy: 0.7820 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0503 - accuracy: 0.8251\n",
      "Epoch 00074: val_accuracy did not improve from 0.78200\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1.0503 - accuracy: 0.8251 - val_loss: 1.1778 - val_accuracy: 0.7810 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0460 - accuracy: 0.8302\n",
      "Epoch 00075: val_accuracy did not improve from 0.78200\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1.0460 - accuracy: 0.8302 - val_loss: 1.1752 - val_accuracy: 0.7776 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0376 - accuracy: 0.8280\n",
      "Epoch 00076: val_accuracy did not improve from 0.78200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.0393 - accuracy: 0.8270 - val_loss: 1.1782 - val_accuracy: 0.7820 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0358 - accuracy: 0.8249\n",
      "Epoch 00077: val_accuracy did not improve from 0.78200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0368 - accuracy: 0.8238 - val_loss: 1.1795 - val_accuracy: 0.7792 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0267 - accuracy: 0.8342\n",
      "Epoch 00078: val_accuracy did not improve from 0.78200\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1.0270 - accuracy: 0.8340 - val_loss: 1.1731 - val_accuracy: 0.7781 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0338 - accuracy: 0.8280\n",
      "Epoch 00079: val_accuracy did not improve from 0.78200\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 1.0338 - accuracy: 0.8280 - val_loss: 1.1685 - val_accuracy: 0.7799 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0280 - accuracy: 0.8323\n",
      "Epoch 00080: val_accuracy did not improve from 0.78200\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 1.0295 - accuracy: 0.8316 - val_loss: 1.1657 - val_accuracy: 0.7820 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0218 - accuracy: 0.8363\n",
      "Epoch 00081: val_accuracy did not improve from 0.78200\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1.0220 - accuracy: 0.8360 - val_loss: 1.1628 - val_accuracy: 0.7805 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0110 - accuracy: 0.8378\n",
      "Epoch 00082: val_accuracy did not improve from 0.78200\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0102 - accuracy: 0.8381 - val_loss: 1.1633 - val_accuracy: 0.7807 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0091 - accuracy: 0.8389\n",
      "Epoch 00083: val_accuracy did not improve from 0.78200\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.0096 - accuracy: 0.8390 - val_loss: 1.1628 - val_accuracy: 0.7805 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0125 - accuracy: 0.8372\n",
      "Epoch 00084: val_accuracy did not improve from 0.78200\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.0138 - accuracy: 0.8366 - val_loss: 1.1627 - val_accuracy: 0.7815 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0087 - accuracy: 0.8359\n",
      "Epoch 00085: val_accuracy did not improve from 0.78200\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1.0087 - accuracy: 0.8359 - val_loss: 1.1618 - val_accuracy: 0.7797 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0122 - accuracy: 0.8373\n",
      "Epoch 00086: val_accuracy improved from 0.78200 to 0.78353, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 1.0127 - accuracy: 0.8368 - val_loss: 1.1615 - val_accuracy: 0.7835 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0199 - accuracy: 0.8334\n",
      "Epoch 00087: val_accuracy improved from 0.78353 to 0.78405, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 1.0206 - accuracy: 0.8331 - val_loss: 1.1607 - val_accuracy: 0.7840 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0055 - accuracy: 0.8376\n",
      "Epoch 00088: val_accuracy did not improve from 0.78405\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.0041 - accuracy: 0.8382 - val_loss: 1.1619 - val_accuracy: 0.7820 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0147 - accuracy: 0.8337\n",
      "Epoch 00089: val_accuracy did not improve from 0.78405\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0177 - accuracy: 0.8328 - val_loss: 1.1607 - val_accuracy: 0.7830 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0019 - accuracy: 0.8401\n",
      "Epoch 00090: val_accuracy did not improve from 0.78405\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1.0019 - accuracy: 0.8401 - val_loss: 1.1600 - val_accuracy: 0.7807 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0177 - accuracy: 0.8329\n",
      "Epoch 00091: val_accuracy improved from 0.78405 to 0.78533, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 1.0183 - accuracy: 0.8326 - val_loss: 1.1587 - val_accuracy: 0.7853 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0063 - accuracy: 0.8348\n",
      "Epoch 00092: val_accuracy did not improve from 0.78533\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.0063 - accuracy: 0.8348 - val_loss: 1.1593 - val_accuracy: 0.7853 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0109 - accuracy: 0.8329\n",
      "Epoch 00093: val_accuracy improved from 0.78533 to 0.78636, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.0103 - accuracy: 0.8332 - val_loss: 1.1586 - val_accuracy: 0.7864 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0069 - accuracy: 0.8380\n",
      "Epoch 00094: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0061 - accuracy: 0.8386 - val_loss: 1.1599 - val_accuracy: 0.7833 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0117 - accuracy: 0.8370\n",
      "Epoch 00095: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1.0134 - accuracy: 0.8368 - val_loss: 1.1583 - val_accuracy: 0.7843 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0015 - accuracy: 0.8421\n",
      "Epoch 00096: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0029 - accuracy: 0.8415 - val_loss: 1.1606 - val_accuracy: 0.7820 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0004 - accuracy: 0.8414\n",
      "Epoch 00097: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.0013 - accuracy: 0.8411 - val_loss: 1.1603 - val_accuracy: 0.7825 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0021 - accuracy: 0.8410\n",
      "Epoch 00098: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0047 - accuracy: 0.8398 - val_loss: 1.1632 - val_accuracy: 0.7810 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9968 - accuracy: 0.8413\n",
      "Epoch 00099: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.9969 - accuracy: 0.8413 - val_loss: 1.1601 - val_accuracy: 0.7835 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0068 - accuracy: 0.8431\n",
      "Epoch 00100: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0068 - accuracy: 0.8431 - val_loss: 1.1607 - val_accuracy: 0.7833 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0054 - accuracy: 0.8393\n",
      "Epoch 00101: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0062 - accuracy: 0.8395 - val_loss: 1.1611 - val_accuracy: 0.7840 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0091 - accuracy: 0.8352\n",
      "Epoch 00102: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0090 - accuracy: 0.8351 - val_loss: 1.1606 - val_accuracy: 0.7833 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0007 - accuracy: 0.8391\n",
      "Epoch 00103: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.0007 - accuracy: 0.8391 - val_loss: 1.1612 - val_accuracy: 0.7825 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9998 - accuracy: 0.8389\n",
      "Epoch 00104: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.0024 - accuracy: 0.8383 - val_loss: 1.1569 - val_accuracy: 0.7835 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9946 - accuracy: 0.8390\n",
      "Epoch 00105: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.9962 - accuracy: 0.8390 - val_loss: 1.1573 - val_accuracy: 0.7817 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9996 - accuracy: 0.8390\n",
      "Epoch 00106: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.9996 - accuracy: 0.8390 - val_loss: 1.1574 - val_accuracy: 0.7828 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9950 - accuracy: 0.8381\n",
      "Epoch 00107: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.9950 - accuracy: 0.8381 - val_loss: 1.1584 - val_accuracy: 0.7820 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0053 - accuracy: 0.8403\n",
      "Epoch 00108: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.0048 - accuracy: 0.8412 - val_loss: 1.1575 - val_accuracy: 0.7843 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0004 - accuracy: 0.8368\n",
      "Epoch 00109: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0004 - accuracy: 0.8369 - val_loss: 1.1572 - val_accuracy: 0.7833 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9936 - accuracy: 0.8415\n",
      "Epoch 00110: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.9936 - accuracy: 0.8415 - val_loss: 1.1561 - val_accuracy: 0.7838 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9985 - accuracy: 0.8405\n",
      "Epoch 00111: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.9985 - accuracy: 0.8405 - val_loss: 1.1571 - val_accuracy: 0.7828 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0009 - accuracy: 0.8380\n",
      "Epoch 00112: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9999 - accuracy: 0.8387 - val_loss: 1.1574 - val_accuracy: 0.7840 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0055 - accuracy: 0.8399\n",
      "Epoch 00113: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0050 - accuracy: 0.8404 - val_loss: 1.1577 - val_accuracy: 0.7835 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9888 - accuracy: 0.8464\n",
      "Epoch 00114: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.9907 - accuracy: 0.8461 - val_loss: 1.1562 - val_accuracy: 0.7856 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9980 - accuracy: 0.8410\n",
      "Epoch 00115: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.9994 - accuracy: 0.8405 - val_loss: 1.1559 - val_accuracy: 0.7825 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9957 - accuracy: 0.8369\n",
      "Epoch 00116: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9979 - accuracy: 0.8365 - val_loss: 1.1566 - val_accuracy: 0.7812 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9976 - accuracy: 0.8427\n",
      "Epoch 00117: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.9972 - accuracy: 0.8432 - val_loss: 1.1572 - val_accuracy: 0.7817 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9915 - accuracy: 0.8434\n",
      "Epoch 00118: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9908 - accuracy: 0.8437 - val_loss: 1.1571 - val_accuracy: 0.7797 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9898 - accuracy: 0.8452\n",
      "Epoch 00119: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9908 - accuracy: 0.8453 - val_loss: 1.1570 - val_accuracy: 0.7823 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9928 - accuracy: 0.8386\n",
      "Epoch 00120: val_accuracy did not improve from 0.78636\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.9918 - accuracy: 0.8387 - val_loss: 1.1556 - val_accuracy: 0.7817 - lr: 1.0000e-06\n",
      "epoch_number 93\n",
      "train accuracy and validation accuracy 0.8331685066223145 0.7863554954528809\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.1586 - accuracy: 0.7864\n",
      "test_accuracy 0.7863554954528809\n",
      "[0.7863554954528809]\n",
      "0.7863554954528809\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S2_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S2_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 4.7857 - accuracy: 0.1412\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.18620, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 384ms/step - loss: 4.7857 - accuracy: 0.1412 - val_loss: 4.6413 - val_accuracy: 0.1862 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.2246 - accuracy: 0.2894\n",
      "Epoch 00002: val_accuracy improved from 0.18620 to 0.36086, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 3.2258 - accuracy: 0.2901 - val_loss: 2.9333 - val_accuracy: 0.3609 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.8651 - accuracy: 0.3731\n",
      "Epoch 00003: val_accuracy improved from 0.36086 to 0.41370, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 2.8624 - accuracy: 0.3733 - val_loss: 2.6954 - val_accuracy: 0.4137 - lr: 0.0010\n",
      "Epoch 4/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.6358 - accuracy: 0.4308\n",
      "Epoch 00004: val_accuracy improved from 0.41370 to 0.44319, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 2.6358 - accuracy: 0.4308 - val_loss: 2.5470 - val_accuracy: 0.4432 - lr: 0.0010\n",
      "Epoch 5/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.5052 - accuracy: 0.4605\n",
      "Epoch 00005: val_accuracy improved from 0.44319 to 0.46166, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 2.5052 - accuracy: 0.4605 - val_loss: 2.4678 - val_accuracy: 0.4617 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.3706 - accuracy: 0.4961\n",
      "Epoch 00006: val_accuracy improved from 0.46166 to 0.48628, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 2.3732 - accuracy: 0.4965 - val_loss: 2.3867 - val_accuracy: 0.4863 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2695 - accuracy: 0.5126\n",
      "Epoch 00007: val_accuracy improved from 0.48628 to 0.49885, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 2.2697 - accuracy: 0.5127 - val_loss: 2.3009 - val_accuracy: 0.4988 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1883 - accuracy: 0.5293\n",
      "Epoch 00008: val_accuracy improved from 0.49885 to 0.50987, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 2.1900 - accuracy: 0.5287 - val_loss: 2.2688 - val_accuracy: 0.5099 - lr: 0.0010\n",
      "Epoch 9/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.1335 - accuracy: 0.5414\n",
      "Epoch 00009: val_accuracy improved from 0.50987 to 0.51167, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 2.1335 - accuracy: 0.5414 - val_loss: 2.2537 - val_accuracy: 0.5117 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0681 - accuracy: 0.5527\n",
      "Epoch 00010: val_accuracy improved from 0.51167 to 0.52321, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 2.0703 - accuracy: 0.5523 - val_loss: 2.1876 - val_accuracy: 0.5232 - lr: 0.0010\n",
      "Epoch 11/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.0133 - accuracy: 0.5678\n",
      "Epoch 00011: val_accuracy improved from 0.52321 to 0.52988, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 2.0133 - accuracy: 0.5678 - val_loss: 2.1448 - val_accuracy: 0.5299 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9511 - accuracy: 0.5799\n",
      "Epoch 00012: val_accuracy improved from 0.52988 to 0.53603, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.9515 - accuracy: 0.5792 - val_loss: 2.1413 - val_accuracy: 0.5360 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9066 - accuracy: 0.5889\n",
      "Epoch 00013: val_accuracy improved from 0.53603 to 0.54193, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.9083 - accuracy: 0.5879 - val_loss: 2.1264 - val_accuracy: 0.5419 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8696 - accuracy: 0.6011\n",
      "Epoch 00014: val_accuracy did not improve from 0.54193\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.8703 - accuracy: 0.6011 - val_loss: 2.1220 - val_accuracy: 0.5378 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8227 - accuracy: 0.6103\n",
      "Epoch 00015: val_accuracy did not improve from 0.54193\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.8272 - accuracy: 0.6093 - val_loss: 2.0989 - val_accuracy: 0.5417 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8049 - accuracy: 0.6134\n",
      "Epoch 00016: val_accuracy did not improve from 0.54193\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.8085 - accuracy: 0.6130 - val_loss: 2.0815 - val_accuracy: 0.5404 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7853 - accuracy: 0.6106\n",
      "Epoch 00017: val_accuracy improved from 0.54193 to 0.55501, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.7859 - accuracy: 0.6106 - val_loss: 2.0540 - val_accuracy: 0.5550 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7156 - accuracy: 0.6346\n",
      "Epoch 00018: val_accuracy did not improve from 0.55501\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.7186 - accuracy: 0.6339 - val_loss: 2.0691 - val_accuracy: 0.5499 - lr: 0.0010\n",
      "Epoch 19/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.7046 - accuracy: 0.6317\n",
      "Epoch 00019: val_accuracy improved from 0.55501 to 0.56322, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.7046 - accuracy: 0.6317 - val_loss: 2.0280 - val_accuracy: 0.5632 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6595 - accuracy: 0.6431\n",
      "Epoch 00020: val_accuracy did not improve from 0.56322\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.6614 - accuracy: 0.6425 - val_loss: 2.0350 - val_accuracy: 0.5617 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6373 - accuracy: 0.6521\n",
      "Epoch 00021: val_accuracy improved from 0.56322 to 0.56579, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.6374 - accuracy: 0.6519 - val_loss: 1.9894 - val_accuracy: 0.5658 - lr: 0.0010\n",
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6147 - accuracy: 0.6557\n",
      "Epoch 00022: val_accuracy did not improve from 0.56579\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.6160 - accuracy: 0.6549 - val_loss: 1.9950 - val_accuracy: 0.5645 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6071 - accuracy: 0.6629\n",
      "Epoch 00023: val_accuracy did not improve from 0.56579\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.6073 - accuracy: 0.6623 - val_loss: 1.9990 - val_accuracy: 0.5578 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5863 - accuracy: 0.6612\n",
      "Epoch 00024: val_accuracy did not improve from 0.56579\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.5843 - accuracy: 0.6618 - val_loss: 1.9981 - val_accuracy: 0.5653 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5616 - accuracy: 0.6683\n",
      "Epoch 00025: val_accuracy improved from 0.56579 to 0.56938, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.5613 - accuracy: 0.6683 - val_loss: 1.9846 - val_accuracy: 0.5694 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5307 - accuracy: 0.6759\n",
      "Epoch 00026: val_accuracy improved from 0.56938 to 0.57194, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.5318 - accuracy: 0.6755 - val_loss: 1.9610 - val_accuracy: 0.5719 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5105 - accuracy: 0.6804\n",
      "Epoch 00027: val_accuracy did not improve from 0.57194\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.5112 - accuracy: 0.6804 - val_loss: 1.9900 - val_accuracy: 0.5696 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5080 - accuracy: 0.6842\n",
      "Epoch 00028: val_accuracy did not improve from 0.57194\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.5155 - accuracy: 0.6818 - val_loss: 2.0018 - val_accuracy: 0.5599 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5421 - accuracy: 0.6741\n",
      "Epoch 00029: val_accuracy did not improve from 0.57194\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.5424 - accuracy: 0.6741 - val_loss: 1.9762 - val_accuracy: 0.5645 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5116 - accuracy: 0.6777\n",
      "Epoch 00030: val_accuracy did not improve from 0.57194\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.5100 - accuracy: 0.6778 - val_loss: 1.9810 - val_accuracy: 0.5681 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4473 - accuracy: 0.6960\n",
      "Epoch 00031: val_accuracy improved from 0.57194 to 0.57502, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.4494 - accuracy: 0.6954 - val_loss: 1.9455 - val_accuracy: 0.5750 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4288 - accuracy: 0.7031\n",
      "Epoch 00032: val_accuracy improved from 0.57502 to 0.57579, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 1.4306 - accuracy: 0.7024 - val_loss: 1.9512 - val_accuracy: 0.5758 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4330 - accuracy: 0.6988\n",
      "Epoch 00033: val_accuracy did not improve from 0.57579\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 1.4341 - accuracy: 0.6983 - val_loss: 1.9668 - val_accuracy: 0.5753 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4177 - accuracy: 0.7018\n",
      "Epoch 00034: val_accuracy improved from 0.57579 to 0.57938, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 1.4176 - accuracy: 0.7015 - val_loss: 1.9169 - val_accuracy: 0.5794 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3971 - accuracy: 0.7059\n",
      "Epoch 00035: val_accuracy did not improve from 0.57938\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 1.3956 - accuracy: 0.7060 - val_loss: 1.9325 - val_accuracy: 0.5684 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3540 - accuracy: 0.7207\n",
      "Epoch 00036: val_accuracy did not improve from 0.57938\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.3519 - accuracy: 0.7213 - val_loss: 1.9099 - val_accuracy: 0.5784 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3300 - accuracy: 0.7206\n",
      "Epoch 00037: val_accuracy did not improve from 0.57938\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.3295 - accuracy: 0.7205 - val_loss: 1.9149 - val_accuracy: 0.5781 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3293 - accuracy: 0.7251\n",
      "Epoch 00038: val_accuracy improved from 0.57938 to 0.58066, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1.3299 - accuracy: 0.7252 - val_loss: 1.9336 - val_accuracy: 0.5807 - lr: 0.0010\n",
      "Epoch 39/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3225 - accuracy: 0.7254\n",
      "Epoch 00039: val_accuracy improved from 0.58066 to 0.58246, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.3225 - accuracy: 0.7254 - val_loss: 1.9401 - val_accuracy: 0.5825 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2838 - accuracy: 0.7356\n",
      "Epoch 00040: val_accuracy improved from 0.58246 to 0.58451, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.2840 - accuracy: 0.7356 - val_loss: 1.8938 - val_accuracy: 0.5845 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2512 - accuracy: 0.7490\n",
      "Epoch 00041: val_accuracy did not improve from 0.58451\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.2496 - accuracy: 0.7494 - val_loss: 1.8858 - val_accuracy: 0.5809 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2261 - accuracy: 0.7479\n",
      "Epoch 00042: val_accuracy improved from 0.58451 to 0.58528, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.2271 - accuracy: 0.7481 - val_loss: 1.8811 - val_accuracy: 0.5853 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2143 - accuracy: 0.7528\n",
      "Epoch 00043: val_accuracy improved from 0.58528 to 0.58656, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 1.2196 - accuracy: 0.7511 - val_loss: 1.8772 - val_accuracy: 0.5866 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1990 - accuracy: 0.7559\n",
      "Epoch 00044: val_accuracy did not improve from 0.58656\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.1990 - accuracy: 0.7559 - val_loss: 1.8784 - val_accuracy: 0.5855 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1963 - accuracy: 0.7589\n",
      "Epoch 00045: val_accuracy improved from 0.58656 to 0.58682, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.1997 - accuracy: 0.7577 - val_loss: 1.8774 - val_accuracy: 0.5868 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1880 - accuracy: 0.7632\n",
      "Epoch 00046: val_accuracy did not improve from 0.58682\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.1905 - accuracy: 0.7627 - val_loss: 1.8834 - val_accuracy: 0.5843 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1743 - accuracy: 0.7688\n",
      "Epoch 00047: val_accuracy improved from 0.58682 to 0.58913, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.1738 - accuracy: 0.7693 - val_loss: 1.8778 - val_accuracy: 0.5891 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1765 - accuracy: 0.7658\n",
      "Epoch 00048: val_accuracy improved from 0.58913 to 0.59066, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.1765 - accuracy: 0.7658 - val_loss: 1.8735 - val_accuracy: 0.5907 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1683 - accuracy: 0.7690\n",
      "Epoch 00049: val_accuracy did not improve from 0.59066\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.1681 - accuracy: 0.7693 - val_loss: 1.8734 - val_accuracy: 0.5873 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1595 - accuracy: 0.7699\n",
      "Epoch 00050: val_accuracy improved from 0.59066 to 0.59272, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.1617 - accuracy: 0.7694 - val_loss: 1.8739 - val_accuracy: 0.5927 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1625 - accuracy: 0.7693\n",
      "Epoch 00051: val_accuracy did not improve from 0.59272\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.1645 - accuracy: 0.7688 - val_loss: 1.8752 - val_accuracy: 0.5925 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1643 - accuracy: 0.7669\n",
      "Epoch 00052: val_accuracy did not improve from 0.59272\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.1669 - accuracy: 0.7660 - val_loss: 1.8801 - val_accuracy: 0.5891 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1352 - accuracy: 0.7727\n",
      "Epoch 00053: val_accuracy improved from 0.59272 to 0.59323, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 1.1362 - accuracy: 0.7722 - val_loss: 1.8664 - val_accuracy: 0.5932 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1338 - accuracy: 0.7790\n",
      "Epoch 00054: val_accuracy did not improve from 0.59323\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 1.1348 - accuracy: 0.7789 - val_loss: 1.8664 - val_accuracy: 0.5909 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1330 - accuracy: 0.7741\n",
      "Epoch 00055: val_accuracy did not improve from 0.59323\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.1349 - accuracy: 0.7739 - val_loss: 1.8689 - val_accuracy: 0.5904 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1343 - accuracy: 0.7772\n",
      "Epoch 00056: val_accuracy improved from 0.59323 to 0.59528, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.1353 - accuracy: 0.7768 - val_loss: 1.8619 - val_accuracy: 0.5953 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1238 - accuracy: 0.7800\n",
      "Epoch 00057: val_accuracy did not improve from 0.59528\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.1253 - accuracy: 0.7796 - val_loss: 1.8731 - val_accuracy: 0.5902 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1232 - accuracy: 0.7801\n",
      "Epoch 00058: val_accuracy did not improve from 0.59528\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.1220 - accuracy: 0.7806 - val_loss: 1.8567 - val_accuracy: 0.5937 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0995 - accuracy: 0.7870\n",
      "Epoch 00059: val_accuracy did not improve from 0.59528\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 1.1021 - accuracy: 0.7861 - val_loss: 1.8641 - val_accuracy: 0.5945 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1145 - accuracy: 0.7813\n",
      "Epoch 00060: val_accuracy did not improve from 0.59528\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.1137 - accuracy: 0.7814 - val_loss: 1.8629 - val_accuracy: 0.5935 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1047 - accuracy: 0.7846\n",
      "Epoch 00061: val_accuracy improved from 0.59528 to 0.59656, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.1047 - accuracy: 0.7846 - val_loss: 1.8651 - val_accuracy: 0.5966 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0996 - accuracy: 0.7848\n",
      "Epoch 00062: val_accuracy did not improve from 0.59656\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.1024 - accuracy: 0.7839 - val_loss: 1.8704 - val_accuracy: 0.5948 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1000 - accuracy: 0.7830\n",
      "Epoch 00063: val_accuracy improved from 0.59656 to 0.59836, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.1028 - accuracy: 0.7825 - val_loss: 1.8704 - val_accuracy: 0.5984 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1082 - accuracy: 0.7811\n",
      "Epoch 00064: val_accuracy did not improve from 0.59836\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.1067 - accuracy: 0.7817 - val_loss: 1.8698 - val_accuracy: 0.5976 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0966 - accuracy: 0.7871\n",
      "Epoch 00065: val_accuracy did not improve from 0.59836\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.1013 - accuracy: 0.7860 - val_loss: 1.8680 - val_accuracy: 0.5968 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0996 - accuracy: 0.7815\n",
      "Epoch 00066: val_accuracy did not improve from 0.59836\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0996 - accuracy: 0.7815 - val_loss: 1.8771 - val_accuracy: 0.5945 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0795 - accuracy: 0.7872\n",
      "Epoch 00067: val_accuracy did not improve from 0.59836\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0833 - accuracy: 0.7858 - val_loss: 1.8699 - val_accuracy: 0.5953 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0821 - accuracy: 0.7876\n",
      "Epoch 00068: val_accuracy did not improve from 0.59836\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0829 - accuracy: 0.7871 - val_loss: 1.8761 - val_accuracy: 0.5968 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0809 - accuracy: 0.7891\n",
      "Epoch 00069: val_accuracy did not improve from 0.59836\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0849 - accuracy: 0.7880 - val_loss: 1.8738 - val_accuracy: 0.5943 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0705 - accuracy: 0.7937\n",
      "Epoch 00070: val_accuracy did not improve from 0.59836\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0727 - accuracy: 0.7928 - val_loss: 1.8704 - val_accuracy: 0.5961 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0802 - accuracy: 0.7881\n",
      "Epoch 00071: val_accuracy did not improve from 0.59836\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.0823 - accuracy: 0.7874 - val_loss: 1.8667 - val_accuracy: 0.5966 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0667 - accuracy: 0.7949\n",
      "Epoch 00072: val_accuracy did not improve from 0.59836\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0681 - accuracy: 0.7947 - val_loss: 1.8620 - val_accuracy: 0.5963 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0792 - accuracy: 0.7894\n",
      "Epoch 00073: val_accuracy did not improve from 0.59836\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.0789 - accuracy: 0.7894 - val_loss: 1.8691 - val_accuracy: 0.5948 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0670 - accuracy: 0.7963\n",
      "Epoch 00074: val_accuracy did not improve from 0.59836\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0683 - accuracy: 0.7956 - val_loss: 1.8665 - val_accuracy: 0.5971 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0601 - accuracy: 0.7949\n",
      "Epoch 00075: val_accuracy did not improve from 0.59836\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.0598 - accuracy: 0.7950 - val_loss: 1.8641 - val_accuracy: 0.5958 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0550 - accuracy: 0.7929\n",
      "Epoch 00076: val_accuracy did not improve from 0.59836\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0550 - accuracy: 0.7929 - val_loss: 1.8589 - val_accuracy: 0.5978 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0637 - accuracy: 0.7883\n",
      "Epoch 00077: val_accuracy did not improve from 0.59836\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1.0668 - accuracy: 0.7874 - val_loss: 1.8531 - val_accuracy: 0.5973 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0648 - accuracy: 0.7950\n",
      "Epoch 00078: val_accuracy improved from 0.59836 to 0.59964, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.0663 - accuracy: 0.7947 - val_loss: 1.8535 - val_accuracy: 0.5996 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0543 - accuracy: 0.7973\n",
      "Epoch 00079: val_accuracy did not improve from 0.59964\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0543 - accuracy: 0.7973 - val_loss: 1.8572 - val_accuracy: 0.5966 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0350 - accuracy: 0.7970\n",
      "Epoch 00080: val_accuracy did not improve from 0.59964\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.0347 - accuracy: 0.7971 - val_loss: 1.8561 - val_accuracy: 0.5953 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0436 - accuracy: 0.7978\n",
      "Epoch 00081: val_accuracy did not improve from 0.59964\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.0449 - accuracy: 0.7971 - val_loss: 1.8553 - val_accuracy: 0.5978 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0369 - accuracy: 0.7982\n",
      "Epoch 00082: val_accuracy did not improve from 0.59964\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0391 - accuracy: 0.7977 - val_loss: 1.8612 - val_accuracy: 0.5991 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0396 - accuracy: 0.7981\n",
      "Epoch 00083: val_accuracy improved from 0.59964 to 0.60067, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.0393 - accuracy: 0.7985 - val_loss: 1.8605 - val_accuracy: 0.6007 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0418 - accuracy: 0.7949\n",
      "Epoch 00084: val_accuracy did not improve from 0.60067\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.0423 - accuracy: 0.7947 - val_loss: 1.8527 - val_accuracy: 0.5984 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0343 - accuracy: 0.8038\n",
      "Epoch 00085: val_accuracy did not improve from 0.60067\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1.0351 - accuracy: 0.8038 - val_loss: 1.8515 - val_accuracy: 0.5955 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0354 - accuracy: 0.8002\n",
      "Epoch 00086: val_accuracy did not improve from 0.60067\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.0368 - accuracy: 0.7993 - val_loss: 1.8523 - val_accuracy: 0.5981 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0395 - accuracy: 0.7982\n",
      "Epoch 00087: val_accuracy did not improve from 0.60067\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0389 - accuracy: 0.7984 - val_loss: 1.8543 - val_accuracy: 0.5991 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0262 - accuracy: 0.8032\n",
      "Epoch 00088: val_accuracy did not improve from 0.60067\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.0262 - accuracy: 0.8032 - val_loss: 1.8547 - val_accuracy: 0.5991 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0419 - accuracy: 0.7984\n",
      "Epoch 00089: val_accuracy did not improve from 0.60067\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0419 - accuracy: 0.7984 - val_loss: 1.8577 - val_accuracy: 0.5991 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0289 - accuracy: 0.8049\n",
      "Epoch 00090: val_accuracy did not improve from 0.60067\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0316 - accuracy: 0.8040 - val_loss: 1.8564 - val_accuracy: 0.5989 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0253 - accuracy: 0.8068\n",
      "Epoch 00091: val_accuracy improved from 0.60067 to 0.60092, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 1.0265 - accuracy: 0.8062 - val_loss: 1.8557 - val_accuracy: 0.6009 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0282 - accuracy: 0.8029\n",
      "Epoch 00092: val_accuracy did not improve from 0.60092\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.0286 - accuracy: 0.8032 - val_loss: 1.8557 - val_accuracy: 0.6004 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0464 - accuracy: 0.7998\n",
      "Epoch 00093: val_accuracy did not improve from 0.60092\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0465 - accuracy: 0.8004 - val_loss: 1.8587 - val_accuracy: 0.5968 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0178 - accuracy: 0.8066\n",
      "Epoch 00094: val_accuracy did not improve from 0.60092\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0196 - accuracy: 0.8064 - val_loss: 1.8570 - val_accuracy: 0.5984 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0278 - accuracy: 0.8019\n",
      "Epoch 00095: val_accuracy did not improve from 0.60092\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.0291 - accuracy: 0.8015 - val_loss: 1.8526 - val_accuracy: 0.5984 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0307 - accuracy: 0.8004\n",
      "Epoch 00096: val_accuracy did not improve from 0.60092\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.0307 - accuracy: 0.8004 - val_loss: 1.8530 - val_accuracy: 0.5986 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0371 - accuracy: 0.8010\n",
      "Epoch 00097: val_accuracy did not improve from 0.60092\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0383 - accuracy: 0.8002 - val_loss: 1.8542 - val_accuracy: 0.6009 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0384 - accuracy: 0.7966\n",
      "Epoch 00098: val_accuracy did not improve from 0.60092\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0384 - accuracy: 0.7966 - val_loss: 1.8559 - val_accuracy: 0.5986 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0390 - accuracy: 0.7978\n",
      "Epoch 00099: val_accuracy did not improve from 0.60092\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0384 - accuracy: 0.7981 - val_loss: 1.8575 - val_accuracy: 0.5973 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0397 - accuracy: 0.7991\n",
      "Epoch 00100: val_accuracy did not improve from 0.60092\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0389 - accuracy: 0.7991 - val_loss: 1.8597 - val_accuracy: 0.5937 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0297 - accuracy: 0.7970\n",
      "Epoch 00101: val_accuracy did not improve from 0.60092\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0289 - accuracy: 0.7977 - val_loss: 1.8572 - val_accuracy: 0.5955 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0315 - accuracy: 0.8004\n",
      "Epoch 00102: val_accuracy did not improve from 0.60092\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0325 - accuracy: 0.8002 - val_loss: 1.8539 - val_accuracy: 0.5973 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0308 - accuracy: 0.8019\n",
      "Epoch 00103: val_accuracy did not improve from 0.60092\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0333 - accuracy: 0.8012 - val_loss: 1.8523 - val_accuracy: 0.5996 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0267 - accuracy: 0.8023\n",
      "Epoch 00104: val_accuracy did not improve from 0.60092\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0271 - accuracy: 0.8017 - val_loss: 1.8541 - val_accuracy: 0.5989 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0251 - accuracy: 0.8043\n",
      "Epoch 00105: val_accuracy did not improve from 0.60092\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0251 - accuracy: 0.8043 - val_loss: 1.8567 - val_accuracy: 0.6002 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0255 - accuracy: 0.8023\n",
      "Epoch 00106: val_accuracy improved from 0.60092 to 0.60144, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.0257 - accuracy: 0.8023 - val_loss: 1.8548 - val_accuracy: 0.6014 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0355 - accuracy: 0.7975\n",
      "Epoch 00107: val_accuracy did not improve from 0.60144\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.0355 - accuracy: 0.7975 - val_loss: 1.8595 - val_accuracy: 0.5973 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0384 - accuracy: 0.7984\n",
      "Epoch 00108: val_accuracy did not improve from 0.60144\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.0384 - accuracy: 0.7984 - val_loss: 1.8579 - val_accuracy: 0.5991 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0056 - accuracy: 0.8151\n",
      "Epoch 00109: val_accuracy did not improve from 0.60144\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0079 - accuracy: 0.8143 - val_loss: 1.8568 - val_accuracy: 0.5989 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0260 - accuracy: 0.8051\n",
      "Epoch 00110: val_accuracy did not improve from 0.60144\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0259 - accuracy: 0.8051 - val_loss: 1.8552 - val_accuracy: 0.5981 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0293 - accuracy: 0.8049\n",
      "Epoch 00111: val_accuracy did not improve from 0.60144\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.0293 - accuracy: 0.8049 - val_loss: 1.8534 - val_accuracy: 0.5966 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0264 - accuracy: 0.8028\n",
      "Epoch 00112: val_accuracy did not improve from 0.60144\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.0264 - accuracy: 0.8028 - val_loss: 1.8578 - val_accuracy: 0.5978 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0159 - accuracy: 0.8013\n",
      "Epoch 00113: val_accuracy did not improve from 0.60144\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1.0177 - accuracy: 0.8007 - val_loss: 1.8512 - val_accuracy: 0.5999 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0306 - accuracy: 0.8002\n",
      "Epoch 00114: val_accuracy did not improve from 0.60144\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.0306 - accuracy: 0.8002 - val_loss: 1.8558 - val_accuracy: 0.6004 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0183 - accuracy: 0.8027\n",
      "Epoch 00115: val_accuracy did not improve from 0.60144\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0183 - accuracy: 0.8027 - val_loss: 1.8555 - val_accuracy: 0.5984 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0311 - accuracy: 0.7999\n",
      "Epoch 00116: val_accuracy did not improve from 0.60144\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0304 - accuracy: 0.8000 - val_loss: 1.8563 - val_accuracy: 0.5989 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0187 - accuracy: 0.8040\n",
      "Epoch 00117: val_accuracy did not improve from 0.60144\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0208 - accuracy: 0.8033 - val_loss: 1.8574 - val_accuracy: 0.5971 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0153 - accuracy: 0.8067\n",
      "Epoch 00118: val_accuracy did not improve from 0.60144\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0153 - accuracy: 0.8067 - val_loss: 1.8558 - val_accuracy: 0.5973 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0202 - accuracy: 0.8050\n",
      "Epoch 00119: val_accuracy did not improve from 0.60144\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.0202 - accuracy: 0.8050 - val_loss: 1.8538 - val_accuracy: 0.5989 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0212 - accuracy: 0.8053\n",
      "Epoch 00120: val_accuracy did not improve from 0.60144\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0221 - accuracy: 0.8050 - val_loss: 1.8570 - val_accuracy: 0.5968 - lr: 1.0000e-06\n",
      "epoch_number 106\n",
      "train accuracy and validation accuracy 0.8022859692573547 0.6014362573623657\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.8548 - accuracy: 0.6014\n",
      "test_accuracy 0.6014362573623657\n",
      "[0.7863554954528809, 0.6014362573623657]\n",
      "0.6938958764076233\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S3_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S3_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.4459 - accuracy: 0.1667\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.31290, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 363ms/step - loss: 4.4334 - accuracy: 0.1679 - val_loss: 3.0710 - val_accuracy: 0.3129 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.9604 - accuracy: 0.3217\n",
      "Epoch 00002: val_accuracy improved from 0.31290 to 0.42985, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 2.9593 - accuracy: 0.3218 - val_loss: 2.5183 - val_accuracy: 0.4299 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.6050 - accuracy: 0.4082\n",
      "Epoch 00003: val_accuracy improved from 0.42985 to 0.49064, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 2.6064 - accuracy: 0.4084 - val_loss: 2.2983 - val_accuracy: 0.4906 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4084 - accuracy: 0.4526\n",
      "Epoch 00004: val_accuracy improved from 0.49064 to 0.51962, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 2.4084 - accuracy: 0.4529 - val_loss: 2.1689 - val_accuracy: 0.5196 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2508 - accuracy: 0.4884\n",
      "Epoch 00005: val_accuracy improved from 0.51962 to 0.54424, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 2.2513 - accuracy: 0.4881 - val_loss: 2.0403 - val_accuracy: 0.5442 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1230 - accuracy: 0.5202\n",
      "Epoch 00006: val_accuracy improved from 0.54424 to 0.55424, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 130ms/step - loss: 2.1265 - accuracy: 0.5193 - val_loss: 1.9615 - val_accuracy: 0.5542 - lr: 0.0010\n",
      "Epoch 7/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.0331 - accuracy: 0.5372\n",
      "Epoch 00007: val_accuracy improved from 0.55424 to 0.57835, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 2.0331 - accuracy: 0.5372 - val_loss: 1.8871 - val_accuracy: 0.5784 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9918 - accuracy: 0.5461\n",
      "Epoch 00008: val_accuracy improved from 0.57835 to 0.58989, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.9934 - accuracy: 0.5458 - val_loss: 1.8406 - val_accuracy: 0.5899 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9249 - accuracy: 0.5642\n",
      "Epoch 00009: val_accuracy improved from 0.58989 to 0.60400, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.9247 - accuracy: 0.5643 - val_loss: 1.7795 - val_accuracy: 0.6040 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8642 - accuracy: 0.5814\n",
      "Epoch 00010: val_accuracy improved from 0.60400 to 0.61529, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.8650 - accuracy: 0.5809 - val_loss: 1.7597 - val_accuracy: 0.6153 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7979 - accuracy: 0.5932\n",
      "Epoch 00011: val_accuracy improved from 0.61529 to 0.61554, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.7986 - accuracy: 0.5930 - val_loss: 1.7219 - val_accuracy: 0.6155 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7412 - accuracy: 0.6092\n",
      "Epoch 00012: val_accuracy improved from 0.61554 to 0.63555, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.7414 - accuracy: 0.6090 - val_loss: 1.6683 - val_accuracy: 0.6355 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7095 - accuracy: 0.6101\n",
      "Epoch 00013: val_accuracy did not improve from 0.63555\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.7080 - accuracy: 0.6105 - val_loss: 1.6492 - val_accuracy: 0.6338 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6901 - accuracy: 0.6161\n",
      "Epoch 00014: val_accuracy improved from 0.63555 to 0.63965, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.6904 - accuracy: 0.6152 - val_loss: 1.6221 - val_accuracy: 0.6397 - lr: 0.0010\n",
      "Epoch 15/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.6563 - accuracy: 0.6257\n",
      "Epoch 00015: val_accuracy improved from 0.63965 to 0.64529, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 1.6563 - accuracy: 0.6257 - val_loss: 1.6053 - val_accuracy: 0.6453 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6043 - accuracy: 0.6416\n",
      "Epoch 00016: val_accuracy did not improve from 0.64529\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.6066 - accuracy: 0.6414 - val_loss: 1.5937 - val_accuracy: 0.6417 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5802 - accuracy: 0.6512\n",
      "Epoch 00017: val_accuracy improved from 0.64529 to 0.65299, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.5806 - accuracy: 0.6507 - val_loss: 1.5586 - val_accuracy: 0.6530 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5450 - accuracy: 0.6523\n",
      "Epoch 00018: val_accuracy did not improve from 0.65299\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.5451 - accuracy: 0.6524 - val_loss: 1.5665 - val_accuracy: 0.6514 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5285 - accuracy: 0.6588\n",
      "Epoch 00019: val_accuracy improved from 0.65299 to 0.66863, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 1.5268 - accuracy: 0.6591 - val_loss: 1.5308 - val_accuracy: 0.6686 - lr: 0.0010\n",
      "Epoch 20/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.4988 - accuracy: 0.6668\n",
      "Epoch 00020: val_accuracy did not improve from 0.66863\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.4988 - accuracy: 0.6668 - val_loss: 1.5420 - val_accuracy: 0.6571 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5017 - accuracy: 0.6676\n",
      "Epoch 00021: val_accuracy did not improve from 0.66863\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.5012 - accuracy: 0.6675 - val_loss: 1.5227 - val_accuracy: 0.6612 - lr: 0.0010\n",
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4788 - accuracy: 0.6684\n",
      "Epoch 00022: val_accuracy did not improve from 0.66863\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.4766 - accuracy: 0.6689 - val_loss: 1.5164 - val_accuracy: 0.6653 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4365 - accuracy: 0.6811\n",
      "Epoch 00023: val_accuracy did not improve from 0.66863\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.4340 - accuracy: 0.6819 - val_loss: 1.5184 - val_accuracy: 0.6663 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4302 - accuracy: 0.6856\n",
      "Epoch 00024: val_accuracy improved from 0.66863 to 0.67607, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.4295 - accuracy: 0.6860 - val_loss: 1.4857 - val_accuracy: 0.6761 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3999 - accuracy: 0.6922\n",
      "Epoch 00025: val_accuracy did not improve from 0.67607\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.4003 - accuracy: 0.6926 - val_loss: 1.4792 - val_accuracy: 0.6745 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3998 - accuracy: 0.6923\n",
      "Epoch 00026: val_accuracy did not improve from 0.67607\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 1.4015 - accuracy: 0.6918 - val_loss: 1.5190 - val_accuracy: 0.6653 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3942 - accuracy: 0.6884\n",
      "Epoch 00027: val_accuracy improved from 0.67607 to 0.67966, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.3935 - accuracy: 0.6882 - val_loss: 1.4681 - val_accuracy: 0.6797 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3667 - accuracy: 0.6971\n",
      "Epoch 00028: val_accuracy did not improve from 0.67966\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.3639 - accuracy: 0.6978 - val_loss: 1.4881 - val_accuracy: 0.6702 - lr: 0.0010\n",
      "Epoch 29/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3531 - accuracy: 0.6980\n",
      "Epoch 00029: val_accuracy did not improve from 0.67966\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.3531 - accuracy: 0.6980 - val_loss: 1.4640 - val_accuracy: 0.6738 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3308 - accuracy: 0.7082\n",
      "Epoch 00030: val_accuracy did not improve from 0.67966\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.3343 - accuracy: 0.7066 - val_loss: 1.5010 - val_accuracy: 0.6676 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3444 - accuracy: 0.7034\n",
      "Epoch 00031: val_accuracy improved from 0.67966 to 0.68351, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 1.3455 - accuracy: 0.7030 - val_loss: 1.4407 - val_accuracy: 0.6835 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3200 - accuracy: 0.7029\n",
      "Epoch 00032: val_accuracy did not improve from 0.68351\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 1.3168 - accuracy: 0.7037 - val_loss: 1.4348 - val_accuracy: 0.6812 - lr: 0.0010\n",
      "Epoch 33/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2830 - accuracy: 0.7189\n",
      "Epoch 00033: val_accuracy did not improve from 0.68351\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.2830 - accuracy: 0.7189 - val_loss: 1.4469 - val_accuracy: 0.6822 - lr: 0.0010\n",
      "Epoch 34/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2897 - accuracy: 0.7161\n",
      "Epoch 00034: val_accuracy did not improve from 0.68351\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.2897 - accuracy: 0.7161 - val_loss: 1.4437 - val_accuracy: 0.6781 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2729 - accuracy: 0.7169\n",
      "Epoch 00035: val_accuracy improved from 0.68351 to 0.68787, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.2749 - accuracy: 0.7161 - val_loss: 1.4306 - val_accuracy: 0.6879 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2534 - accuracy: 0.7231\n",
      "Epoch 00036: val_accuracy improved from 0.68787 to 0.69197, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.2570 - accuracy: 0.7229 - val_loss: 1.4106 - val_accuracy: 0.6920 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2596 - accuracy: 0.7219\n",
      "Epoch 00037: val_accuracy did not improve from 0.69197\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.2592 - accuracy: 0.7224 - val_loss: 1.4320 - val_accuracy: 0.6753 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2301 - accuracy: 0.7290\n",
      "Epoch 00038: val_accuracy did not improve from 0.69197\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.2310 - accuracy: 0.7287 - val_loss: 1.4038 - val_accuracy: 0.6907 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2172 - accuracy: 0.7318\n",
      "Epoch 00039: val_accuracy did not improve from 0.69197\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.2160 - accuracy: 0.7322 - val_loss: 1.4251 - val_accuracy: 0.6866 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1832 - accuracy: 0.7439\n",
      "Epoch 00040: val_accuracy improved from 0.69197 to 0.69761, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.1809 - accuracy: 0.7446 - val_loss: 1.3739 - val_accuracy: 0.6976 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1393 - accuracy: 0.7578\n",
      "Epoch 00041: val_accuracy improved from 0.69761 to 0.70326, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.1400 - accuracy: 0.7573 - val_loss: 1.3610 - val_accuracy: 0.7033 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1245 - accuracy: 0.7624\n",
      "Epoch 00042: val_accuracy improved from 0.70326 to 0.70454, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.1245 - accuracy: 0.7624 - val_loss: 1.3482 - val_accuracy: 0.7045 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1169 - accuracy: 0.7622\n",
      "Epoch 00043: val_accuracy improved from 0.70454 to 0.70582, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.1155 - accuracy: 0.7625 - val_loss: 1.3429 - val_accuracy: 0.7058 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1180 - accuracy: 0.7563\n",
      "Epoch 00044: val_accuracy improved from 0.70582 to 0.70916, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.1180 - accuracy: 0.7563 - val_loss: 1.3378 - val_accuracy: 0.7092 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0876 - accuracy: 0.7703\n",
      "Epoch 00045: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0913 - accuracy: 0.7694 - val_loss: 1.3406 - val_accuracy: 0.7056 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0912 - accuracy: 0.7691\n",
      "Epoch 00046: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0912 - accuracy: 0.7691 - val_loss: 1.3426 - val_accuracy: 0.7020 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0704 - accuracy: 0.7739\n",
      "Epoch 00047: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0713 - accuracy: 0.7739 - val_loss: 1.3406 - val_accuracy: 0.7043 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0710 - accuracy: 0.7744\n",
      "Epoch 00048: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0787 - accuracy: 0.7721 - val_loss: 1.3389 - val_accuracy: 0.7056 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0678 - accuracy: 0.7687\n",
      "Epoch 00049: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.0690 - accuracy: 0.7688 - val_loss: 1.3305 - val_accuracy: 0.7079 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0643 - accuracy: 0.7732\n",
      "Epoch 00050: val_accuracy improved from 0.70916 to 0.71018, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.0653 - accuracy: 0.7731 - val_loss: 1.3248 - val_accuracy: 0.7102 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0717 - accuracy: 0.7743\n",
      "Epoch 00051: val_accuracy did not improve from 0.71018\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.0720 - accuracy: 0.7745 - val_loss: 1.3220 - val_accuracy: 0.7102 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0580 - accuracy: 0.7762\n",
      "Epoch 00052: val_accuracy improved from 0.71018 to 0.71095, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.0598 - accuracy: 0.7759 - val_loss: 1.3217 - val_accuracy: 0.7110 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0495 - accuracy: 0.7763\n",
      "Epoch 00053: val_accuracy did not improve from 0.71095\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.0520 - accuracy: 0.7754 - val_loss: 1.3131 - val_accuracy: 0.7110 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0376 - accuracy: 0.7816\n",
      "Epoch 00054: val_accuracy did not improve from 0.71095\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0385 - accuracy: 0.7816 - val_loss: 1.3164 - val_accuracy: 0.7102 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0420 - accuracy: 0.7851\n",
      "Epoch 00055: val_accuracy improved from 0.71095 to 0.71172, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 1.0419 - accuracy: 0.7849 - val_loss: 1.3133 - val_accuracy: 0.7117 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0386 - accuracy: 0.7840\n",
      "Epoch 00056: val_accuracy did not improve from 0.71172\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0400 - accuracy: 0.7832 - val_loss: 1.3176 - val_accuracy: 0.7102 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0477 - accuracy: 0.7824\n",
      "Epoch 00057: val_accuracy did not improve from 0.71172\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0477 - accuracy: 0.7824 - val_loss: 1.3208 - val_accuracy: 0.7086 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0268 - accuracy: 0.7840\n",
      "Epoch 00058: val_accuracy improved from 0.71172 to 0.71711, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.0264 - accuracy: 0.7844 - val_loss: 1.3149 - val_accuracy: 0.7171 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0260 - accuracy: 0.7858\n",
      "Epoch 00059: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.0282 - accuracy: 0.7853 - val_loss: 1.3082 - val_accuracy: 0.7122 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0283 - accuracy: 0.7792\n",
      "Epoch 00060: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0283 - accuracy: 0.7792 - val_loss: 1.3098 - val_accuracy: 0.7153 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0221 - accuracy: 0.7877\n",
      "Epoch 00061: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.0205 - accuracy: 0.7879 - val_loss: 1.3076 - val_accuracy: 0.7135 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0176 - accuracy: 0.7816\n",
      "Epoch 00062: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 1.0176 - accuracy: 0.7816 - val_loss: 1.3059 - val_accuracy: 0.7117 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0166 - accuracy: 0.7942\n",
      "Epoch 00063: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.0176 - accuracy: 0.7937 - val_loss: 1.3021 - val_accuracy: 0.7156 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0156 - accuracy: 0.7848\n",
      "Epoch 00064: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.0156 - accuracy: 0.7853 - val_loss: 1.3057 - val_accuracy: 0.7125 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0037 - accuracy: 0.7846\n",
      "Epoch 00065: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0036 - accuracy: 0.7848 - val_loss: 1.3089 - val_accuracy: 0.7140 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0118 - accuracy: 0.7829\n",
      "Epoch 00066: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.0109 - accuracy: 0.7829 - val_loss: 1.3079 - val_accuracy: 0.7143 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0066 - accuracy: 0.7867\n",
      "Epoch 00067: val_accuracy improved from 0.71711 to 0.71916, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.0066 - accuracy: 0.7867 - val_loss: 1.3068 - val_accuracy: 0.7192 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0025 - accuracy: 0.7904\n",
      "Epoch 00068: val_accuracy did not improve from 0.71916\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.0024 - accuracy: 0.7905 - val_loss: 1.3035 - val_accuracy: 0.7140 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9897 - accuracy: 0.7937\n",
      "Epoch 00069: val_accuracy did not improve from 0.71916\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.9888 - accuracy: 0.7938 - val_loss: 1.2969 - val_accuracy: 0.7158 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9825 - accuracy: 0.7956\n",
      "Epoch 00070: val_accuracy did not improve from 0.71916\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.9833 - accuracy: 0.7950 - val_loss: 1.2937 - val_accuracy: 0.7143 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9842 - accuracy: 0.7949\n",
      "Epoch 00071: val_accuracy improved from 0.71916 to 0.72044, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 0.9857 - accuracy: 0.7947 - val_loss: 1.2953 - val_accuracy: 0.7204 - lr: 1.0000e-04\n",
      "Epoch 72/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9773 - accuracy: 0.7948\n",
      "Epoch 00072: val_accuracy did not improve from 0.72044\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9794 - accuracy: 0.7939 - val_loss: 1.2980 - val_accuracy: 0.7151 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9695 - accuracy: 0.7971\n",
      "Epoch 00073: val_accuracy did not improve from 0.72044\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.9726 - accuracy: 0.7964 - val_loss: 1.2994 - val_accuracy: 0.7169 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9858 - accuracy: 0.7932\n",
      "Epoch 00074: val_accuracy did not improve from 0.72044\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9878 - accuracy: 0.7923 - val_loss: 1.3248 - val_accuracy: 0.7089 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9895 - accuracy: 0.7933\n",
      "Epoch 00075: val_accuracy did not improve from 0.72044\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9890 - accuracy: 0.7937 - val_loss: 1.2976 - val_accuracy: 0.7163 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9816 - accuracy: 0.7940\n",
      "Epoch 00076: val_accuracy did not improve from 0.72044\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.9826 - accuracy: 0.7938 - val_loss: 1.2911 - val_accuracy: 0.7161 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9682 - accuracy: 0.7996\n",
      "Epoch 00077: val_accuracy did not improve from 0.72044\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9695 - accuracy: 0.7993 - val_loss: 1.2930 - val_accuracy: 0.7176 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9537 - accuracy: 0.8004\n",
      "Epoch 00078: val_accuracy did not improve from 0.72044\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.9592 - accuracy: 0.7988 - val_loss: 1.2912 - val_accuracy: 0.7138 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9625 - accuracy: 0.7984\n",
      "Epoch 00079: val_accuracy did not improve from 0.72044\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.9617 - accuracy: 0.7990 - val_loss: 1.2870 - val_accuracy: 0.7181 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9611 - accuracy: 0.7976\n",
      "Epoch 00080: val_accuracy did not improve from 0.72044\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.9621 - accuracy: 0.7971 - val_loss: 1.2867 - val_accuracy: 0.7181 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9719 - accuracy: 0.7983\n",
      "Epoch 00081: val_accuracy improved from 0.72044 to 0.72095, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.9715 - accuracy: 0.7988 - val_loss: 1.2880 - val_accuracy: 0.7210 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9637 - accuracy: 0.8032\n",
      "Epoch 00082: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.9637 - accuracy: 0.8032 - val_loss: 1.2864 - val_accuracy: 0.7192 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9659 - accuracy: 0.7973\n",
      "Epoch 00083: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.9658 - accuracy: 0.7975 - val_loss: 1.2857 - val_accuracy: 0.7186 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9546 - accuracy: 0.8029\n",
      "Epoch 00084: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9567 - accuracy: 0.8021 - val_loss: 1.2897 - val_accuracy: 0.7179 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9535 - accuracy: 0.8054\n",
      "Epoch 00085: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.9537 - accuracy: 0.8056 - val_loss: 1.2844 - val_accuracy: 0.7184 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9514 - accuracy: 0.8033\n",
      "Epoch 00086: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.9515 - accuracy: 0.8028 - val_loss: 1.2806 - val_accuracy: 0.7186 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9604 - accuracy: 0.8021\n",
      "Epoch 00087: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.9604 - accuracy: 0.8021 - val_loss: 1.2870 - val_accuracy: 0.7166 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9537 - accuracy: 0.8008\n",
      "Epoch 00088: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9558 - accuracy: 0.8002 - val_loss: 1.2833 - val_accuracy: 0.7184 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9486 - accuracy: 0.8052\n",
      "Epoch 00089: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.9495 - accuracy: 0.8047 - val_loss: 1.2824 - val_accuracy: 0.7181 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9473 - accuracy: 0.8040\n",
      "Epoch 00090: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9481 - accuracy: 0.8034 - val_loss: 1.2819 - val_accuracy: 0.7171 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9437 - accuracy: 0.8059\n",
      "Epoch 00091: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.9446 - accuracy: 0.8053 - val_loss: 1.2803 - val_accuracy: 0.7174 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9547 - accuracy: 0.8024\n",
      "Epoch 00092: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9570 - accuracy: 0.8017 - val_loss: 1.2827 - val_accuracy: 0.7192 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9515 - accuracy: 0.8012\n",
      "Epoch 00093: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.9515 - accuracy: 0.8012 - val_loss: 1.2802 - val_accuracy: 0.7176 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9412 - accuracy: 0.8040\n",
      "Epoch 00094: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9432 - accuracy: 0.8037 - val_loss: 1.2830 - val_accuracy: 0.7192 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9432 - accuracy: 0.8047\n",
      "Epoch 00095: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.9422 - accuracy: 0.8055 - val_loss: 1.2802 - val_accuracy: 0.7197 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9375 - accuracy: 0.8091\n",
      "Epoch 00096: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9388 - accuracy: 0.8088 - val_loss: 1.2832 - val_accuracy: 0.7176 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9574 - accuracy: 0.8004\n",
      "Epoch 00097: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9594 - accuracy: 0.7992 - val_loss: 1.2842 - val_accuracy: 0.7176 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9478 - accuracy: 0.8000\n",
      "Epoch 00098: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9478 - accuracy: 0.8000 - val_loss: 1.2808 - val_accuracy: 0.7192 - lr: 1.0000e-05\n",
      "Epoch 99/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9375 - accuracy: 0.8039\n",
      "Epoch 00099: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.9384 - accuracy: 0.8038 - val_loss: 1.2813 - val_accuracy: 0.7202 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9467 - accuracy: 0.8021\n",
      "Epoch 00100: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9486 - accuracy: 0.8017 - val_loss: 1.2810 - val_accuracy: 0.7189 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9420 - accuracy: 0.8042\n",
      "Epoch 00101: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9431 - accuracy: 0.8033 - val_loss: 1.2832 - val_accuracy: 0.7176 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9439 - accuracy: 0.8057\n",
      "Epoch 00102: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.9474 - accuracy: 0.8040 - val_loss: 1.2805 - val_accuracy: 0.7166 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9422 - accuracy: 0.8061\n",
      "Epoch 00103: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9412 - accuracy: 0.8061 - val_loss: 1.2819 - val_accuracy: 0.7192 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9447 - accuracy: 0.8063\n",
      "Epoch 00104: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.9443 - accuracy: 0.8065 - val_loss: 1.2785 - val_accuracy: 0.7179 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9479 - accuracy: 0.8051\n",
      "Epoch 00105: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.9488 - accuracy: 0.8050 - val_loss: 1.2785 - val_accuracy: 0.7174 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9514 - accuracy: 0.8039\n",
      "Epoch 00106: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9515 - accuracy: 0.8040 - val_loss: 1.2786 - val_accuracy: 0.7189 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9347 - accuracy: 0.8093\n",
      "Epoch 00107: val_accuracy did not improve from 0.72095\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9347 - accuracy: 0.8093 - val_loss: 1.2792 - val_accuracy: 0.7184 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9530 - accuracy: 0.8057\n",
      "Epoch 00108: val_accuracy improved from 0.72095 to 0.72249, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.9517 - accuracy: 0.8060 - val_loss: 1.2819 - val_accuracy: 0.7225 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9519 - accuracy: 0.8058\n",
      "Epoch 00109: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9519 - accuracy: 0.8058 - val_loss: 1.2814 - val_accuracy: 0.7202 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9446 - accuracy: 0.8057\n",
      "Epoch 00110: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9465 - accuracy: 0.8053 - val_loss: 1.2810 - val_accuracy: 0.7204 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9428 - accuracy: 0.8014\n",
      "Epoch 00111: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9438 - accuracy: 0.8017 - val_loss: 1.2806 - val_accuracy: 0.7217 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9396 - accuracy: 0.8081\n",
      "Epoch 00112: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.9416 - accuracy: 0.8078 - val_loss: 1.2800 - val_accuracy: 0.7186 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9423 - accuracy: 0.8083\n",
      "Epoch 00113: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9427 - accuracy: 0.8080 - val_loss: 1.2802 - val_accuracy: 0.7163 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9463 - accuracy: 0.8048\n",
      "Epoch 00114: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9463 - accuracy: 0.8048 - val_loss: 1.2811 - val_accuracy: 0.7169 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9577 - accuracy: 0.7986\n",
      "Epoch 00115: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.9596 - accuracy: 0.7979 - val_loss: 1.2822 - val_accuracy: 0.7174 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9564 - accuracy: 0.7990\n",
      "Epoch 00116: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9570 - accuracy: 0.7985 - val_loss: 1.2807 - val_accuracy: 0.7179 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9460 - accuracy: 0.8051\n",
      "Epoch 00117: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9460 - accuracy: 0.8050 - val_loss: 1.2791 - val_accuracy: 0.7186 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9374 - accuracy: 0.8025\n",
      "Epoch 00118: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9374 - accuracy: 0.8025 - val_loss: 1.2808 - val_accuracy: 0.7194 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9298 - accuracy: 0.8131\n",
      "Epoch 00119: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9305 - accuracy: 0.8131 - val_loss: 1.2821 - val_accuracy: 0.7166 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9382 - accuracy: 0.8065\n",
      "Epoch 00120: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9382 - accuracy: 0.8065 - val_loss: 1.2789 - val_accuracy: 0.7174 - lr: 1.0000e-06\n",
      "epoch_number 108\n",
      "train accuracy and validation accuracy 0.8060226440429688 0.7224929332733154\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.2819 - accuracy: 0.7225\n",
      "test_accuracy 0.7224929332733154\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154]\n",
      "0.7034282286961874\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S4_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S4_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.5628 - accuracy: 0.1519\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.25648, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 388ms/step - loss: 4.5496 - accuracy: 0.1522 - val_loss: 3.7172 - val_accuracy: 0.2565 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.9748 - accuracy: 0.2827\n",
      "Epoch 00002: val_accuracy improved from 0.25648 to 0.37445, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.9773 - accuracy: 0.2826 - val_loss: 2.5246 - val_accuracy: 0.3745 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.6095 - accuracy: 0.3614\n",
      "Epoch 00003: val_accuracy improved from 0.37445 to 0.43524, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 2.6090 - accuracy: 0.3615 - val_loss: 2.2664 - val_accuracy: 0.4352 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4115 - accuracy: 0.4087\n",
      "Epoch 00004: val_accuracy improved from 0.43524 to 0.47628, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 2.4121 - accuracy: 0.4087 - val_loss: 2.1222 - val_accuracy: 0.4763 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2544 - accuracy: 0.4449\n",
      "Epoch 00005: val_accuracy improved from 0.47628 to 0.51218, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 2.2519 - accuracy: 0.4459 - val_loss: 1.9938 - val_accuracy: 0.5122 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1123 - accuracy: 0.4781\n",
      "Epoch 00006: val_accuracy improved from 0.51218 to 0.54681, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 2.1117 - accuracy: 0.4782 - val_loss: 1.8708 - val_accuracy: 0.5468 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0088 - accuracy: 0.5038\n",
      "Epoch 00007: val_accuracy improved from 0.54681 to 0.57399, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 2.0096 - accuracy: 0.5039 - val_loss: 1.7774 - val_accuracy: 0.5740 - lr: 0.0010\n",
      "Epoch 8/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.9305 - accuracy: 0.5207\n",
      "Epoch 00008: val_accuracy improved from 0.57399 to 0.60144, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 1.9305 - accuracy: 0.5207 - val_loss: 1.6954 - val_accuracy: 0.6014 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8703 - accuracy: 0.5421\n",
      "Epoch 00009: val_accuracy did not improve from 0.60144\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 1.8722 - accuracy: 0.5418 - val_loss: 1.6693 - val_accuracy: 0.5999 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7902 - accuracy: 0.5612\n",
      "Epoch 00010: val_accuracy improved from 0.60144 to 0.60580, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 1.7894 - accuracy: 0.5615 - val_loss: 1.6262 - val_accuracy: 0.6058 - lr: 0.0010\n",
      "Epoch 11/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.7381 - accuracy: 0.5842\n",
      "Epoch 00011: val_accuracy improved from 0.60580 to 0.61067, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 1.7381 - accuracy: 0.5842 - val_loss: 1.6164 - val_accuracy: 0.6107 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7283 - accuracy: 0.5822\n",
      "Epoch 00012: val_accuracy improved from 0.61067 to 0.62555, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.7283 - accuracy: 0.5814 - val_loss: 1.5677 - val_accuracy: 0.6255 - lr: 0.0010\n",
      "Epoch 13/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.6854 - accuracy: 0.5852\n",
      "Epoch 00013: val_accuracy did not improve from 0.62555\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 1.6854 - accuracy: 0.5852 - val_loss: 1.5407 - val_accuracy: 0.6212 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6222 - accuracy: 0.6089\n",
      "Epoch 00014: val_accuracy improved from 0.62555 to 0.63324, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.6256 - accuracy: 0.6073 - val_loss: 1.5030 - val_accuracy: 0.6332 - lr: 0.0010\n",
      "Epoch 15/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.5865 - accuracy: 0.6211\n",
      "Epoch 00015: val_accuracy improved from 0.63324 to 0.64247, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 1.5865 - accuracy: 0.6211 - val_loss: 1.5012 - val_accuracy: 0.6425 - lr: 0.0010\n",
      "Epoch 16/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.5600 - accuracy: 0.6225\n",
      "Epoch 00016: val_accuracy improved from 0.64247 to 0.65042, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.5600 - accuracy: 0.6225 - val_loss: 1.4654 - val_accuracy: 0.6504 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5308 - accuracy: 0.6327\n",
      "Epoch 00017: val_accuracy improved from 0.65042 to 0.65196, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.5324 - accuracy: 0.6324 - val_loss: 1.4904 - val_accuracy: 0.6520 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5494 - accuracy: 0.6306\n",
      "Epoch 00018: val_accuracy did not improve from 0.65196\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.5506 - accuracy: 0.6300 - val_loss: 1.4606 - val_accuracy: 0.6489 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5043 - accuracy: 0.6438\n",
      "Epoch 00019: val_accuracy did not improve from 0.65196\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 1.5048 - accuracy: 0.6438 - val_loss: 1.4458 - val_accuracy: 0.6448 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4737 - accuracy: 0.6473\n",
      "Epoch 00020: val_accuracy improved from 0.65196 to 0.65555, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 1.4721 - accuracy: 0.6478 - val_loss: 1.4495 - val_accuracy: 0.6556 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4311 - accuracy: 0.6602\n",
      "Epoch 00021: val_accuracy improved from 0.65555 to 0.65940, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.4309 - accuracy: 0.6604 - val_loss: 1.4366 - val_accuracy: 0.6594 - lr: 0.0010\n",
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4095 - accuracy: 0.6670\n",
      "Epoch 00022: val_accuracy improved from 0.65940 to 0.66376, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 106ms/step - loss: 1.4082 - accuracy: 0.6674 - val_loss: 1.4097 - val_accuracy: 0.6638 - lr: 0.0010\n",
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3976 - accuracy: 0.6704\n",
      "Epoch 00023: val_accuracy did not improve from 0.66376\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.3984 - accuracy: 0.6701 - val_loss: 1.3898 - val_accuracy: 0.6632 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3674 - accuracy: 0.6751\n",
      "Epoch 00024: val_accuracy improved from 0.66376 to 0.67633, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.3695 - accuracy: 0.6744 - val_loss: 1.3691 - val_accuracy: 0.6763 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3681 - accuracy: 0.6830\n",
      "Epoch 00025: val_accuracy did not improve from 0.67633\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.3700 - accuracy: 0.6825 - val_loss: 1.3680 - val_accuracy: 0.6717 - lr: 0.0010\n",
      "Epoch 26/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3343 - accuracy: 0.6886\n",
      "Epoch 00026: val_accuracy did not improve from 0.67633\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.3343 - accuracy: 0.6886 - val_loss: 1.3836 - val_accuracy: 0.6686 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3242 - accuracy: 0.6890\n",
      "Epoch 00027: val_accuracy improved from 0.67633 to 0.68120, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.3231 - accuracy: 0.6886 - val_loss: 1.3377 - val_accuracy: 0.6812 - lr: 0.0010\n",
      "Epoch 28/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2907 - accuracy: 0.7005\n",
      "Epoch 00028: val_accuracy improved from 0.68120 to 0.68428, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 1.2907 - accuracy: 0.7005 - val_loss: 1.3241 - val_accuracy: 0.6843 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2693 - accuracy: 0.7032\n",
      "Epoch 00029: val_accuracy improved from 0.68428 to 0.68453, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.2717 - accuracy: 0.7025 - val_loss: 1.3310 - val_accuracy: 0.6845 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2668 - accuracy: 0.6981\n",
      "Epoch 00030: val_accuracy did not improve from 0.68453\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.2671 - accuracy: 0.6973 - val_loss: 1.3546 - val_accuracy: 0.6756 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2608 - accuracy: 0.7061\n",
      "Epoch 00031: val_accuracy did not improve from 0.68453\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.2588 - accuracy: 0.7067 - val_loss: 1.3080 - val_accuracy: 0.6812 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2365 - accuracy: 0.7108\n",
      "Epoch 00032: val_accuracy improved from 0.68453 to 0.68966, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.2394 - accuracy: 0.7099 - val_loss: 1.3102 - val_accuracy: 0.6897 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2283 - accuracy: 0.7132\n",
      "Epoch 00033: val_accuracy did not improve from 0.68966\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.2290 - accuracy: 0.7126 - val_loss: 1.3053 - val_accuracy: 0.6858 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2264 - accuracy: 0.7141\n",
      "Epoch 00034: val_accuracy did not improve from 0.68966\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 1.2254 - accuracy: 0.7144 - val_loss: 1.3340 - val_accuracy: 0.6866 - lr: 0.0010\n",
      "Epoch 35/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2131 - accuracy: 0.7138\n",
      "Epoch 00035: val_accuracy improved from 0.68966 to 0.69351, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 2s 197ms/step - loss: 1.2131 - accuracy: 0.7138 - val_loss: 1.3073 - val_accuracy: 0.6935 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1836 - accuracy: 0.7209\n",
      "Epoch 00036: val_accuracy did not improve from 0.69351\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.1849 - accuracy: 0.7207 - val_loss: 1.3146 - val_accuracy: 0.6899 - lr: 0.0010\n",
      "Epoch 37/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2004 - accuracy: 0.7191\n",
      "Epoch 00037: val_accuracy did not improve from 0.69351\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.2004 - accuracy: 0.7191 - val_loss: 1.3476 - val_accuracy: 0.6856 - lr: 0.0010\n",
      "Epoch 38/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1879 - accuracy: 0.7224\n",
      "Epoch 00038: val_accuracy did not improve from 0.69351\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.1879 - accuracy: 0.7224 - val_loss: 1.2995 - val_accuracy: 0.6920 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1654 - accuracy: 0.7299\n",
      "Epoch 00039: val_accuracy did not improve from 0.69351\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.1644 - accuracy: 0.7301 - val_loss: 1.3072 - val_accuracy: 0.6927 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1319 - accuracy: 0.7389\n",
      "Epoch 00040: val_accuracy improved from 0.69351 to 0.70146, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.1336 - accuracy: 0.7389 - val_loss: 1.2720 - val_accuracy: 0.7015 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0909 - accuracy: 0.7523\n",
      "Epoch 00041: val_accuracy improved from 0.70146 to 0.70967, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.0923 - accuracy: 0.7515 - val_loss: 1.2597 - val_accuracy: 0.7097 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0725 - accuracy: 0.7627\n",
      "Epoch 00042: val_accuracy did not improve from 0.70967\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 1.0733 - accuracy: 0.7626 - val_loss: 1.2494 - val_accuracy: 0.7027 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0638 - accuracy: 0.7594\n",
      "Epoch 00043: val_accuracy did not improve from 0.70967\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.0643 - accuracy: 0.7591 - val_loss: 1.2449 - val_accuracy: 0.7092 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0500 - accuracy: 0.7621\n",
      "Epoch 00044: val_accuracy improved from 0.70967 to 0.71121, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.0499 - accuracy: 0.7623 - val_loss: 1.2385 - val_accuracy: 0.7112 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0360 - accuracy: 0.7636\n",
      "Epoch 00045: val_accuracy improved from 0.71121 to 0.71198, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.0360 - accuracy: 0.7636 - val_loss: 1.2408 - val_accuracy: 0.7120 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0471 - accuracy: 0.7671\n",
      "Epoch 00046: val_accuracy improved from 0.71198 to 0.71300, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 105ms/step - loss: 1.0462 - accuracy: 0.7671 - val_loss: 1.2318 - val_accuracy: 0.7130 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0290 - accuracy: 0.7682\n",
      "Epoch 00047: val_accuracy did not improve from 0.71300\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 1.0290 - accuracy: 0.7682 - val_loss: 1.2271 - val_accuracy: 0.7127 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0237 - accuracy: 0.7724\n",
      "Epoch 00048: val_accuracy did not improve from 0.71300\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.0283 - accuracy: 0.7709 - val_loss: 1.2246 - val_accuracy: 0.7130 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0237 - accuracy: 0.7716\n",
      "Epoch 00049: val_accuracy did not improve from 0.71300\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0220 - accuracy: 0.7724 - val_loss: 1.2272 - val_accuracy: 0.7112 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0286 - accuracy: 0.7718\n",
      "Epoch 00050: val_accuracy improved from 0.71300 to 0.71403, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.0286 - accuracy: 0.7718 - val_loss: 1.2237 - val_accuracy: 0.7140 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0218 - accuracy: 0.7732\n",
      "Epoch 00051: val_accuracy improved from 0.71403 to 0.71711, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.0212 - accuracy: 0.7727 - val_loss: 1.2132 - val_accuracy: 0.7171 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0170 - accuracy: 0.7724\n",
      "Epoch 00052: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.0192 - accuracy: 0.7721 - val_loss: 1.2211 - val_accuracy: 0.7161 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0055 - accuracy: 0.7774\n",
      "Epoch 00053: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0054 - accuracy: 0.7769 - val_loss: 1.2155 - val_accuracy: 0.7143 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9993 - accuracy: 0.7800\n",
      "Epoch 00054: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0002 - accuracy: 0.7796 - val_loss: 1.2253 - val_accuracy: 0.7143 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0005 - accuracy: 0.7791\n",
      "Epoch 00055: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.9999 - accuracy: 0.7790 - val_loss: 1.2123 - val_accuracy: 0.7148 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0043 - accuracy: 0.7798\n",
      "Epoch 00056: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0043 - accuracy: 0.7798 - val_loss: 1.2145 - val_accuracy: 0.7138 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9931 - accuracy: 0.7770\n",
      "Epoch 00057: val_accuracy improved from 0.71711 to 0.71839, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.9933 - accuracy: 0.7772 - val_loss: 1.2057 - val_accuracy: 0.7184 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9845 - accuracy: 0.7830\n",
      "Epoch 00058: val_accuracy improved from 0.71839 to 0.71942, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 0.9844 - accuracy: 0.7829 - val_loss: 1.2133 - val_accuracy: 0.7194 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9772 - accuracy: 0.7838\n",
      "Epoch 00059: val_accuracy improved from 0.71942 to 0.71967, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.9796 - accuracy: 0.7828 - val_loss: 1.2092 - val_accuracy: 0.7197 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9725 - accuracy: 0.7894\n",
      "Epoch 00060: val_accuracy did not improve from 0.71967\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.9725 - accuracy: 0.7894 - val_loss: 1.2127 - val_accuracy: 0.7174 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9744 - accuracy: 0.7849\n",
      "Epoch 00061: val_accuracy did not improve from 0.71967\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 0.9747 - accuracy: 0.7850 - val_loss: 1.2052 - val_accuracy: 0.7169 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9754 - accuracy: 0.7826\n",
      "Epoch 00062: val_accuracy did not improve from 0.71967\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9765 - accuracy: 0.7822 - val_loss: 1.2089 - val_accuracy: 0.7171 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9790 - accuracy: 0.7831\n",
      "Epoch 00063: val_accuracy did not improve from 0.71967\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.9790 - accuracy: 0.7831 - val_loss: 1.2062 - val_accuracy: 0.7169 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9745 - accuracy: 0.7868\n",
      "Epoch 00064: val_accuracy did not improve from 0.71967\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.9743 - accuracy: 0.7867 - val_loss: 1.2009 - val_accuracy: 0.7143 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9593 - accuracy: 0.7899\n",
      "Epoch 00065: val_accuracy improved from 0.71967 to 0.72480, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.9593 - accuracy: 0.7899 - val_loss: 1.1933 - val_accuracy: 0.7248 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9431 - accuracy: 0.7938\n",
      "Epoch 00066: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9451 - accuracy: 0.7927 - val_loss: 1.1994 - val_accuracy: 0.7202 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9627 - accuracy: 0.7884\n",
      "Epoch 00067: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.9627 - accuracy: 0.7884 - val_loss: 1.1963 - val_accuracy: 0.7197 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9513 - accuracy: 0.7896\n",
      "Epoch 00068: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.9518 - accuracy: 0.7898 - val_loss: 1.2072 - val_accuracy: 0.7169 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9449 - accuracy: 0.7916\n",
      "Epoch 00069: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.9449 - accuracy: 0.7916 - val_loss: 1.2003 - val_accuracy: 0.7215 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9461 - accuracy: 0.7891\n",
      "Epoch 00070: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.9461 - accuracy: 0.7891 - val_loss: 1.1948 - val_accuracy: 0.7225 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9444 - accuracy: 0.7961\n",
      "Epoch 00071: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 0.9456 - accuracy: 0.7951 - val_loss: 1.1850 - val_accuracy: 0.7225 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9421 - accuracy: 0.7944\n",
      "Epoch 00072: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9418 - accuracy: 0.7945 - val_loss: 1.1952 - val_accuracy: 0.7220 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9360 - accuracy: 0.7914\n",
      "Epoch 00073: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9354 - accuracy: 0.7912 - val_loss: 1.1903 - val_accuracy: 0.7210 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9417 - accuracy: 0.7923\n",
      "Epoch 00074: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9400 - accuracy: 0.7929 - val_loss: 1.1879 - val_accuracy: 0.7217 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9271 - accuracy: 0.7980\n",
      "Epoch 00075: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.9271 - accuracy: 0.7980 - val_loss: 1.1850 - val_accuracy: 0.7212 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9325 - accuracy: 0.7946\n",
      "Epoch 00076: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.9325 - accuracy: 0.7946 - val_loss: 1.1860 - val_accuracy: 0.7230 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9351 - accuracy: 0.7922\n",
      "Epoch 00077: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9351 - accuracy: 0.7924 - val_loss: 1.1968 - val_accuracy: 0.7199 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9257 - accuracy: 0.7969\n",
      "Epoch 00078: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9275 - accuracy: 0.7965 - val_loss: 1.1872 - val_accuracy: 0.7212 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9085 - accuracy: 0.7998\n",
      "Epoch 00079: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9117 - accuracy: 0.7989 - val_loss: 1.1882 - val_accuracy: 0.7207 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9164 - accuracy: 0.8010\n",
      "Epoch 00080: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9189 - accuracy: 0.7999 - val_loss: 1.1854 - val_accuracy: 0.7240 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9174 - accuracy: 0.7999\n",
      "Epoch 00081: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.9174 - accuracy: 0.7999 - val_loss: 1.1832 - val_accuracy: 0.7233 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9260 - accuracy: 0.7965\n",
      "Epoch 00082: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.9260 - accuracy: 0.7965 - val_loss: 1.1804 - val_accuracy: 0.7235 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9102 - accuracy: 0.7996\n",
      "Epoch 00083: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9102 - accuracy: 0.7996 - val_loss: 1.1822 - val_accuracy: 0.7230 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9138 - accuracy: 0.7969\n",
      "Epoch 00084: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9138 - accuracy: 0.7969 - val_loss: 1.1827 - val_accuracy: 0.7227 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9056 - accuracy: 0.8012\n",
      "Epoch 00085: val_accuracy did not improve from 0.72480\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9088 - accuracy: 0.8004 - val_loss: 1.1812 - val_accuracy: 0.7217 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9163 - accuracy: 0.7971\n",
      "Epoch 00086: val_accuracy improved from 0.72480 to 0.72531, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.9163 - accuracy: 0.7971 - val_loss: 1.1783 - val_accuracy: 0.7253 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9247 - accuracy: 0.7939\n",
      "Epoch 00087: val_accuracy did not improve from 0.72531\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9252 - accuracy: 0.7940 - val_loss: 1.1813 - val_accuracy: 0.7212 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9233 - accuracy: 0.7977\n",
      "Epoch 00088: val_accuracy did not improve from 0.72531\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9233 - accuracy: 0.7977 - val_loss: 1.1810 - val_accuracy: 0.7243 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9075 - accuracy: 0.8009\n",
      "Epoch 00089: val_accuracy did not improve from 0.72531\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9097 - accuracy: 0.8004 - val_loss: 1.1826 - val_accuracy: 0.7227 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9002 - accuracy: 0.8054\n",
      "Epoch 00090: val_accuracy did not improve from 0.72531\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.9002 - accuracy: 0.8054 - val_loss: 1.1782 - val_accuracy: 0.7253 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9045 - accuracy: 0.8056\n",
      "Epoch 00091: val_accuracy did not improve from 0.72531\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9045 - accuracy: 0.8056 - val_loss: 1.1820 - val_accuracy: 0.7235 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9092 - accuracy: 0.8029\n",
      "Epoch 00092: val_accuracy did not improve from 0.72531\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9128 - accuracy: 0.8018 - val_loss: 1.1816 - val_accuracy: 0.7220 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9093 - accuracy: 0.8004\n",
      "Epoch 00093: val_accuracy did not improve from 0.72531\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9097 - accuracy: 0.7999 - val_loss: 1.1834 - val_accuracy: 0.7235 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9065 - accuracy: 0.8023\n",
      "Epoch 00094: val_accuracy did not improve from 0.72531\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9063 - accuracy: 0.8023 - val_loss: 1.1824 - val_accuracy: 0.7243 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9034 - accuracy: 0.8032\n",
      "Epoch 00095: val_accuracy improved from 0.72531 to 0.72583, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.9043 - accuracy: 0.8032 - val_loss: 1.1832 - val_accuracy: 0.7258 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9010 - accuracy: 0.8049\n",
      "Epoch 00096: val_accuracy did not improve from 0.72583\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9024 - accuracy: 0.8042 - val_loss: 1.1823 - val_accuracy: 0.7256 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9081 - accuracy: 0.8016\n",
      "Epoch 00097: val_accuracy did not improve from 0.72583\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9081 - accuracy: 0.8016 - val_loss: 1.1807 - val_accuracy: 0.7253 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9211 - accuracy: 0.7927\n",
      "Epoch 00098: val_accuracy did not improve from 0.72583\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9211 - accuracy: 0.7927 - val_loss: 1.1812 - val_accuracy: 0.7245 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9032 - accuracy: 0.8053\n",
      "Epoch 00099: val_accuracy did not improve from 0.72583\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9032 - accuracy: 0.8053 - val_loss: 1.1819 - val_accuracy: 0.7238 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9039 - accuracy: 0.8054\n",
      "Epoch 00100: val_accuracy improved from 0.72583 to 0.72762, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.9044 - accuracy: 0.8057 - val_loss: 1.1812 - val_accuracy: 0.7276 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9071 - accuracy: 0.8037\n",
      "Epoch 00101: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9071 - accuracy: 0.8037 - val_loss: 1.1804 - val_accuracy: 0.7245 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9058 - accuracy: 0.8048\n",
      "Epoch 00102: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9080 - accuracy: 0.8045 - val_loss: 1.1805 - val_accuracy: 0.7251 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9021 - accuracy: 0.8044\n",
      "Epoch 00103: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.9049 - accuracy: 0.8037 - val_loss: 1.1825 - val_accuracy: 0.7253 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9020 - accuracy: 0.8041\n",
      "Epoch 00104: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9045 - accuracy: 0.8033 - val_loss: 1.1834 - val_accuracy: 0.7238 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8986 - accuracy: 0.8050\n",
      "Epoch 00105: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8986 - accuracy: 0.8050 - val_loss: 1.1801 - val_accuracy: 0.7256 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9013 - accuracy: 0.8018\n",
      "Epoch 00106: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9002 - accuracy: 0.8025 - val_loss: 1.1809 - val_accuracy: 0.7251 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9079 - accuracy: 0.8042\n",
      "Epoch 00107: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9079 - accuracy: 0.8043 - val_loss: 1.1799 - val_accuracy: 0.7243 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9035 - accuracy: 0.7993\n",
      "Epoch 00108: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9045 - accuracy: 0.7989 - val_loss: 1.1796 - val_accuracy: 0.7230 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9073 - accuracy: 0.8017\n",
      "Epoch 00109: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9090 - accuracy: 0.8011 - val_loss: 1.1797 - val_accuracy: 0.7238 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9010 - accuracy: 0.8030\n",
      "Epoch 00110: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9013 - accuracy: 0.8028 - val_loss: 1.1829 - val_accuracy: 0.7217 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8997 - accuracy: 0.8034\n",
      "Epoch 00111: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 0.9008 - accuracy: 0.8031 - val_loss: 1.1777 - val_accuracy: 0.7253 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9085 - accuracy: 0.8006\n",
      "Epoch 00112: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.9085 - accuracy: 0.8006 - val_loss: 1.1802 - val_accuracy: 0.7235 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8991 - accuracy: 0.8059\n",
      "Epoch 00113: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.8982 - accuracy: 0.8062 - val_loss: 1.1780 - val_accuracy: 0.7253 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9087 - accuracy: 0.8007\n",
      "Epoch 00114: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 0.9087 - accuracy: 0.8007 - val_loss: 1.1765 - val_accuracy: 0.7271 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8889 - accuracy: 0.8037\n",
      "Epoch 00115: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8893 - accuracy: 0.8034 - val_loss: 1.1775 - val_accuracy: 0.7256 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9050 - accuracy: 0.8037\n",
      "Epoch 00116: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.9064 - accuracy: 0.8033 - val_loss: 1.1762 - val_accuracy: 0.7253 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9000 - accuracy: 0.8033\n",
      "Epoch 00117: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9021 - accuracy: 0.8026 - val_loss: 1.1767 - val_accuracy: 0.7276 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9119 - accuracy: 0.7979\n",
      "Epoch 00118: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9115 - accuracy: 0.7980 - val_loss: 1.1775 - val_accuracy: 0.7261 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8904 - accuracy: 0.8101\n",
      "Epoch 00119: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8911 - accuracy: 0.8099 - val_loss: 1.1785 - val_accuracy: 0.7248 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9011 - accuracy: 0.8034\n",
      "Epoch 00120: val_accuracy did not improve from 0.72762\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.9010 - accuracy: 0.8035 - val_loss: 1.1782 - val_accuracy: 0.7243 - lr: 1.0000e-06\n",
      "epoch_number 100\n",
      "train accuracy and validation accuracy 0.8056929111480713 0.7276224493980408\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.1812 - accuracy: 0.7276\n",
      "test_accuracy 0.7276224493980408\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408]\n",
      "0.7094767838716507\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S5_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S5_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 5.8193 - accuracy: 0.0855\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.16850, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 388ms/step - loss: 5.8193 - accuracy: 0.0855 - val_loss: 4.6185 - val_accuracy: 0.1685 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.4542 - accuracy: 0.1922\n",
      "Epoch 00002: val_accuracy improved from 0.16850 to 0.26494, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 3.4533 - accuracy: 0.1919 - val_loss: 3.0282 - val_accuracy: 0.2649 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.9490 - accuracy: 0.2776\n",
      "Epoch 00003: val_accuracy improved from 0.26494 to 0.39036, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 2.9506 - accuracy: 0.2772 - val_loss: 2.5404 - val_accuracy: 0.3904 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.6670 - accuracy: 0.3433\n",
      "Epoch 00004: val_accuracy improved from 0.39036 to 0.43909, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 2.6667 - accuracy: 0.3433 - val_loss: 2.3036 - val_accuracy: 0.4391 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4348 - accuracy: 0.3886\n",
      "Epoch 00005: val_accuracy improved from 0.43909 to 0.48910, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 2.4345 - accuracy: 0.3882 - val_loss: 2.1196 - val_accuracy: 0.4891 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2487 - accuracy: 0.4478\n",
      "Epoch 00006: val_accuracy improved from 0.48910 to 0.52142, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 2.2482 - accuracy: 0.4477 - val_loss: 2.0021 - val_accuracy: 0.5214 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1268 - accuracy: 0.4749\n",
      "Epoch 00007: val_accuracy improved from 0.52142 to 0.55578, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 2.1275 - accuracy: 0.4750 - val_loss: 1.8480 - val_accuracy: 0.5558 - lr: 0.0010\n",
      "Epoch 8/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.0050 - accuracy: 0.5016\n",
      "Epoch 00008: val_accuracy improved from 0.55578 to 0.57835, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 2.0050 - accuracy: 0.5016 - val_loss: 1.7501 - val_accuracy: 0.5784 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9388 - accuracy: 0.5222\n",
      "Epoch 00009: val_accuracy improved from 0.57835 to 0.59810, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 1.9376 - accuracy: 0.5234 - val_loss: 1.6696 - val_accuracy: 0.5981 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8460 - accuracy: 0.5470\n",
      "Epoch 00010: val_accuracy improved from 0.59810 to 0.60964, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.8457 - accuracy: 0.5470 - val_loss: 1.6184 - val_accuracy: 0.6096 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7779 - accuracy: 0.5578\n",
      "Epoch 00011: val_accuracy improved from 0.60964 to 0.63170, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.7778 - accuracy: 0.5581 - val_loss: 1.5349 - val_accuracy: 0.6317 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7177 - accuracy: 0.5801\n",
      "Epoch 00012: val_accuracy improved from 0.63170 to 0.64427, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.7208 - accuracy: 0.5795 - val_loss: 1.4966 - val_accuracy: 0.6443 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6678 - accuracy: 0.5893\n",
      "Epoch 00013: val_accuracy did not improve from 0.64427\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.6658 - accuracy: 0.5891 - val_loss: 1.6774 - val_accuracy: 0.6094 - lr: 0.0010\n",
      "Epoch 14/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.6493 - accuracy: 0.5993\n",
      "Epoch 00014: val_accuracy improved from 0.64427 to 0.65837, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.6493 - accuracy: 0.5993 - val_loss: 1.4359 - val_accuracy: 0.6584 - lr: 0.0010\n",
      "Epoch 15/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.5761 - accuracy: 0.6157\n",
      "Epoch 00015: val_accuracy did not improve from 0.65837\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 1.5761 - accuracy: 0.6157 - val_loss: 1.4069 - val_accuracy: 0.6584 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5411 - accuracy: 0.6261\n",
      "Epoch 00016: val_accuracy improved from 0.65837 to 0.66222, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.5433 - accuracy: 0.6255 - val_loss: 1.3904 - val_accuracy: 0.6622 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4943 - accuracy: 0.6386\n",
      "Epoch 00017: val_accuracy improved from 0.66222 to 0.67428, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.4968 - accuracy: 0.6377 - val_loss: 1.3556 - val_accuracy: 0.6743 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4800 - accuracy: 0.6431\n",
      "Epoch 00018: val_accuracy improved from 0.67428 to 0.68325, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.4820 - accuracy: 0.6426 - val_loss: 1.3207 - val_accuracy: 0.6833 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4216 - accuracy: 0.6587\n",
      "Epoch 00019: val_accuracy did not improve from 0.68325\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.4226 - accuracy: 0.6579 - val_loss: 1.3264 - val_accuracy: 0.6794 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4323 - accuracy: 0.6539\n",
      "Epoch 00020: val_accuracy did not improve from 0.68325\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.4331 - accuracy: 0.6536 - val_loss: 1.3322 - val_accuracy: 0.6766 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3992 - accuracy: 0.6578\n",
      "Epoch 00021: val_accuracy improved from 0.68325 to 0.69813, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.4004 - accuracy: 0.6572 - val_loss: 1.2719 - val_accuracy: 0.6981 - lr: 0.0010\n",
      "Epoch 22/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3551 - accuracy: 0.6745\n",
      "Epoch 00022: val_accuracy did not improve from 0.69813\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.3551 - accuracy: 0.6745 - val_loss: 1.2864 - val_accuracy: 0.6917 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3405 - accuracy: 0.6759\n",
      "Epoch 00023: val_accuracy did not improve from 0.69813\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.3452 - accuracy: 0.6746 - val_loss: 1.2977 - val_accuracy: 0.6886 - lr: 0.0010\n",
      "Epoch 24/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3720 - accuracy: 0.6693\n",
      "Epoch 00024: val_accuracy did not improve from 0.69813\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.3720 - accuracy: 0.6693 - val_loss: 1.3512 - val_accuracy: 0.6732 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3562 - accuracy: 0.6723\n",
      "Epoch 00025: val_accuracy did not improve from 0.69813\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.3582 - accuracy: 0.6719 - val_loss: 1.2616 - val_accuracy: 0.6953 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2812 - accuracy: 0.6921\n",
      "Epoch 00026: val_accuracy improved from 0.69813 to 0.70274, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.2814 - accuracy: 0.6917 - val_loss: 1.2374 - val_accuracy: 0.7027 - lr: 0.0010\n",
      "Epoch 27/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2635 - accuracy: 0.6925\n",
      "Epoch 00027: val_accuracy did not improve from 0.70274\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.2635 - accuracy: 0.6925 - val_loss: 1.2519 - val_accuracy: 0.6968 - lr: 0.0010\n",
      "Epoch 28/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2497 - accuracy: 0.7003\n",
      "Epoch 00028: val_accuracy improved from 0.70274 to 0.70890, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.2497 - accuracy: 0.7003 - val_loss: 1.2328 - val_accuracy: 0.7089 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2301 - accuracy: 0.7039\n",
      "Epoch 00029: val_accuracy did not improve from 0.70890\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.2290 - accuracy: 0.7044 - val_loss: 1.2245 - val_accuracy: 0.7043 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2125 - accuracy: 0.7151\n",
      "Epoch 00030: val_accuracy improved from 0.70890 to 0.71018, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.2133 - accuracy: 0.7146 - val_loss: 1.2103 - val_accuracy: 0.7102 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2012 - accuracy: 0.7153\n",
      "Epoch 00031: val_accuracy improved from 0.71018 to 0.72660, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.2026 - accuracy: 0.7147 - val_loss: 1.1829 - val_accuracy: 0.7266 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1526 - accuracy: 0.7237\n",
      "Epoch 00032: val_accuracy did not improve from 0.72660\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.1535 - accuracy: 0.7238 - val_loss: 1.1974 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 33/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1457 - accuracy: 0.7289\n",
      "Epoch 00033: val_accuracy did not improve from 0.72660\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.1457 - accuracy: 0.7289 - val_loss: 1.1832 - val_accuracy: 0.7266 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1472 - accuracy: 0.7307\n",
      "Epoch 00034: val_accuracy did not improve from 0.72660\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.1480 - accuracy: 0.7312 - val_loss: 1.2291 - val_accuracy: 0.7115 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1438 - accuracy: 0.7351\n",
      "Epoch 00035: val_accuracy did not improve from 0.72660\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.1435 - accuracy: 0.7351 - val_loss: 1.1810 - val_accuracy: 0.7156 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0982 - accuracy: 0.7442\n",
      "Epoch 00036: val_accuracy did not improve from 0.72660\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.0971 - accuracy: 0.7440 - val_loss: 1.1710 - val_accuracy: 0.7192 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0870 - accuracy: 0.7448\n",
      "Epoch 00037: val_accuracy did not improve from 0.72660\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.0886 - accuracy: 0.7447 - val_loss: 1.1688 - val_accuracy: 0.7263 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0961 - accuracy: 0.7424\n",
      "Epoch 00038: val_accuracy improved from 0.72660 to 0.73480, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.0967 - accuracy: 0.7421 - val_loss: 1.1460 - val_accuracy: 0.7348 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0664 - accuracy: 0.7490\n",
      "Epoch 00039: val_accuracy did not improve from 0.73480\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0665 - accuracy: 0.7488 - val_loss: 1.1695 - val_accuracy: 0.7222 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0380 - accuracy: 0.7600\n",
      "Epoch 00040: val_accuracy did not improve from 0.73480\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.0372 - accuracy: 0.7605 - val_loss: 1.1374 - val_accuracy: 0.7315 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0179 - accuracy: 0.7669\n",
      "Epoch 00041: val_accuracy improved from 0.73480 to 0.74070, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.0179 - accuracy: 0.7669 - val_loss: 1.1084 - val_accuracy: 0.7407 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9838 - accuracy: 0.7742\n",
      "Epoch 00042: val_accuracy improved from 0.74070 to 0.74199, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 0.9833 - accuracy: 0.7742 - val_loss: 1.1022 - val_accuracy: 0.7420 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9799 - accuracy: 0.7737\n",
      "Epoch 00043: val_accuracy improved from 0.74199 to 0.74327, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 0.9830 - accuracy: 0.7728 - val_loss: 1.0993 - val_accuracy: 0.7433 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9683 - accuracy: 0.7789\n",
      "Epoch 00044: val_accuracy improved from 0.74327 to 0.74429, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.9694 - accuracy: 0.7783 - val_loss: 1.1029 - val_accuracy: 0.7443 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9650 - accuracy: 0.7787\n",
      "Epoch 00045: val_accuracy improved from 0.74429 to 0.74455, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 0.9655 - accuracy: 0.7784 - val_loss: 1.0979 - val_accuracy: 0.7445 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9517 - accuracy: 0.7844\n",
      "Epoch 00046: val_accuracy improved from 0.74455 to 0.74506, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.9531 - accuracy: 0.7837 - val_loss: 1.0939 - val_accuracy: 0.7451 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9455 - accuracy: 0.7846\n",
      "Epoch 00047: val_accuracy improved from 0.74506 to 0.74635, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.9475 - accuracy: 0.7837 - val_loss: 1.0931 - val_accuracy: 0.7463 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9316 - accuracy: 0.7850\n",
      "Epoch 00048: val_accuracy did not improve from 0.74635\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.9315 - accuracy: 0.7855 - val_loss: 1.0898 - val_accuracy: 0.7453 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9282 - accuracy: 0.7901\n",
      "Epoch 00049: val_accuracy improved from 0.74635 to 0.74917, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 0.9292 - accuracy: 0.7900 - val_loss: 1.0871 - val_accuracy: 0.7492 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9354 - accuracy: 0.7846\n",
      "Epoch 00050: val_accuracy did not improve from 0.74917\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9354 - accuracy: 0.7846 - val_loss: 1.0874 - val_accuracy: 0.7481 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9226 - accuracy: 0.7893\n",
      "Epoch 00051: val_accuracy improved from 0.74917 to 0.75096, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.9226 - accuracy: 0.7893 - val_loss: 1.0788 - val_accuracy: 0.7510 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9231 - accuracy: 0.7931\n",
      "Epoch 00052: val_accuracy did not improve from 0.75096\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9231 - accuracy: 0.7931 - val_loss: 1.0841 - val_accuracy: 0.7499 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9175 - accuracy: 0.7929\n",
      "Epoch 00053: val_accuracy did not improve from 0.75096\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.9160 - accuracy: 0.7933 - val_loss: 1.0786 - val_accuracy: 0.7469 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9141 - accuracy: 0.7968\n",
      "Epoch 00054: val_accuracy did not improve from 0.75096\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9141 - accuracy: 0.7968 - val_loss: 1.0799 - val_accuracy: 0.7471 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9125 - accuracy: 0.7917\n",
      "Epoch 00055: val_accuracy did not improve from 0.75096\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.9170 - accuracy: 0.7907 - val_loss: 1.0755 - val_accuracy: 0.7458 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9082 - accuracy: 0.7957\n",
      "Epoch 00056: val_accuracy improved from 0.75096 to 0.75122, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.9082 - accuracy: 0.7957 - val_loss: 1.0841 - val_accuracy: 0.7512 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9108 - accuracy: 0.7921\n",
      "Epoch 00057: val_accuracy improved from 0.75122 to 0.75250, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 0.9114 - accuracy: 0.7921 - val_loss: 1.0742 - val_accuracy: 0.7525 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9023 - accuracy: 0.7987\n",
      "Epoch 00058: val_accuracy did not improve from 0.75250\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.9068 - accuracy: 0.7973 - val_loss: 1.0717 - val_accuracy: 0.7525 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8890 - accuracy: 0.7987\n",
      "Epoch 00059: val_accuracy did not improve from 0.75250\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8894 - accuracy: 0.7983 - val_loss: 1.0734 - val_accuracy: 0.7497 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8964 - accuracy: 0.7966\n",
      "Epoch 00060: val_accuracy improved from 0.75250 to 0.75404, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.8978 - accuracy: 0.7961 - val_loss: 1.0681 - val_accuracy: 0.7540 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8793 - accuracy: 0.8044\n",
      "Epoch 00061: val_accuracy improved from 0.75404 to 0.75583, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.8800 - accuracy: 0.8043 - val_loss: 1.0619 - val_accuracy: 0.7558 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8809 - accuracy: 0.8032\n",
      "Epoch 00062: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8809 - accuracy: 0.8032 - val_loss: 1.0651 - val_accuracy: 0.7551 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8795 - accuracy: 0.8000\n",
      "Epoch 00063: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8795 - accuracy: 0.8000 - val_loss: 1.0687 - val_accuracy: 0.7535 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8922 - accuracy: 0.8032\n",
      "Epoch 00064: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8922 - accuracy: 0.8032 - val_loss: 1.0751 - val_accuracy: 0.7494 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8703 - accuracy: 0.8040\n",
      "Epoch 00065: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8733 - accuracy: 0.8037 - val_loss: 1.0654 - val_accuracy: 0.7489 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8742 - accuracy: 0.8039\n",
      "Epoch 00066: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8747 - accuracy: 0.8035 - val_loss: 1.0712 - val_accuracy: 0.7499 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8749 - accuracy: 0.8012\n",
      "Epoch 00067: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.8739 - accuracy: 0.8015 - val_loss: 1.0627 - val_accuracy: 0.7551 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8699 - accuracy: 0.8060\n",
      "Epoch 00068: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8709 - accuracy: 0.8054 - val_loss: 1.0626 - val_accuracy: 0.7540 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8684 - accuracy: 0.8060\n",
      "Epoch 00069: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8677 - accuracy: 0.8060 - val_loss: 1.0547 - val_accuracy: 0.7522 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8615 - accuracy: 0.8109\n",
      "Epoch 00070: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8631 - accuracy: 0.8102 - val_loss: 1.0631 - val_accuracy: 0.7502 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8588 - accuracy: 0.8081\n",
      "Epoch 00071: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8588 - accuracy: 0.8081 - val_loss: 1.0642 - val_accuracy: 0.7528 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8533 - accuracy: 0.8118\n",
      "Epoch 00072: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8539 - accuracy: 0.8115 - val_loss: 1.0592 - val_accuracy: 0.7533 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8566 - accuracy: 0.8087\n",
      "Epoch 00073: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8578 - accuracy: 0.8087 - val_loss: 1.0620 - val_accuracy: 0.7497 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8617 - accuracy: 0.8074\n",
      "Epoch 00074: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8650 - accuracy: 0.8067 - val_loss: 1.0617 - val_accuracy: 0.7528 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8469 - accuracy: 0.8124\n",
      "Epoch 00075: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8528 - accuracy: 0.8107 - val_loss: 1.0590 - val_accuracy: 0.7556 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8698 - accuracy: 0.8065\n",
      "Epoch 00076: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8698 - accuracy: 0.8065 - val_loss: 1.0583 - val_accuracy: 0.7556 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8571 - accuracy: 0.8077\n",
      "Epoch 00077: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8580 - accuracy: 0.8077 - val_loss: 1.0621 - val_accuracy: 0.7512 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8496 - accuracy: 0.8104\n",
      "Epoch 00078: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8478 - accuracy: 0.8110 - val_loss: 1.0622 - val_accuracy: 0.7522 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8390 - accuracy: 0.8144\n",
      "Epoch 00079: val_accuracy improved from 0.75583 to 0.75660, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.8390 - accuracy: 0.8144 - val_loss: 1.0508 - val_accuracy: 0.7566 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8408 - accuracy: 0.8119\n",
      "Epoch 00080: val_accuracy did not improve from 0.75660\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.8397 - accuracy: 0.8126 - val_loss: 1.0512 - val_accuracy: 0.7533 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8351 - accuracy: 0.8159\n",
      "Epoch 00081: val_accuracy did not improve from 0.75660\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8351 - accuracy: 0.8159 - val_loss: 1.0521 - val_accuracy: 0.7551 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8261 - accuracy: 0.8203\n",
      "Epoch 00082: val_accuracy did not improve from 0.75660\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8272 - accuracy: 0.8202 - val_loss: 1.0520 - val_accuracy: 0.7540 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8390 - accuracy: 0.8101\n",
      "Epoch 00083: val_accuracy did not improve from 0.75660\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8388 - accuracy: 0.8103 - val_loss: 1.0534 - val_accuracy: 0.7540 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8251 - accuracy: 0.8176\n",
      "Epoch 00084: val_accuracy improved from 0.75660 to 0.75840, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.8263 - accuracy: 0.8169 - val_loss: 1.0511 - val_accuracy: 0.7584 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8388 - accuracy: 0.8164\n",
      "Epoch 00085: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.8405 - accuracy: 0.8161 - val_loss: 1.0542 - val_accuracy: 0.7556 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8248 - accuracy: 0.8213\n",
      "Epoch 00086: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8263 - accuracy: 0.8206 - val_loss: 1.0490 - val_accuracy: 0.7551 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8203 - accuracy: 0.8164\n",
      "Epoch 00087: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8225 - accuracy: 0.8157 - val_loss: 1.0515 - val_accuracy: 0.7548 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8280 - accuracy: 0.8189\n",
      "Epoch 00088: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8280 - accuracy: 0.8189 - val_loss: 1.0498 - val_accuracy: 0.7538 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8264 - accuracy: 0.8180\n",
      "Epoch 00089: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.8271 - accuracy: 0.8173 - val_loss: 1.0472 - val_accuracy: 0.7548 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8294 - accuracy: 0.8184\n",
      "Epoch 00090: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8308 - accuracy: 0.8179 - val_loss: 1.0482 - val_accuracy: 0.7553 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8228 - accuracy: 0.8171\n",
      "Epoch 00091: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8241 - accuracy: 0.8170 - val_loss: 1.0614 - val_accuracy: 0.7528 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8314 - accuracy: 0.8187\n",
      "Epoch 00092: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8314 - accuracy: 0.8187 - val_loss: 1.0530 - val_accuracy: 0.7538 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8203 - accuracy: 0.8126\n",
      "Epoch 00093: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8203 - accuracy: 0.8126 - val_loss: 1.0477 - val_accuracy: 0.7558 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8277 - accuracy: 0.8133\n",
      "Epoch 00094: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8284 - accuracy: 0.8129 - val_loss: 1.0489 - val_accuracy: 0.7546 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8233 - accuracy: 0.8167\n",
      "Epoch 00095: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8250 - accuracy: 0.8158 - val_loss: 1.0491 - val_accuracy: 0.7571 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8267 - accuracy: 0.8186\n",
      "Epoch 00096: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8271 - accuracy: 0.8187 - val_loss: 1.0472 - val_accuracy: 0.7546 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8134 - accuracy: 0.8186\n",
      "Epoch 00097: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8136 - accuracy: 0.8186 - val_loss: 1.0491 - val_accuracy: 0.7546 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8245 - accuracy: 0.8190\n",
      "Epoch 00098: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8227 - accuracy: 0.8194 - val_loss: 1.0489 - val_accuracy: 0.7533 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8300 - accuracy: 0.8158\n",
      "Epoch 00099: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8303 - accuracy: 0.8158 - val_loss: 1.0492 - val_accuracy: 0.7553 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8115 - accuracy: 0.8250\n",
      "Epoch 00100: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8149 - accuracy: 0.8242 - val_loss: 1.0461 - val_accuracy: 0.7543 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8284 - accuracy: 0.8147\n",
      "Epoch 00101: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8299 - accuracy: 0.8149 - val_loss: 1.0487 - val_accuracy: 0.7551 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8287 - accuracy: 0.8140\n",
      "Epoch 00102: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8282 - accuracy: 0.8140 - val_loss: 1.0481 - val_accuracy: 0.7546 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8156 - accuracy: 0.8214\n",
      "Epoch 00103: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8154 - accuracy: 0.8213 - val_loss: 1.0482 - val_accuracy: 0.7548 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8088 - accuracy: 0.8230\n",
      "Epoch 00104: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8109 - accuracy: 0.8222 - val_loss: 1.0487 - val_accuracy: 0.7546 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8134 - accuracy: 0.8194\n",
      "Epoch 00105: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8140 - accuracy: 0.8192 - val_loss: 1.0474 - val_accuracy: 0.7525 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8218 - accuracy: 0.8178\n",
      "Epoch 00106: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8213 - accuracy: 0.8179 - val_loss: 1.0492 - val_accuracy: 0.7533 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8211 - accuracy: 0.8166\n",
      "Epoch 00107: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8212 - accuracy: 0.8169 - val_loss: 1.0507 - val_accuracy: 0.7543 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8163 - accuracy: 0.8199\n",
      "Epoch 00108: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.8186 - accuracy: 0.8187 - val_loss: 1.0458 - val_accuracy: 0.7558 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8173 - accuracy: 0.8209\n",
      "Epoch 00109: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8178 - accuracy: 0.8205 - val_loss: 1.0491 - val_accuracy: 0.7520 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8095 - accuracy: 0.8203\n",
      "Epoch 00110: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8112 - accuracy: 0.8193 - val_loss: 1.0505 - val_accuracy: 0.7535 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8093 - accuracy: 0.8202\n",
      "Epoch 00111: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8093 - accuracy: 0.8202 - val_loss: 1.0498 - val_accuracy: 0.7556 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8218 - accuracy: 0.8193\n",
      "Epoch 00112: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8204 - accuracy: 0.8199 - val_loss: 1.0469 - val_accuracy: 0.7546 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8130 - accuracy: 0.8193\n",
      "Epoch 00113: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.8144 - accuracy: 0.8190 - val_loss: 1.0444 - val_accuracy: 0.7556 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8174 - accuracy: 0.8210\n",
      "Epoch 00114: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8187 - accuracy: 0.8206 - val_loss: 1.0468 - val_accuracy: 0.7540 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8186 - accuracy: 0.8162\n",
      "Epoch 00115: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.8197 - accuracy: 0.8162 - val_loss: 1.0425 - val_accuracy: 0.7561 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8237 - accuracy: 0.8181\n",
      "Epoch 00116: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8237 - accuracy: 0.8181 - val_loss: 1.0442 - val_accuracy: 0.7561 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8192 - accuracy: 0.8212\n",
      "Epoch 00117: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8201 - accuracy: 0.8206 - val_loss: 1.0443 - val_accuracy: 0.7569 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8132 - accuracy: 0.8209\n",
      "Epoch 00118: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.8169 - accuracy: 0.8201 - val_loss: 1.0440 - val_accuracy: 0.7556 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8193 - accuracy: 0.8169\n",
      "Epoch 00119: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8223 - accuracy: 0.8160 - val_loss: 1.0433 - val_accuracy: 0.7558 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8179 - accuracy: 0.8164\n",
      "Epoch 00120: val_accuracy did not improve from 0.75840\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8183 - accuracy: 0.8162 - val_loss: 1.0447 - val_accuracy: 0.7551 - lr: 1.0000e-06\n",
      "epoch_number 84\n",
      "train accuracy and validation accuracy 0.8169029355049133 0.7583996057510376\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.0511 - accuracy: 0.7584\n",
      "test_accuracy 0.7583996057510376\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376]\n",
      "0.7192613482475281\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S6_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S6_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.3196 - accuracy: 0.1639\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.28443, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 366ms/step - loss: 4.3061 - accuracy: 0.1651 - val_loss: 3.1008 - val_accuracy: 0.2844 - lr: 0.0010\n",
      "Epoch 2/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.7001 - accuracy: 0.3179\n",
      "Epoch 00002: val_accuracy improved from 0.28443 to 0.44216, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 2.7001 - accuracy: 0.3179 - val_loss: 2.1547 - val_accuracy: 0.4422 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2895 - accuracy: 0.4137\n",
      "Epoch 00003: val_accuracy improved from 0.44216 to 0.51680, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 2.2893 - accuracy: 0.4137 - val_loss: 1.8927 - val_accuracy: 0.5168 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0150 - accuracy: 0.4787 ETA: 0s - loss: 2.0616 - accu\n",
      "Epoch 00004: val_accuracy improved from 0.51680 to 0.58553, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 132ms/step - loss: 2.0152 - accuracy: 0.4795 - val_loss: 1.6718 - val_accuracy: 0.5855 - lr: 0.0010\n",
      "Epoch 5/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.8428 - accuracy: 0.5268\n",
      "Epoch 00005: val_accuracy improved from 0.58553 to 0.63144, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.8428 - accuracy: 0.5268 - val_loss: 1.5429 - val_accuracy: 0.6314 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7088 - accuracy: 0.5631 ETA: 0s - loss: 1.7078 - accuracy: 0.56\n",
      "Epoch 00006: val_accuracy improved from 0.63144 to 0.65222, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 1.7064 - accuracy: 0.5634 - val_loss: 1.4169 - val_accuracy: 0.6522 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5891 - accuracy: 0.5889\n",
      "Epoch 00007: val_accuracy improved from 0.65222 to 0.66761, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.5912 - accuracy: 0.5885 - val_loss: 1.3351 - val_accuracy: 0.6676 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5053 - accuracy: 0.6097\n",
      "Epoch 00008: val_accuracy improved from 0.66761 to 0.69172, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 1.5054 - accuracy: 0.6098 - val_loss: 1.2577 - val_accuracy: 0.6917 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4489 - accuracy: 0.6294\n",
      "Epoch 00009: val_accuracy did not improve from 0.69172\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.4463 - accuracy: 0.6302 - val_loss: 1.2395 - val_accuracy: 0.6909 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3769 - accuracy: 0.6522\n",
      "Epoch 00010: val_accuracy improved from 0.69172 to 0.70069, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.3789 - accuracy: 0.6510 - val_loss: 1.1783 - val_accuracy: 0.7007 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3488 - accuracy: 0.6594\n",
      "Epoch 00011: val_accuracy improved from 0.70069 to 0.71685, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.3490 - accuracy: 0.6588 - val_loss: 1.1419 - val_accuracy: 0.7169 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3051 - accuracy: 0.6684\n",
      "Epoch 00012: val_accuracy did not improve from 0.71685\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.3051 - accuracy: 0.6683 - val_loss: 1.1264 - val_accuracy: 0.7140 - lr: 0.0010\n",
      "Epoch 13/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2623 - accuracy: 0.6874\n",
      "Epoch 00013: val_accuracy improved from 0.71685 to 0.72275, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 1.2623 - accuracy: 0.6874 - val_loss: 1.0954 - val_accuracy: 0.7227 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2263 - accuracy: 0.6924\n",
      "Epoch 00014: val_accuracy improved from 0.72275 to 0.72993, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.2252 - accuracy: 0.6925 - val_loss: 1.0940 - val_accuracy: 0.7299 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1852 - accuracy: 0.7031\n",
      "Epoch 00015: val_accuracy improved from 0.72993 to 0.73891, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 1.1885 - accuracy: 0.7022 - val_loss: 1.0486 - val_accuracy: 0.7389 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1884 - accuracy: 0.7100\n",
      "Epoch 00016: val_accuracy improved from 0.73891 to 0.74711, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 1.1889 - accuracy: 0.7095 - val_loss: 1.0251 - val_accuracy: 0.7471 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1320 - accuracy: 0.7207\n",
      "Epoch 00017: val_accuracy improved from 0.74711 to 0.75660, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.1314 - accuracy: 0.7206 - val_loss: 1.0121 - val_accuracy: 0.7566 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1112 - accuracy: 0.7286\n",
      "Epoch 00018: val_accuracy improved from 0.75660 to 0.76122, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 1.1142 - accuracy: 0.7276 - val_loss: 1.0057 - val_accuracy: 0.7612 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1109 - accuracy: 0.7278\n",
      "Epoch 00019: val_accuracy did not improve from 0.76122\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.1130 - accuracy: 0.7268 - val_loss: 1.0089 - val_accuracy: 0.7546 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0863 - accuracy: 0.7363\n",
      "Epoch 00020: val_accuracy did not improve from 0.76122\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.0840 - accuracy: 0.7372 - val_loss: 0.9956 - val_accuracy: 0.7597 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0791 - accuracy: 0.7339\n",
      "Epoch 00021: val_accuracy did not improve from 0.76122\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.0786 - accuracy: 0.7338 - val_loss: 0.9795 - val_accuracy: 0.7610 - lr: 0.0010\n",
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0454 - accuracy: 0.7450\n",
      "Epoch 00022: val_accuracy improved from 0.76122 to 0.77071, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 103ms/step - loss: 1.0450 - accuracy: 0.7454 - val_loss: 0.9558 - val_accuracy: 0.7707 - lr: 0.0010\n",
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0239 - accuracy: 0.7527\n",
      "Epoch 00023: val_accuracy improved from 0.77071 to 0.77789, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.0236 - accuracy: 0.7532 - val_loss: 0.9283 - val_accuracy: 0.7779 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0133 - accuracy: 0.7569\n",
      "Epoch 00024: val_accuracy did not improve from 0.77789\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0137 - accuracy: 0.7567 - val_loss: 0.9508 - val_accuracy: 0.7746 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0104 - accuracy: 0.7562\n",
      "Epoch 00025: val_accuracy improved from 0.77789 to 0.78046, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.0108 - accuracy: 0.7562 - val_loss: 0.9288 - val_accuracy: 0.7805 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9968 - accuracy: 0.7627\n",
      "Epoch 00026: val_accuracy did not improve from 0.78046\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.9960 - accuracy: 0.7634 - val_loss: 0.9343 - val_accuracy: 0.7805 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9469 - accuracy: 0.7733\n",
      "Epoch 00027: val_accuracy improved from 0.78046 to 0.78533, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 0.9464 - accuracy: 0.7739 - val_loss: 0.9249 - val_accuracy: 0.7853 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9387 - accuracy: 0.7760\n",
      "Epoch 00028: val_accuracy improved from 0.78533 to 0.79559, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 0.9378 - accuracy: 0.7765 - val_loss: 0.9044 - val_accuracy: 0.7956 - lr: 0.0010\n",
      "Epoch 29/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9205 - accuracy: 0.7840\n",
      "Epoch 00029: val_accuracy did not improve from 0.79559\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.9205 - accuracy: 0.7840 - val_loss: 0.8987 - val_accuracy: 0.7889 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9078 - accuracy: 0.7880\n",
      "Epoch 00030: val_accuracy did not improve from 0.79559\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.9090 - accuracy: 0.7877 - val_loss: 0.8730 - val_accuracy: 0.7951 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9100 - accuracy: 0.7828\n",
      "Epoch 00031: val_accuracy did not improve from 0.79559\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.9108 - accuracy: 0.7826 - val_loss: 0.8940 - val_accuracy: 0.7897 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9051 - accuracy: 0.7923\n",
      "Epoch 00032: val_accuracy did not improve from 0.79559\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.9064 - accuracy: 0.7915 - val_loss: 0.8709 - val_accuracy: 0.7946 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9099 - accuracy: 0.7829\n",
      "Epoch 00033: val_accuracy did not improve from 0.79559\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9092 - accuracy: 0.7826 - val_loss: 0.8830 - val_accuracy: 0.7940 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9095 - accuracy: 0.7803\n",
      "Epoch 00034: val_accuracy improved from 0.79559 to 0.79995, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.9085 - accuracy: 0.7804 - val_loss: 0.8745 - val_accuracy: 0.7999 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8677 - accuracy: 0.7973\n",
      "Epoch 00035: val_accuracy improved from 0.79995 to 0.80046, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.8691 - accuracy: 0.7969 - val_loss: 0.8749 - val_accuracy: 0.8005 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8807 - accuracy: 0.7908\n",
      "Epoch 00036: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8833 - accuracy: 0.7904 - val_loss: 0.8837 - val_accuracy: 0.7869 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8951 - accuracy: 0.7910\n",
      "Epoch 00037: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8965 - accuracy: 0.7903 - val_loss: 0.8785 - val_accuracy: 0.7876 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8779 - accuracy: 0.7908\n",
      "Epoch 00038: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.8778 - accuracy: 0.7907 - val_loss: 0.8608 - val_accuracy: 0.7999 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8561 - accuracy: 0.8003\n",
      "Epoch 00039: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8562 - accuracy: 0.8006 - val_loss: 0.8746 - val_accuracy: 0.7938 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8136 - accuracy: 0.8123\n",
      "Epoch 00040: val_accuracy improved from 0.80046 to 0.80097, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.8147 - accuracy: 0.8115 - val_loss: 0.8520 - val_accuracy: 0.8010 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7981 - accuracy: 0.8161\n",
      "Epoch 00041: val_accuracy improved from 0.80097 to 0.80739, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.7981 - accuracy: 0.8161 - val_loss: 0.8339 - val_accuracy: 0.8074 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7943 - accuracy: 0.8189\n",
      "Epoch 00042: val_accuracy improved from 0.80739 to 0.80918, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.7943 - accuracy: 0.8189 - val_loss: 0.8326 - val_accuracy: 0.8092 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7632 - accuracy: 0.8264\n",
      "Epoch 00043: val_accuracy improved from 0.80918 to 0.81021, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.7637 - accuracy: 0.8261 - val_loss: 0.8206 - val_accuracy: 0.8102 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7598 - accuracy: 0.8297\n",
      "Epoch 00044: val_accuracy did not improve from 0.81021\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7613 - accuracy: 0.8295 - val_loss: 0.8214 - val_accuracy: 0.8066 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7600 - accuracy: 0.8282\n",
      "Epoch 00045: val_accuracy did not improve from 0.81021\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.7650 - accuracy: 0.8270 - val_loss: 0.8161 - val_accuracy: 0.8094 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7510 - accuracy: 0.8337\n",
      "Epoch 00046: val_accuracy improved from 0.81021 to 0.81200, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 107ms/step - loss: 0.7520 - accuracy: 0.8328 - val_loss: 0.8177 - val_accuracy: 0.8120 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7514 - accuracy: 0.8346\n",
      "Epoch 00047: val_accuracy improved from 0.81200 to 0.81252, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.7514 - accuracy: 0.8346 - val_loss: 0.8120 - val_accuracy: 0.8125 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7390 - accuracy: 0.8349\n",
      "Epoch 00048: val_accuracy improved from 0.81252 to 0.81482, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.7408 - accuracy: 0.8343 - val_loss: 0.8059 - val_accuracy: 0.8148 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7311 - accuracy: 0.8386\n",
      "Epoch 00049: val_accuracy did not improve from 0.81482\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.7320 - accuracy: 0.8381 - val_loss: 0.8105 - val_accuracy: 0.8143 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7270 - accuracy: 0.8422\n",
      "Epoch 00050: val_accuracy improved from 0.81482 to 0.81688, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.7270 - accuracy: 0.8422 - val_loss: 0.8037 - val_accuracy: 0.8169 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7341 - accuracy: 0.8367\n",
      "Epoch 00051: val_accuracy improved from 0.81688 to 0.81944, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 0.7341 - accuracy: 0.8367 - val_loss: 0.8079 - val_accuracy: 0.8194 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7192 - accuracy: 0.8422\n",
      "Epoch 00052: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7177 - accuracy: 0.8427 - val_loss: 0.8056 - val_accuracy: 0.8153 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7259 - accuracy: 0.8418\n",
      "Epoch 00053: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.7273 - accuracy: 0.8412 - val_loss: 0.8057 - val_accuracy: 0.8176 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7168 - accuracy: 0.8400\n",
      "Epoch 00054: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.7168 - accuracy: 0.8400 - val_loss: 0.7989 - val_accuracy: 0.8153 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7198 - accuracy: 0.8388\n",
      "Epoch 00055: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 0.7198 - accuracy: 0.8388 - val_loss: 0.7959 - val_accuracy: 0.8187 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7112 - accuracy: 0.8443\n",
      "Epoch 00056: val_accuracy improved from 0.81944 to 0.82124, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 0.7126 - accuracy: 0.8438 - val_loss: 0.7908 - val_accuracy: 0.8212 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7099 - accuracy: 0.8458\n",
      "Epoch 00057: val_accuracy did not improve from 0.82124\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.7099 - accuracy: 0.8458 - val_loss: 0.7942 - val_accuracy: 0.8194 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7056 - accuracy: 0.8440\n",
      "Epoch 00058: val_accuracy improved from 0.82124 to 0.82252, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 0.7070 - accuracy: 0.8439 - val_loss: 0.7911 - val_accuracy: 0.8225 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6927 - accuracy: 0.8497\n",
      "Epoch 00059: val_accuracy improved from 0.82252 to 0.82431, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.6954 - accuracy: 0.8490 - val_loss: 0.7864 - val_accuracy: 0.8243 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6976 - accuracy: 0.8473\n",
      "Epoch 00060: val_accuracy did not improve from 0.82431\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.6974 - accuracy: 0.8472 - val_loss: 0.7864 - val_accuracy: 0.8241 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6902 - accuracy: 0.8486\n",
      "Epoch 00061: val_accuracy did not improve from 0.82431\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.6902 - accuracy: 0.8486 - val_loss: 0.7829 - val_accuracy: 0.8238 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7037 - accuracy: 0.8392\n",
      "Epoch 00062: val_accuracy improved from 0.82431 to 0.82534, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 0.7037 - accuracy: 0.8392 - val_loss: 0.7796 - val_accuracy: 0.8253 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6949 - accuracy: 0.8484\n",
      "Epoch 00063: val_accuracy did not improve from 0.82534\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6949 - accuracy: 0.8484 - val_loss: 0.7815 - val_accuracy: 0.8246 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6876 - accuracy: 0.8522\n",
      "Epoch 00064: val_accuracy did not improve from 0.82534\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6890 - accuracy: 0.8519 - val_loss: 0.7854 - val_accuracy: 0.8223 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6882 - accuracy: 0.8486\n",
      "Epoch 00065: val_accuracy did not improve from 0.82534\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6882 - accuracy: 0.8486 - val_loss: 0.7812 - val_accuracy: 0.8228 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6812 - accuracy: 0.8524\n",
      "Epoch 00066: val_accuracy did not improve from 0.82534\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.6854 - accuracy: 0.8509 - val_loss: 0.7883 - val_accuracy: 0.8205 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6842 - accuracy: 0.8484\n",
      "Epoch 00067: val_accuracy did not improve from 0.82534\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.6853 - accuracy: 0.8482 - val_loss: 0.7863 - val_accuracy: 0.8230 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6776 - accuracy: 0.8547\n",
      "Epoch 00068: val_accuracy did not improve from 0.82534\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6787 - accuracy: 0.8547 - val_loss: 0.7805 - val_accuracy: 0.8212 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6799 - accuracy: 0.8521\n",
      "Epoch 00069: val_accuracy improved from 0.82534 to 0.82688, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.6801 - accuracy: 0.8517 - val_loss: 0.7795 - val_accuracy: 0.8269 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6836 - accuracy: 0.8537\n",
      "Epoch 00070: val_accuracy improved from 0.82688 to 0.82714, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 113ms/step - loss: 0.6848 - accuracy: 0.8537 - val_loss: 0.7758 - val_accuracy: 0.8271 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6751 - accuracy: 0.8538\n",
      "Epoch 00071: val_accuracy did not improve from 0.82714\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 0.6751 - accuracy: 0.8540 - val_loss: 0.7745 - val_accuracy: 0.8246 - lr: 1.0000e-04\n",
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6692 - accuracy: 0.8569\n",
      "Epoch 00072: val_accuracy did not improve from 0.82714\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 0.6711 - accuracy: 0.8561 - val_loss: 0.7743 - val_accuracy: 0.8269 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6808 - accuracy: 0.8519\n",
      "Epoch 00073: val_accuracy did not improve from 0.82714\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6808 - accuracy: 0.8519 - val_loss: 0.7765 - val_accuracy: 0.8256 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6694 - accuracy: 0.8546\n",
      "Epoch 00074: val_accuracy did not improve from 0.82714\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.6710 - accuracy: 0.8544 - val_loss: 0.7783 - val_accuracy: 0.8241 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6671 - accuracy: 0.8561\n",
      "Epoch 00075: val_accuracy did not improve from 0.82714\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.6686 - accuracy: 0.8556 - val_loss: 0.7749 - val_accuracy: 0.8251 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6637 - accuracy: 0.8586\n",
      "Epoch 00076: val_accuracy did not improve from 0.82714\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.6626 - accuracy: 0.8586 - val_loss: 0.7810 - val_accuracy: 0.8241 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6549 - accuracy: 0.8590\n",
      "Epoch 00077: val_accuracy did not improve from 0.82714\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.6544 - accuracy: 0.8591 - val_loss: 0.7744 - val_accuracy: 0.8225 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6564 - accuracy: 0.8627\n",
      "Epoch 00078: val_accuracy did not improve from 0.82714\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 0.6564 - accuracy: 0.8627 - val_loss: 0.7717 - val_accuracy: 0.8246 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6605 - accuracy: 0.8568\n",
      "Epoch 00079: val_accuracy did not improve from 0.82714\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.6629 - accuracy: 0.8558 - val_loss: 0.7742 - val_accuracy: 0.8269 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6639 - accuracy: 0.8540\n",
      "Epoch 00080: val_accuracy improved from 0.82714 to 0.82790, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 0.6648 - accuracy: 0.8538 - val_loss: 0.7745 - val_accuracy: 0.8279 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6703 - accuracy: 0.8552\n",
      "Epoch 00081: val_accuracy improved from 0.82790 to 0.82919, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 2s 172ms/step - loss: 0.6705 - accuracy: 0.8551 - val_loss: 0.7710 - val_accuracy: 0.8292 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6520 - accuracy: 0.8630\n",
      "Epoch 00082: val_accuracy improved from 0.82919 to 0.82970, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 0.6535 - accuracy: 0.8630 - val_loss: 0.7713 - val_accuracy: 0.8297 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6466 - accuracy: 0.8608\n",
      "Epoch 00083: val_accuracy did not improve from 0.82970\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.6498 - accuracy: 0.8593 - val_loss: 0.7677 - val_accuracy: 0.8287 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6582 - accuracy: 0.8563\n",
      "Epoch 00084: val_accuracy did not improve from 0.82970\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.6596 - accuracy: 0.8557 - val_loss: 0.7716 - val_accuracy: 0.8264 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6560 - accuracy: 0.8576\n",
      "Epoch 00085: val_accuracy did not improve from 0.82970\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.6581 - accuracy: 0.8570 - val_loss: 0.7673 - val_accuracy: 0.8292 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6549 - accuracy: 0.8603\n",
      "Epoch 00086: val_accuracy did not improve from 0.82970\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.6552 - accuracy: 0.8602 - val_loss: 0.7657 - val_accuracy: 0.8279 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6551 - accuracy: 0.8576\n",
      "Epoch 00087: val_accuracy did not improve from 0.82970\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.6551 - accuracy: 0.8576 - val_loss: 0.7652 - val_accuracy: 0.8294 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6583 - accuracy: 0.8587\n",
      "Epoch 00088: val_accuracy did not improve from 0.82970\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.6604 - accuracy: 0.8582 - val_loss: 0.7668 - val_accuracy: 0.8284 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6443 - accuracy: 0.8657\n",
      "Epoch 00089: val_accuracy did not improve from 0.82970\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6462 - accuracy: 0.8653 - val_loss: 0.7673 - val_accuracy: 0.8287 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6498 - accuracy: 0.8593\n",
      "Epoch 00090: val_accuracy did not improve from 0.82970\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.6498 - accuracy: 0.8593 - val_loss: 0.7688 - val_accuracy: 0.8269 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6469 - accuracy: 0.8618\n",
      "Epoch 00091: val_accuracy did not improve from 0.82970\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.6493 - accuracy: 0.8613 - val_loss: 0.7673 - val_accuracy: 0.8284 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6425 - accuracy: 0.8642\n",
      "Epoch 00092: val_accuracy improved from 0.82970 to 0.82996, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.6424 - accuracy: 0.8643 - val_loss: 0.7665 - val_accuracy: 0.8300 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6442 - accuracy: 0.8626\n",
      "Epoch 00093: val_accuracy improved from 0.82996 to 0.83175, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 128ms/step - loss: 0.6449 - accuracy: 0.8624 - val_loss: 0.7638 - val_accuracy: 0.8318 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6355 - accuracy: 0.8687\n",
      "Epoch 00094: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.6355 - accuracy: 0.8687 - val_loss: 0.7678 - val_accuracy: 0.8315 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6521 - accuracy: 0.8603\n",
      "Epoch 00095: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.6521 - accuracy: 0.8603 - val_loss: 0.7679 - val_accuracy: 0.8307 - lr: 1.0000e-05\n",
      "Epoch 96/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6412 - accuracy: 0.8656\n",
      "Epoch 00096: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6434 - accuracy: 0.8647 - val_loss: 0.7663 - val_accuracy: 0.8292 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6433 - accuracy: 0.8624\n",
      "Epoch 00097: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.6444 - accuracy: 0.8622 - val_loss: 0.7642 - val_accuracy: 0.8307 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6483 - accuracy: 0.8603\n",
      "Epoch 00098: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6491 - accuracy: 0.8602 - val_loss: 0.7658 - val_accuracy: 0.8300 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6471 - accuracy: 0.8579\n",
      "Epoch 00099: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.6480 - accuracy: 0.8572 - val_loss: 0.7645 - val_accuracy: 0.8302 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6485 - accuracy: 0.8617\n",
      "Epoch 00100: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.6500 - accuracy: 0.8606 - val_loss: 0.7657 - val_accuracy: 0.8287 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6392 - accuracy: 0.8633\n",
      "Epoch 00101: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6401 - accuracy: 0.8633 - val_loss: 0.7671 - val_accuracy: 0.8266 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6414 - accuracy: 0.8633\n",
      "Epoch 00102: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6411 - accuracy: 0.8635 - val_loss: 0.7641 - val_accuracy: 0.8292 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6396 - accuracy: 0.8643\n",
      "Epoch 00103: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.6396 - accuracy: 0.8643 - val_loss: 0.7659 - val_accuracy: 0.8284 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6421 - accuracy: 0.8617\n",
      "Epoch 00104: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 0.6421 - accuracy: 0.8617 - val_loss: 0.7637 - val_accuracy: 0.8300 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6476 - accuracy: 0.8629\n",
      "Epoch 00105: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 0.6471 - accuracy: 0.8632 - val_loss: 0.7637 - val_accuracy: 0.8279 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6515 - accuracy: 0.8593\n",
      "Epoch 00106: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.6509 - accuracy: 0.8595 - val_loss: 0.7634 - val_accuracy: 0.8302 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6373 - accuracy: 0.8656\n",
      "Epoch 00107: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.6373 - accuracy: 0.8656 - val_loss: 0.7629 - val_accuracy: 0.8284 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6395 - accuracy: 0.8637\n",
      "Epoch 00108: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.6393 - accuracy: 0.8637 - val_loss: 0.7633 - val_accuracy: 0.8284 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6379 - accuracy: 0.8674\n",
      "Epoch 00109: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6380 - accuracy: 0.8675 - val_loss: 0.7649 - val_accuracy: 0.8292 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6459 - accuracy: 0.8618\n",
      "Epoch 00110: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6467 - accuracy: 0.8614 - val_loss: 0.7671 - val_accuracy: 0.8312 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6487 - accuracy: 0.8583\n",
      "Epoch 00111: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6487 - accuracy: 0.8583 - val_loss: 0.7646 - val_accuracy: 0.8315 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6522 - accuracy: 0.8611\n",
      "Epoch 00112: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.6516 - accuracy: 0.8612 - val_loss: 0.7648 - val_accuracy: 0.8302 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6346 - accuracy: 0.8621\n",
      "Epoch 00113: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.6363 - accuracy: 0.8616 - val_loss: 0.7646 - val_accuracy: 0.8302 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6470 - accuracy: 0.8613\n",
      "Epoch 00114: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6470 - accuracy: 0.8613 - val_loss: 0.7641 - val_accuracy: 0.8294 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6403 - accuracy: 0.8654\n",
      "Epoch 00115: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.6403 - accuracy: 0.8654 - val_loss: 0.7629 - val_accuracy: 0.8294 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6268 - accuracy: 0.8699\n",
      "Epoch 00116: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.6268 - accuracy: 0.8699 - val_loss: 0.7665 - val_accuracy: 0.8266 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6422 - accuracy: 0.8627\n",
      "Epoch 00117: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6433 - accuracy: 0.8625 - val_loss: 0.7637 - val_accuracy: 0.8279 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6381 - accuracy: 0.8622\n",
      "Epoch 00118: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.6376 - accuracy: 0.8624 - val_loss: 0.7635 - val_accuracy: 0.8292 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6310 - accuracy: 0.8669\n",
      "Epoch 00119: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.6345 - accuracy: 0.8659 - val_loss: 0.7612 - val_accuracy: 0.8289 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6408 - accuracy: 0.8639\n",
      "Epoch 00120: val_accuracy did not improve from 0.83175\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6407 - accuracy: 0.8638 - val_loss: 0.7616 - val_accuracy: 0.8294 - lr: 1.0000e-06\n",
      "epoch_number 93\n",
      "train accuracy and validation accuracy 0.8624024391174316 0.8317517042160034\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.7638 - accuracy: 0.8318\n",
      "test_accuracy 0.8317517042160034\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034]\n",
      "0.7380097409089407\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S7_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S7_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 4.3396 - accuracy: 0.1894\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.29264, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 358ms/step - loss: 4.3396 - accuracy: 0.1894 - val_loss: 3.1831 - val_accuracy: 0.2926 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.5415 - accuracy: 0.3516\n",
      "Epoch 00002: val_accuracy improved from 0.29264 to 0.49064, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 2.5407 - accuracy: 0.3516 - val_loss: 1.9921 - val_accuracy: 0.4906 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0998 - accuracy: 0.4654\n",
      "Epoch 00003: val_accuracy improved from 0.49064 to 0.59759, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 2.1025 - accuracy: 0.4650 - val_loss: 1.6796 - val_accuracy: 0.5976 - lr: 0.0010\n",
      "Epoch 4/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.8783 - accuracy: 0.5297\n",
      "Epoch 00004: val_accuracy improved from 0.59759 to 0.64991, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 1.8783 - accuracy: 0.5297 - val_loss: 1.5102 - val_accuracy: 0.6499 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6842 - accuracy: 0.5692\n",
      "Epoch 00005: val_accuracy improved from 0.64991 to 0.69325, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 1.6831 - accuracy: 0.5698 - val_loss: 1.3476 - val_accuracy: 0.6933 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5277 - accuracy: 0.6134\n",
      "Epoch 00006: val_accuracy improved from 0.69325 to 0.72095, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.5275 - accuracy: 0.6131 - val_loss: 1.2158 - val_accuracy: 0.7210 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4380 - accuracy: 0.6403\n",
      "Epoch 00007: val_accuracy improved from 0.72095 to 0.73378, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 1.4365 - accuracy: 0.6407 - val_loss: 1.1512 - val_accuracy: 0.7338 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3467 - accuracy: 0.6683\n",
      "Epoch 00008: val_accuracy improved from 0.73378 to 0.75301, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.3498 - accuracy: 0.6673 - val_loss: 1.0754 - val_accuracy: 0.7530 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2695 - accuracy: 0.6892\n",
      "Epoch 00009: val_accuracy improved from 0.75301 to 0.76276, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.2681 - accuracy: 0.6894 - val_loss: 1.0296 - val_accuracy: 0.7628 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1896 - accuracy: 0.7064\n",
      "Epoch 00010: val_accuracy improved from 0.76276 to 0.77840, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.1898 - accuracy: 0.7068 - val_loss: 0.9821 - val_accuracy: 0.7784 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1399 - accuracy: 0.7218\n",
      "Epoch 00011: val_accuracy improved from 0.77840 to 0.78251, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.1404 - accuracy: 0.7215 - val_loss: 0.9576 - val_accuracy: 0.7825 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1073 - accuracy: 0.7288\n",
      "Epoch 00012: val_accuracy improved from 0.78251 to 0.79251, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.1065 - accuracy: 0.7288 - val_loss: 0.9186 - val_accuracy: 0.7925 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0650 - accuracy: 0.7423\n",
      "Epoch 00013: val_accuracy improved from 0.79251 to 0.80893, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.0645 - accuracy: 0.7424 - val_loss: 0.8878 - val_accuracy: 0.8089 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0368 - accuracy: 0.7462\n",
      "Epoch 00014: val_accuracy did not improve from 0.80893\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.0342 - accuracy: 0.7471 - val_loss: 0.8799 - val_accuracy: 0.8007 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9889 - accuracy: 0.7613\n",
      "Epoch 00015: val_accuracy improved from 0.80893 to 0.81226, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 0.9909 - accuracy: 0.7609 - val_loss: 0.8424 - val_accuracy: 0.8123 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9574 - accuracy: 0.7734\n",
      "Epoch 00016: val_accuracy improved from 0.81226 to 0.81431, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.9575 - accuracy: 0.7736 - val_loss: 0.8422 - val_accuracy: 0.8143 - lr: 0.0010\n",
      "Epoch 17/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9369 - accuracy: 0.7755\n",
      "Epoch 00017: val_accuracy improved from 0.81431 to 0.82226, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.9369 - accuracy: 0.7755 - val_loss: 0.8169 - val_accuracy: 0.8223 - lr: 0.0010\n",
      "Epoch 18/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9223 - accuracy: 0.7881\n",
      "Epoch 00018: val_accuracy improved from 0.82226 to 0.83278, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.9223 - accuracy: 0.7881 - val_loss: 0.7997 - val_accuracy: 0.8328 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8875 - accuracy: 0.7970\n",
      "Epoch 00019: val_accuracy did not improve from 0.83278\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.8881 - accuracy: 0.7968 - val_loss: 0.7994 - val_accuracy: 0.8233 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8810 - accuracy: 0.7924\n",
      "Epoch 00020: val_accuracy did not improve from 0.83278\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8815 - accuracy: 0.7921 - val_loss: 0.8091 - val_accuracy: 0.8261 - lr: 0.0010\n",
      "Epoch 21/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8693 - accuracy: 0.7994\n",
      "Epoch 00021: val_accuracy improved from 0.83278 to 0.83329, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.8693 - accuracy: 0.7994 - val_loss: 0.7953 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 22/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8630 - accuracy: 0.8009\n",
      "Epoch 00022: val_accuracy did not improve from 0.83329\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.8647 - accuracy: 0.8004 - val_loss: 0.7869 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8488 - accuracy: 0.8063\n",
      "Epoch 00023: val_accuracy improved from 0.83329 to 0.83560, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 0.8482 - accuracy: 0.8065 - val_loss: 0.7658 - val_accuracy: 0.8356 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8491 - accuracy: 0.8053\n",
      "Epoch 00024: val_accuracy improved from 0.83560 to 0.84124, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 0.8481 - accuracy: 0.8057 - val_loss: 0.7581 - val_accuracy: 0.8412 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8009 - accuracy: 0.8217\n",
      "Epoch 00025: val_accuracy did not improve from 0.84124\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.8033 - accuracy: 0.8211 - val_loss: 0.7544 - val_accuracy: 0.8392 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8345 - accuracy: 0.8070\n",
      "Epoch 00026: val_accuracy did not improve from 0.84124\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8325 - accuracy: 0.8077 - val_loss: 0.7786 - val_accuracy: 0.8366 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7923 - accuracy: 0.8161\n",
      "Epoch 00027: val_accuracy improved from 0.84124 to 0.84432, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 0.7925 - accuracy: 0.8160 - val_loss: 0.7494 - val_accuracy: 0.8443 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7485 - accuracy: 0.8344\n",
      "Epoch 00028: val_accuracy improved from 0.84432 to 0.84586, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.7493 - accuracy: 0.8338 - val_loss: 0.7326 - val_accuracy: 0.8459 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7315 - accuracy: 0.8386\n",
      "Epoch 00029: val_accuracy did not improve from 0.84586\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.7322 - accuracy: 0.8386 - val_loss: 0.7336 - val_accuracy: 0.8453 - lr: 0.0010\n",
      "Epoch 30/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7305 - accuracy: 0.8365\n",
      "Epoch 00030: val_accuracy did not improve from 0.84586\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.7305 - accuracy: 0.8365 - val_loss: 0.7434 - val_accuracy: 0.8407 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7303 - accuracy: 0.8377\n",
      "Epoch 00031: val_accuracy improved from 0.84586 to 0.84842, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 0.7315 - accuracy: 0.8369 - val_loss: 0.7217 - val_accuracy: 0.8484 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7143 - accuracy: 0.8421\n",
      "Epoch 00032: val_accuracy improved from 0.84842 to 0.84996, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 0.7177 - accuracy: 0.8412 - val_loss: 0.7264 - val_accuracy: 0.8500 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7158 - accuracy: 0.8427\n",
      "Epoch 00033: val_accuracy did not improve from 0.84996\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.7174 - accuracy: 0.8425 - val_loss: 0.7230 - val_accuracy: 0.8410 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7070 - accuracy: 0.8440\n",
      "Epoch 00034: val_accuracy improved from 0.84996 to 0.85124, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.7084 - accuracy: 0.8438 - val_loss: 0.7135 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6957 - accuracy: 0.8467\n",
      "Epoch 00035: val_accuracy improved from 0.85124 to 0.85407, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.6955 - accuracy: 0.8470 - val_loss: 0.7162 - val_accuracy: 0.8541 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6897 - accuracy: 0.8516\n",
      "Epoch 00036: val_accuracy improved from 0.85407 to 0.85432, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.6891 - accuracy: 0.8519 - val_loss: 0.7085 - val_accuracy: 0.8543 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6733 - accuracy: 0.8567\n",
      "Epoch 00037: val_accuracy improved from 0.85432 to 0.85843, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.6750 - accuracy: 0.8555 - val_loss: 0.6886 - val_accuracy: 0.8584 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6647 - accuracy: 0.8543 ETA: 0s - loss: 0.6580 - accuracy: 0.85\n",
      "Epoch 00038: val_accuracy did not improve from 0.85843\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.6650 - accuracy: 0.8546 - val_loss: 0.7162 - val_accuracy: 0.8469 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6623 - accuracy: 0.8561\n",
      "Epoch 00039: val_accuracy did not improve from 0.85843\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.6632 - accuracy: 0.8559 - val_loss: 0.7069 - val_accuracy: 0.8515 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6417 - accuracy: 0.8687\n",
      "Epoch 00040: val_accuracy improved from 0.85843 to 0.85894, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.6421 - accuracy: 0.8687 - val_loss: 0.6821 - val_accuracy: 0.8589 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6132 - accuracy: 0.8750\n",
      "Epoch 00041: val_accuracy improved from 0.85894 to 0.86561, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.6144 - accuracy: 0.8745 - val_loss: 0.6704 - val_accuracy: 0.8656 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5908 - accuracy: 0.8802\n",
      "Epoch 00042: val_accuracy did not improve from 0.86561\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.5908 - accuracy: 0.8802 - val_loss: 0.6647 - val_accuracy: 0.8654 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5762 - accuracy: 0.8863\n",
      "Epoch 00043: val_accuracy did not improve from 0.86561\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.5762 - accuracy: 0.8863 - val_loss: 0.6639 - val_accuracy: 0.8625 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5719 - accuracy: 0.8897\n",
      "Epoch 00044: val_accuracy did not improve from 0.86561\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5737 - accuracy: 0.8891 - val_loss: 0.6642 - val_accuracy: 0.8646 - lr: 1.0000e-04\n",
      "Epoch 45/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5660 - accuracy: 0.8918\n",
      "Epoch 00045: val_accuracy did not improve from 0.86561\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.5670 - accuracy: 0.8913 - val_loss: 0.6610 - val_accuracy: 0.8651 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5631 - accuracy: 0.8870\n",
      "Epoch 00046: val_accuracy did not improve from 0.86561\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.5634 - accuracy: 0.8869 - val_loss: 0.6596 - val_accuracy: 0.8643 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5617 - accuracy: 0.8901\n",
      "Epoch 00047: val_accuracy did not improve from 0.86561\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.5610 - accuracy: 0.8903 - val_loss: 0.6575 - val_accuracy: 0.8648 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5522 - accuracy: 0.8948\n",
      "Epoch 00048: val_accuracy improved from 0.86561 to 0.86586, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.5539 - accuracy: 0.8942 - val_loss: 0.6597 - val_accuracy: 0.8659 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5532 - accuracy: 0.8906\n",
      "Epoch 00049: val_accuracy did not improve from 0.86586\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.5533 - accuracy: 0.8904 - val_loss: 0.6582 - val_accuracy: 0.8656 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5518 - accuracy: 0.8934\n",
      "Epoch 00050: val_accuracy did not improve from 0.86586\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.5528 - accuracy: 0.8933 - val_loss: 0.6599 - val_accuracy: 0.8656 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5465 - accuracy: 0.8969\n",
      "Epoch 00051: val_accuracy did not improve from 0.86586\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5465 - accuracy: 0.8969 - val_loss: 0.6599 - val_accuracy: 0.8659 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5431 - accuracy: 0.8934\n",
      "Epoch 00052: val_accuracy improved from 0.86586 to 0.86971, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 0.5426 - accuracy: 0.8939 - val_loss: 0.6497 - val_accuracy: 0.8697 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5437 - accuracy: 0.8966\n",
      "Epoch 00053: val_accuracy improved from 0.86971 to 0.87176, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.5437 - accuracy: 0.8966 - val_loss: 0.6469 - val_accuracy: 0.8718 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5376 - accuracy: 0.8968\n",
      "Epoch 00054: val_accuracy did not improve from 0.87176\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.5382 - accuracy: 0.8965 - val_loss: 0.6467 - val_accuracy: 0.8692 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5390 - accuracy: 0.8981\n",
      "Epoch 00055: val_accuracy improved from 0.87176 to 0.87227, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.5384 - accuracy: 0.8985 - val_loss: 0.6463 - val_accuracy: 0.8723 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5309 - accuracy: 0.9022\n",
      "Epoch 00056: val_accuracy did not improve from 0.87227\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.5310 - accuracy: 0.9023 - val_loss: 0.6436 - val_accuracy: 0.8710 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5235 - accuracy: 0.9003\n",
      "Epoch 00057: val_accuracy improved from 0.87227 to 0.87253, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 0.5235 - accuracy: 0.9005 - val_loss: 0.6391 - val_accuracy: 0.8725 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5374 - accuracy: 0.8983\n",
      "Epoch 00058: val_accuracy improved from 0.87253 to 0.87279, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 0.5382 - accuracy: 0.8983 - val_loss: 0.6423 - val_accuracy: 0.8728 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5270 - accuracy: 0.9004\n",
      "Epoch 00059: val_accuracy did not improve from 0.87279\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5270 - accuracy: 0.9004 - val_loss: 0.6417 - val_accuracy: 0.8715 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5122 - accuracy: 0.9091\n",
      "Epoch 00060: val_accuracy did not improve from 0.87279\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5128 - accuracy: 0.9087 - val_loss: 0.6409 - val_accuracy: 0.8723 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5238 - accuracy: 0.9009\n",
      "Epoch 00061: val_accuracy improved from 0.87279 to 0.87381, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.5238 - accuracy: 0.9009 - val_loss: 0.6409 - val_accuracy: 0.8738 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5080 - accuracy: 0.9057\n",
      "Epoch 00062: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5080 - accuracy: 0.9057 - val_loss: 0.6400 - val_accuracy: 0.8736 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5152 - accuracy: 0.9056\n",
      "Epoch 00063: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.5155 - accuracy: 0.9053 - val_loss: 0.6388 - val_accuracy: 0.8720 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5181 - accuracy: 0.9032\n",
      "Epoch 00064: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5188 - accuracy: 0.9028 - val_loss: 0.6433 - val_accuracy: 0.8674 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5036 - accuracy: 0.9075\n",
      "Epoch 00065: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5036 - accuracy: 0.9075 - val_loss: 0.6434 - val_accuracy: 0.8723 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5060 - accuracy: 0.9092\n",
      "Epoch 00066: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.5051 - accuracy: 0.9093 - val_loss: 0.6490 - val_accuracy: 0.8679 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5036 - accuracy: 0.9047\n",
      "Epoch 00067: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.5036 - accuracy: 0.9047 - val_loss: 0.6427 - val_accuracy: 0.8707 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5078 - accuracy: 0.9057\n",
      "Epoch 00068: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 0.5078 - accuracy: 0.9057 - val_loss: 0.6377 - val_accuracy: 0.8710 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5017 - accuracy: 0.9051\n",
      "Epoch 00069: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.5019 - accuracy: 0.9048 - val_loss: 0.6369 - val_accuracy: 0.8687 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5053 - accuracy: 0.9064\n",
      "Epoch 00070: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.5053 - accuracy: 0.9064 - val_loss: 0.6371 - val_accuracy: 0.8702 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4921 - accuracy: 0.9114\n",
      "Epoch 00071: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.4947 - accuracy: 0.9108 - val_loss: 0.6412 - val_accuracy: 0.8712 - lr: 1.0000e-04\n",
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4982 - accuracy: 0.9087\n",
      "Epoch 00072: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5012 - accuracy: 0.9078 - val_loss: 0.6460 - val_accuracy: 0.8700 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5009 - accuracy: 0.9068\n",
      "Epoch 00073: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5002 - accuracy: 0.9070 - val_loss: 0.6401 - val_accuracy: 0.8720 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4893 - accuracy: 0.9120\n",
      "Epoch 00074: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.4895 - accuracy: 0.9121 - val_loss: 0.6347 - val_accuracy: 0.8705 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5057 - accuracy: 0.9070\n",
      "Epoch 00075: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.5057 - accuracy: 0.9070 - val_loss: 0.6361 - val_accuracy: 0.8692 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4970 - accuracy: 0.9093\n",
      "Epoch 00076: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.4970 - accuracy: 0.9093 - val_loss: 0.6350 - val_accuracy: 0.8725 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4818 - accuracy: 0.9118\n",
      "Epoch 00077: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.4822 - accuracy: 0.9116 - val_loss: 0.6367 - val_accuracy: 0.8723 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4922 - accuracy: 0.9078\n",
      "Epoch 00078: val_accuracy did not improve from 0.87381\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.4922 - accuracy: 0.9078 - val_loss: 0.6311 - val_accuracy: 0.8730 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4790 - accuracy: 0.9137\n",
      "Epoch 00079: val_accuracy improved from 0.87381 to 0.87433, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 0.4812 - accuracy: 0.9127 - val_loss: 0.6297 - val_accuracy: 0.8743 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4835 - accuracy: 0.9151\n",
      "Epoch 00080: val_accuracy did not improve from 0.87433\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.4837 - accuracy: 0.9150 - val_loss: 0.6298 - val_accuracy: 0.8730 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4811 - accuracy: 0.9119\n",
      "Epoch 00081: val_accuracy improved from 0.87433 to 0.87484, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 0.4828 - accuracy: 0.9112 - val_loss: 0.6286 - val_accuracy: 0.8748 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4830 - accuracy: 0.9146\n",
      "Epoch 00082: val_accuracy did not improve from 0.87484\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.4830 - accuracy: 0.9146 - val_loss: 0.6293 - val_accuracy: 0.8733 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4753 - accuracy: 0.9147\n",
      "Epoch 00083: val_accuracy did not improve from 0.87484\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.4759 - accuracy: 0.9143 - val_loss: 0.6310 - val_accuracy: 0.8741 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4865 - accuracy: 0.9122\n",
      "Epoch 00084: val_accuracy did not improve from 0.87484\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.4865 - accuracy: 0.9122 - val_loss: 0.6304 - val_accuracy: 0.8738 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4835 - accuracy: 0.9126\n",
      "Epoch 00085: val_accuracy did not improve from 0.87484\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.4845 - accuracy: 0.9122 - val_loss: 0.6295 - val_accuracy: 0.8741 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4714 - accuracy: 0.9173\n",
      "Epoch 00086: val_accuracy did not improve from 0.87484\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.4742 - accuracy: 0.9163 - val_loss: 0.6309 - val_accuracy: 0.8733 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4880 - accuracy: 0.9124\n",
      "Epoch 00087: val_accuracy did not improve from 0.87484\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.4880 - accuracy: 0.9124 - val_loss: 0.6308 - val_accuracy: 0.8741 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4839 - accuracy: 0.9111\n",
      "Epoch 00088: val_accuracy did not improve from 0.87484\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.4839 - accuracy: 0.9111 - val_loss: 0.6301 - val_accuracy: 0.8746 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4725 - accuracy: 0.9158\n",
      "Epoch 00089: val_accuracy did not improve from 0.87484\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.4747 - accuracy: 0.9152 - val_loss: 0.6297 - val_accuracy: 0.8743 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4658 - accuracy: 0.9180\n",
      "Epoch 00090: val_accuracy improved from 0.87484 to 0.87561, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.4667 - accuracy: 0.9176 - val_loss: 0.6290 - val_accuracy: 0.8756 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4791 - accuracy: 0.9146\n",
      "Epoch 00091: val_accuracy did not improve from 0.87561\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.4787 - accuracy: 0.9148 - val_loss: 0.6287 - val_accuracy: 0.8746 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4757 - accuracy: 0.9163\n",
      "Epoch 00092: val_accuracy did not improve from 0.87561\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.4765 - accuracy: 0.9159 - val_loss: 0.6279 - val_accuracy: 0.8756 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4822 - accuracy: 0.9129\n",
      "Epoch 00093: val_accuracy improved from 0.87561 to 0.87638, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.4822 - accuracy: 0.9127 - val_loss: 0.6277 - val_accuracy: 0.8764 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4784 - accuracy: 0.9156\n",
      "Epoch 00094: val_accuracy did not improve from 0.87638\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 0.4820 - accuracy: 0.9142 - val_loss: 0.6270 - val_accuracy: 0.8751 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4772 - accuracy: 0.9153\n",
      "Epoch 00095: val_accuracy improved from 0.87638 to 0.87766, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.4781 - accuracy: 0.9152 - val_loss: 0.6263 - val_accuracy: 0.8777 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4735 - accuracy: 0.9178\n",
      "Epoch 00096: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.4735 - accuracy: 0.9178 - val_loss: 0.6290 - val_accuracy: 0.8759 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4758 - accuracy: 0.9168\n",
      "Epoch 00097: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.4758 - accuracy: 0.9168 - val_loss: 0.6268 - val_accuracy: 0.8759 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4737 - accuracy: 0.9124\n",
      "Epoch 00098: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.4737 - accuracy: 0.9125 - val_loss: 0.6266 - val_accuracy: 0.8771 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4721 - accuracy: 0.9172 ETA: 0s - loss: 0.4748 - accuracy: 0.\n",
      "Epoch 00099: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.4739 - accuracy: 0.9167 - val_loss: 0.6272 - val_accuracy: 0.8756 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4765 - accuracy: 0.9129\n",
      "Epoch 00100: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4767 - accuracy: 0.9131 - val_loss: 0.6267 - val_accuracy: 0.8759 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4693 - accuracy: 0.9150\n",
      "Epoch 00101: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4695 - accuracy: 0.9150 - val_loss: 0.6274 - val_accuracy: 0.8754 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4644 - accuracy: 0.9218\n",
      "Epoch 00102: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.4654 - accuracy: 0.9213 - val_loss: 0.6282 - val_accuracy: 0.8751 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4690 - accuracy: 0.9148\n",
      "Epoch 00103: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.4700 - accuracy: 0.9142 - val_loss: 0.6276 - val_accuracy: 0.8751 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4689 - accuracy: 0.9167\n",
      "Epoch 00104: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.4689 - accuracy: 0.9167 - val_loss: 0.6283 - val_accuracy: 0.8756 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4816 - accuracy: 0.9121\n",
      "Epoch 00105: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.4816 - accuracy: 0.9121 - val_loss: 0.6279 - val_accuracy: 0.8766 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4656 - accuracy: 0.9176\n",
      "Epoch 00106: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.4681 - accuracy: 0.9168 - val_loss: 0.6268 - val_accuracy: 0.8748 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4644 - accuracy: 0.9171\n",
      "Epoch 00107: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.4644 - accuracy: 0.9171 - val_loss: 0.6244 - val_accuracy: 0.8769 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4581 - accuracy: 0.9189\n",
      "Epoch 00108: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.4581 - accuracy: 0.9189 - val_loss: 0.6254 - val_accuracy: 0.8764 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4708 - accuracy: 0.9163\n",
      "Epoch 00109: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.4708 - accuracy: 0.9163 - val_loss: 0.6276 - val_accuracy: 0.8756 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4768 - accuracy: 0.9121\n",
      "Epoch 00110: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.4771 - accuracy: 0.9117 - val_loss: 0.6257 - val_accuracy: 0.8759 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4844 - accuracy: 0.9105\n",
      "Epoch 00111: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.4844 - accuracy: 0.9105 - val_loss: 0.6276 - val_accuracy: 0.8754 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4729 - accuracy: 0.9137\n",
      "Epoch 00112: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.4729 - accuracy: 0.9137 - val_loss: 0.6276 - val_accuracy: 0.8769 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4678 - accuracy: 0.9140\n",
      "Epoch 00113: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 0.4695 - accuracy: 0.9132 - val_loss: 0.6240 - val_accuracy: 0.8777 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4690 - accuracy: 0.9180\n",
      "Epoch 00114: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.4690 - accuracy: 0.9180 - val_loss: 0.6272 - val_accuracy: 0.8759 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4653 - accuracy: 0.9193\n",
      "Epoch 00115: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.4653 - accuracy: 0.9193 - val_loss: 0.6273 - val_accuracy: 0.8759 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4684 - accuracy: 0.9166\n",
      "Epoch 00116: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.4681 - accuracy: 0.9164 - val_loss: 0.6266 - val_accuracy: 0.8764 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4662 - accuracy: 0.9188\n",
      "Epoch 00117: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.4674 - accuracy: 0.9181 - val_loss: 0.6268 - val_accuracy: 0.8759 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4744 - accuracy: 0.9166\n",
      "Epoch 00118: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.4744 - accuracy: 0.9166 - val_loss: 0.6264 - val_accuracy: 0.8746 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4680 - accuracy: 0.9148\n",
      "Epoch 00119: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.4680 - accuracy: 0.9148 - val_loss: 0.6243 - val_accuracy: 0.8754 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.4722 - accuracy: 0.9147\n",
      "Epoch 00120: val_accuracy did not improve from 0.87766\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 0.4715 - accuracy: 0.9150 - val_loss: 0.6236 - val_accuracy: 0.8759 - lr: 1.0000e-06\n",
      "epoch_number 95\n",
      "train accuracy and validation accuracy 0.9151555299758911 0.8776609301567078\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.6263 - accuracy: 0.8777\n",
      "test_accuracy 0.8776609301567078\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078]\n",
      "0.7579599108014788\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S8_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S8_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.4973 - accuracy: 0.1316\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.25776, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 356ms/step - loss: 4.4814 - accuracy: 0.1330 - val_loss: 3.4011 - val_accuracy: 0.2578 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.0321 - accuracy: 0.2672\n",
      "Epoch 00002: val_accuracy improved from 0.25776 to 0.34240, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 3.0356 - accuracy: 0.2672 - val_loss: 2.6671 - val_accuracy: 0.3424 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.6659 - accuracy: 0.3432\n",
      "Epoch 00003: val_accuracy improved from 0.34240 to 0.41421, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 2.6663 - accuracy: 0.3433 - val_loss: 2.4078 - val_accuracy: 0.4142 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4324 - accuracy: 0.3974\n",
      "Epoch 00004: val_accuracy improved from 0.41421 to 0.46909, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 2.4320 - accuracy: 0.3976 - val_loss: 2.2234 - val_accuracy: 0.4691 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2398 - accuracy: 0.4400\n",
      "Epoch 00005: val_accuracy improved from 0.46909 to 0.49628, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 2.2385 - accuracy: 0.4403 - val_loss: 2.0678 - val_accuracy: 0.4963 - lr: 0.0010\n",
      "Epoch 6/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.1122 - accuracy: 0.4755\n",
      "Epoch 00006: val_accuracy improved from 0.49628 to 0.52398, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 2.1122 - accuracy: 0.4755 - val_loss: 1.9566 - val_accuracy: 0.5240 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9834 - accuracy: 0.4981\n",
      "Epoch 00007: val_accuracy improved from 0.52398 to 0.54912, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.9836 - accuracy: 0.4976 - val_loss: 1.8840 - val_accuracy: 0.5491 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8974 - accuracy: 0.5232\n",
      "Epoch 00008: val_accuracy improved from 0.54912 to 0.56732, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.9015 - accuracy: 0.5220 - val_loss: 1.8237 - val_accuracy: 0.5673 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8258 - accuracy: 0.5417\n",
      "Epoch 00009: val_accuracy improved from 0.56732 to 0.57015, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.8263 - accuracy: 0.5418 - val_loss: 1.7694 - val_accuracy: 0.5701 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7493 - accuracy: 0.5633\n",
      "Epoch 00010: val_accuracy improved from 0.57015 to 0.58810, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.7526 - accuracy: 0.5626 - val_loss: 1.7120 - val_accuracy: 0.5881 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7098 - accuracy: 0.5671\n",
      "Epoch 00011: val_accuracy improved from 0.58810 to 0.59323, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.7104 - accuracy: 0.5670 - val_loss: 1.6898 - val_accuracy: 0.5932 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6730 - accuracy: 0.5811\n",
      "Epoch 00012: val_accuracy improved from 0.59323 to 0.59836, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.6745 - accuracy: 0.5806 - val_loss: 1.6834 - val_accuracy: 0.5984 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6001 - accuracy: 0.6001\n",
      "Epoch 00013: val_accuracy improved from 0.59836 to 0.60144, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.5964 - accuracy: 0.6013 - val_loss: 1.6471 - val_accuracy: 0.6014 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5564 - accuracy: 0.6160\n",
      "Epoch 00014: val_accuracy improved from 0.60144 to 0.60990, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.5598 - accuracy: 0.6148 - val_loss: 1.6143 - val_accuracy: 0.6099 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5362 - accuracy: 0.6144\n",
      "Epoch 00015: val_accuracy improved from 0.60990 to 0.61708, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.5346 - accuracy: 0.6151 - val_loss: 1.6255 - val_accuracy: 0.6171 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5080 - accuracy: 0.6258 ETA: 0s - loss: 1.5371 - accu\n",
      "Epoch 00016: val_accuracy did not improve from 0.61708\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.5079 - accuracy: 0.6248 - val_loss: 1.5895 - val_accuracy: 0.6158 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4895 - accuracy: 0.6232\n",
      "Epoch 00017: val_accuracy did not improve from 0.61708\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.4898 - accuracy: 0.6234 - val_loss: 1.5810 - val_accuracy: 0.6155 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4416 - accuracy: 0.6368\n",
      "Epoch 00018: val_accuracy improved from 0.61708 to 0.63350, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 1.4426 - accuracy: 0.6371 - val_loss: 1.5458 - val_accuracy: 0.6335 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3996 - accuracy: 0.6449\n",
      "Epoch 00019: val_accuracy did not improve from 0.63350\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.3987 - accuracy: 0.6451 - val_loss: 1.5449 - val_accuracy: 0.6296 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3589 - accuracy: 0.6598\n",
      "Epoch 00020: val_accuracy improved from 0.63350 to 0.63478, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.3565 - accuracy: 0.6605 - val_loss: 1.5477 - val_accuracy: 0.6348 - lr: 0.0010\n",
      "Epoch 21/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3330 - accuracy: 0.6677\n",
      "Epoch 00021: val_accuracy did not improve from 0.63478\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.3330 - accuracy: 0.6677 - val_loss: 1.5357 - val_accuracy: 0.6335 - lr: 0.0010\n",
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3094 - accuracy: 0.6704\n",
      "Epoch 00022: val_accuracy improved from 0.63478 to 0.64324, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 110ms/step - loss: 1.3081 - accuracy: 0.6708 - val_loss: 1.5194 - val_accuracy: 0.6432 - lr: 0.0010\n",
      "Epoch 23/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2687 - accuracy: 0.6818\n",
      "Epoch 00023: val_accuracy did not improve from 0.64324\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.2687 - accuracy: 0.6818 - val_loss: 1.5514 - val_accuracy: 0.6294 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2962 - accuracy: 0.6783\n",
      "Epoch 00024: val_accuracy did not improve from 0.64324\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.2958 - accuracy: 0.6788 - val_loss: 1.5415 - val_accuracy: 0.6404 - lr: 0.0010\n",
      "Epoch 25/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2671 - accuracy: 0.6854\n",
      "Epoch 00025: val_accuracy did not improve from 0.64324\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.2671 - accuracy: 0.6854 - val_loss: 1.5455 - val_accuracy: 0.6430 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2584 - accuracy: 0.6818\n",
      "Epoch 00026: val_accuracy did not improve from 0.64324\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.2576 - accuracy: 0.6825 - val_loss: 1.5306 - val_accuracy: 0.6404 - lr: 0.0010\n",
      "Epoch 27/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2426 - accuracy: 0.6908\n",
      "Epoch 00027: val_accuracy improved from 0.64324 to 0.65581, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 1.2426 - accuracy: 0.6908 - val_loss: 1.4939 - val_accuracy: 0.6558 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2087 - accuracy: 0.6984\n",
      "Epoch 00028: val_accuracy did not improve from 0.65581\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 1.2073 - accuracy: 0.6989 - val_loss: 1.4825 - val_accuracy: 0.6473 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1864 - accuracy: 0.7011\n",
      "Epoch 00029: val_accuracy did not improve from 0.65581\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.1866 - accuracy: 0.7005 - val_loss: 1.5322 - val_accuracy: 0.6430 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1754 - accuracy: 0.7057\n",
      "Epoch 00030: val_accuracy did not improve from 0.65581\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.1743 - accuracy: 0.7052 - val_loss: 1.5265 - val_accuracy: 0.6476 - lr: 0.0010\n",
      "Epoch 31/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1657 - accuracy: 0.7113\n",
      "Epoch 00031: val_accuracy did not improve from 0.65581\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.1657 - accuracy: 0.7113 - val_loss: 1.4983 - val_accuracy: 0.6512 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1545 - accuracy: 0.7162\n",
      "Epoch 00032: val_accuracy did not improve from 0.65581\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.1543 - accuracy: 0.7163 - val_loss: 1.5200 - val_accuracy: 0.6443 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1532 - accuracy: 0.7153\n",
      "Epoch 00033: val_accuracy did not improve from 0.65581\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.1531 - accuracy: 0.7150 - val_loss: 1.4944 - val_accuracy: 0.6502 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1353 - accuracy: 0.7189\n",
      "Epoch 00034: val_accuracy did not improve from 0.65581\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.1368 - accuracy: 0.7183 - val_loss: 1.4984 - val_accuracy: 0.6527 - lr: 0.0010\n",
      "Epoch 35/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1266 - accuracy: 0.7239\n",
      "Epoch 00035: val_accuracy improved from 0.65581 to 0.66171, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.1266 - accuracy: 0.7239 - val_loss: 1.4914 - val_accuracy: 0.6617 - lr: 0.0010\n",
      "Epoch 36/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0969 - accuracy: 0.7292\n",
      "Epoch 00036: val_accuracy did not improve from 0.66171\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.0969 - accuracy: 0.7292 - val_loss: 1.4944 - val_accuracy: 0.6540 - lr: 0.0010\n",
      "Epoch 37/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1171 - accuracy: 0.7236\n",
      "Epoch 00037: val_accuracy did not improve from 0.66171\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 1.1171 - accuracy: 0.7236 - val_loss: 1.5013 - val_accuracy: 0.6561 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0851 - accuracy: 0.7327\n",
      "Epoch 00038: val_accuracy improved from 0.66171 to 0.66350, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 1.0855 - accuracy: 0.7329 - val_loss: 1.4896 - val_accuracy: 0.6635 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0799 - accuracy: 0.7280\n",
      "Epoch 00039: val_accuracy improved from 0.66350 to 0.66838, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.0883 - accuracy: 0.7263 - val_loss: 1.4747 - val_accuracy: 0.6684 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0382 - accuracy: 0.7467\n",
      "Epoch 00040: val_accuracy did not improve from 0.66838\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.0400 - accuracy: 0.7460 - val_loss: 1.4658 - val_accuracy: 0.6638 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0158 - accuracy: 0.7549\n",
      "Epoch 00041: val_accuracy improved from 0.66838 to 0.66863, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.0175 - accuracy: 0.7548 - val_loss: 1.4567 - val_accuracy: 0.6686 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9950 - accuracy: 0.7540\n",
      "Epoch 00042: val_accuracy improved from 0.66863 to 0.66940, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.9950 - accuracy: 0.7540 - val_loss: 1.4511 - val_accuracy: 0.6694 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9677 - accuracy: 0.7690\n",
      "Epoch 00043: val_accuracy did not improve from 0.66940\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9666 - accuracy: 0.7691 - val_loss: 1.4513 - val_accuracy: 0.6684 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9680 - accuracy: 0.7665\n",
      "Epoch 00044: val_accuracy improved from 0.66940 to 0.67197, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.9680 - accuracy: 0.7665 - val_loss: 1.4432 - val_accuracy: 0.6720 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9625 - accuracy: 0.7678\n",
      "Epoch 00045: val_accuracy improved from 0.67197 to 0.67351, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 0.9623 - accuracy: 0.7685 - val_loss: 1.4423 - val_accuracy: 0.6735 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9330 - accuracy: 0.7793\n",
      "Epoch 00046: val_accuracy improved from 0.67351 to 0.67761, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.9345 - accuracy: 0.7793 - val_loss: 1.4372 - val_accuracy: 0.6776 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9358 - accuracy: 0.7751\n",
      "Epoch 00047: val_accuracy did not improve from 0.67761\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.9358 - accuracy: 0.7751 - val_loss: 1.4436 - val_accuracy: 0.6776 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9469 - accuracy: 0.7750\n",
      "Epoch 00048: val_accuracy improved from 0.67761 to 0.67864, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.9469 - accuracy: 0.7750 - val_loss: 1.4443 - val_accuracy: 0.6786 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9368 - accuracy: 0.7736\n",
      "Epoch 00049: val_accuracy improved from 0.67864 to 0.67889, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 0.9361 - accuracy: 0.7738 - val_loss: 1.4439 - val_accuracy: 0.6789 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9311 - accuracy: 0.7786\n",
      "Epoch 00050: val_accuracy did not improve from 0.67889\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9314 - accuracy: 0.7787 - val_loss: 1.4456 - val_accuracy: 0.6766 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9240 - accuracy: 0.7779\n",
      "Epoch 00051: val_accuracy did not improve from 0.67889\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9240 - accuracy: 0.7779 - val_loss: 1.4455 - val_accuracy: 0.6753 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9177 - accuracy: 0.7852\n",
      "Epoch 00052: val_accuracy did not improve from 0.67889\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9188 - accuracy: 0.7850 - val_loss: 1.4457 - val_accuracy: 0.6743 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9115 - accuracy: 0.7837\n",
      "Epoch 00053: val_accuracy did not improve from 0.67889\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9172 - accuracy: 0.7826 - val_loss: 1.4392 - val_accuracy: 0.6766 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9135 - accuracy: 0.7804\n",
      "Epoch 00054: val_accuracy did not improve from 0.67889\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9146 - accuracy: 0.7801 - val_loss: 1.4442 - val_accuracy: 0.6727 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8929 - accuracy: 0.7942\n",
      "Epoch 00055: val_accuracy did not improve from 0.67889\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8958 - accuracy: 0.7936 - val_loss: 1.4412 - val_accuracy: 0.6763 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8892 - accuracy: 0.7946\n",
      "Epoch 00056: val_accuracy improved from 0.67889 to 0.67966, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 0.8898 - accuracy: 0.7945 - val_loss: 1.4406 - val_accuracy: 0.6797 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9034 - accuracy: 0.7826\n",
      "Epoch 00057: val_accuracy did not improve from 0.67966\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9034 - accuracy: 0.7826 - val_loss: 1.4421 - val_accuracy: 0.6789 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8889 - accuracy: 0.7900\n",
      "Epoch 00058: val_accuracy did not improve from 0.67966\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8877 - accuracy: 0.7905 - val_loss: 1.4411 - val_accuracy: 0.6766 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8876 - accuracy: 0.7904\n",
      "Epoch 00059: val_accuracy did not improve from 0.67966\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8867 - accuracy: 0.7903 - val_loss: 1.4429 - val_accuracy: 0.6779 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8861 - accuracy: 0.7913\n",
      "Epoch 00060: val_accuracy did not improve from 0.67966\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8855 - accuracy: 0.7920 - val_loss: 1.4499 - val_accuracy: 0.6779 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8917 - accuracy: 0.7870\n",
      "Epoch 00061: val_accuracy did not improve from 0.67966\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8936 - accuracy: 0.7864 - val_loss: 1.4526 - val_accuracy: 0.6745 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8848 - accuracy: 0.7889\n",
      "Epoch 00062: val_accuracy improved from 0.67966 to 0.68017, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.8876 - accuracy: 0.7885 - val_loss: 1.4435 - val_accuracy: 0.6802 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8920 - accuracy: 0.7872\n",
      "Epoch 00063: val_accuracy did not improve from 0.68017\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8930 - accuracy: 0.7869 - val_loss: 1.4440 - val_accuracy: 0.6789 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8729 - accuracy: 0.7957\n",
      "Epoch 00064: val_accuracy did not improve from 0.68017\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8729 - accuracy: 0.7957 - val_loss: 1.4413 - val_accuracy: 0.6776 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8755 - accuracy: 0.7918\n",
      "Epoch 00065: val_accuracy improved from 0.68017 to 0.68094, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 0.8755 - accuracy: 0.7918 - val_loss: 1.4394 - val_accuracy: 0.6809 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8651 - accuracy: 0.7963\n",
      "Epoch 00066: val_accuracy improved from 0.68094 to 0.68274, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.8655 - accuracy: 0.7957 - val_loss: 1.4379 - val_accuracy: 0.6827 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8565 - accuracy: 0.7972\n",
      "Epoch 00067: val_accuracy did not improve from 0.68274\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8597 - accuracy: 0.7960 - val_loss: 1.4400 - val_accuracy: 0.6827 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8587 - accuracy: 0.7944\n",
      "Epoch 00068: val_accuracy did not improve from 0.68274\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8618 - accuracy: 0.7935 - val_loss: 1.4401 - val_accuracy: 0.6809 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8637 - accuracy: 0.7943\n",
      "Epoch 00069: val_accuracy did not improve from 0.68274\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8660 - accuracy: 0.7933 - val_loss: 1.4415 - val_accuracy: 0.6789 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8634 - accuracy: 0.7955\n",
      "Epoch 00070: val_accuracy did not improve from 0.68274\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.8634 - accuracy: 0.7955 - val_loss: 1.4348 - val_accuracy: 0.6822 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8661 - accuracy: 0.7959\n",
      "Epoch 00071: val_accuracy improved from 0.68274 to 0.68428, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 0.8661 - accuracy: 0.7960 - val_loss: 1.4354 - val_accuracy: 0.6843 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8593 - accuracy: 0.7972\n",
      "Epoch 00072: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8610 - accuracy: 0.7964 - val_loss: 1.4378 - val_accuracy: 0.6807 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8570 - accuracy: 0.7944\n",
      "Epoch 00073: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8583 - accuracy: 0.7938 - val_loss: 1.4312 - val_accuracy: 0.6807 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8526 - accuracy: 0.7987\n",
      "Epoch 00074: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8539 - accuracy: 0.7980 - val_loss: 1.4373 - val_accuracy: 0.6815 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8412 - accuracy: 0.8016\n",
      "Epoch 00075: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8426 - accuracy: 0.8009 - val_loss: 1.4391 - val_accuracy: 0.6812 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8364 - accuracy: 0.7978\n",
      "Epoch 00076: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8396 - accuracy: 0.7970 - val_loss: 1.4356 - val_accuracy: 0.6812 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8355 - accuracy: 0.8019\n",
      "Epoch 00077: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8364 - accuracy: 0.8015 - val_loss: 1.4384 - val_accuracy: 0.6781 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8327 - accuracy: 0.8059\n",
      "Epoch 00078: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8351 - accuracy: 0.8050 - val_loss: 1.4431 - val_accuracy: 0.6791 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8352 - accuracy: 0.8035\n",
      "Epoch 00079: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8352 - accuracy: 0.8035 - val_loss: 1.4530 - val_accuracy: 0.6753 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8360 - accuracy: 0.7968\n",
      "Epoch 00080: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8360 - accuracy: 0.7968 - val_loss: 1.4557 - val_accuracy: 0.6753 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8216 - accuracy: 0.8070\n",
      "Epoch 00081: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8242 - accuracy: 0.8067 - val_loss: 1.4514 - val_accuracy: 0.6779 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8383 - accuracy: 0.7991\n",
      "Epoch 00082: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8408 - accuracy: 0.7982 - val_loss: 1.4557 - val_accuracy: 0.6756 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8279 - accuracy: 0.8039\n",
      "Epoch 00083: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8279 - accuracy: 0.8039 - val_loss: 1.4498 - val_accuracy: 0.6763 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8240 - accuracy: 0.8038\n",
      "Epoch 00084: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8250 - accuracy: 0.8035 - val_loss: 1.4559 - val_accuracy: 0.6758 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8217 - accuracy: 0.8048\n",
      "Epoch 00085: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8237 - accuracy: 0.8040 - val_loss: 1.4535 - val_accuracy: 0.6768 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8268 - accuracy: 0.8072\n",
      "Epoch 00086: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8275 - accuracy: 0.8072 - val_loss: 1.4479 - val_accuracy: 0.6799 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8172 - accuracy: 0.8050\n",
      "Epoch 00087: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8179 - accuracy: 0.8049 - val_loss: 1.4521 - val_accuracy: 0.6768 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8149 - accuracy: 0.8065\n",
      "Epoch 00088: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8149 - accuracy: 0.8065 - val_loss: 1.4511 - val_accuracy: 0.6791 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8119 - accuracy: 0.8093\n",
      "Epoch 00089: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8155 - accuracy: 0.8081 - val_loss: 1.4482 - val_accuracy: 0.6807 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8370 - accuracy: 0.8015\n",
      "Epoch 00090: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8370 - accuracy: 0.8015 - val_loss: 1.4520 - val_accuracy: 0.6804 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8232 - accuracy: 0.8051\n",
      "Epoch 00091: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8262 - accuracy: 0.8039 - val_loss: 1.4435 - val_accuracy: 0.6822 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8227 - accuracy: 0.8057\n",
      "Epoch 00092: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8237 - accuracy: 0.8055 - val_loss: 1.4472 - val_accuracy: 0.6797 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8159 - accuracy: 0.8063\n",
      "Epoch 00093: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8159 - accuracy: 0.8061 - val_loss: 1.4420 - val_accuracy: 0.6825 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8200 - accuracy: 0.8040\n",
      "Epoch 00094: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8202 - accuracy: 0.8036 - val_loss: 1.4437 - val_accuracy: 0.6817 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8191 - accuracy: 0.8068\n",
      "Epoch 00095: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8211 - accuracy: 0.8066 - val_loss: 1.4452 - val_accuracy: 0.6809 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8196 - accuracy: 0.8022\n",
      "Epoch 00096: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8196 - accuracy: 0.8022 - val_loss: 1.4448 - val_accuracy: 0.6812 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8154 - accuracy: 0.8095\n",
      "Epoch 00097: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8154 - accuracy: 0.8095 - val_loss: 1.4476 - val_accuracy: 0.6799 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8060 - accuracy: 0.8132\n",
      "Epoch 00098: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8060 - accuracy: 0.8132 - val_loss: 1.4408 - val_accuracy: 0.6835 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8183 - accuracy: 0.8038\n",
      "Epoch 00099: val_accuracy did not improve from 0.68428\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8216 - accuracy: 0.8028 - val_loss: 1.4494 - val_accuracy: 0.6809 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8196 - accuracy: 0.8048\n",
      "Epoch 00100: val_accuracy improved from 0.68428 to 0.68479, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.8196 - accuracy: 0.8048 - val_loss: 1.4434 - val_accuracy: 0.6848 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8085 - accuracy: 0.8086\n",
      "Epoch 00101: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8109 - accuracy: 0.8073 - val_loss: 1.4485 - val_accuracy: 0.6825 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8252 - accuracy: 0.8023\n",
      "Epoch 00102: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8252 - accuracy: 0.8023 - val_loss: 1.4518 - val_accuracy: 0.6789 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8184 - accuracy: 0.8069\n",
      "Epoch 00103: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8203 - accuracy: 0.8061 - val_loss: 1.4452 - val_accuracy: 0.6812 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8234 - accuracy: 0.8009\n",
      "Epoch 00104: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8256 - accuracy: 0.8000 - val_loss: 1.4502 - val_accuracy: 0.6799 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8149 - accuracy: 0.8099\n",
      "Epoch 00105: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8149 - accuracy: 0.8099 - val_loss: 1.4456 - val_accuracy: 0.6815 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8129 - accuracy: 0.8104\n",
      "Epoch 00106: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8150 - accuracy: 0.8092 - val_loss: 1.4398 - val_accuracy: 0.6845 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8037 - accuracy: 0.8097\n",
      "Epoch 00107: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8012 - accuracy: 0.8106 - val_loss: 1.4422 - val_accuracy: 0.6835 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8155 - accuracy: 0.8089\n",
      "Epoch 00108: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8178 - accuracy: 0.8082 - val_loss: 1.4413 - val_accuracy: 0.6817 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8027 - accuracy: 0.8107\n",
      "Epoch 00109: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8035 - accuracy: 0.8103 - val_loss: 1.4402 - val_accuracy: 0.6830 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8167 - accuracy: 0.8056\n",
      "Epoch 00110: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8168 - accuracy: 0.8050 - val_loss: 1.4435 - val_accuracy: 0.6812 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8055 - accuracy: 0.8104\n",
      "Epoch 00111: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.8079 - accuracy: 0.8096 - val_loss: 1.4409 - val_accuracy: 0.6845 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8114 - accuracy: 0.8051\n",
      "Epoch 00112: val_accuracy improved from 0.68479 to 0.68607, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 0.8129 - accuracy: 0.8049 - val_loss: 1.4373 - val_accuracy: 0.6861 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8251 - accuracy: 0.8020\n",
      "Epoch 00113: val_accuracy did not improve from 0.68607\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8256 - accuracy: 0.8016 - val_loss: 1.4450 - val_accuracy: 0.6848 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7993 - accuracy: 0.8106\n",
      "Epoch 00114: val_accuracy did not improve from 0.68607\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.8044 - accuracy: 0.8093 - val_loss: 1.4400 - val_accuracy: 0.6856 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8054 - accuracy: 0.8104\n",
      "Epoch 00115: val_accuracy did not improve from 0.68607\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8062 - accuracy: 0.8107 - val_loss: 1.4469 - val_accuracy: 0.6830 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8069 - accuracy: 0.8112\n",
      "Epoch 00116: val_accuracy did not improve from 0.68607\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.8069 - accuracy: 0.8112 - val_loss: 1.4432 - val_accuracy: 0.6817 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8091 - accuracy: 0.8122\n",
      "Epoch 00117: val_accuracy did not improve from 0.68607\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8102 - accuracy: 0.8118 - val_loss: 1.4414 - val_accuracy: 0.6817 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8208 - accuracy: 0.8088\n",
      "Epoch 00118: val_accuracy did not improve from 0.68607\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.8208 - accuracy: 0.8088 - val_loss: 1.4465 - val_accuracy: 0.6835 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8218 - accuracy: 0.8042\n",
      "Epoch 00119: val_accuracy did not improve from 0.68607\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.8218 - accuracy: 0.8042 - val_loss: 1.4461 - val_accuracy: 0.6843 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8152 - accuracy: 0.8071\n",
      "Epoch 00120: val_accuracy did not improve from 0.68607\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8152 - accuracy: 0.8071 - val_loss: 1.4447 - val_accuracy: 0.6856 - lr: 1.0000e-06\n",
      "epoch_number 112\n",
      "train accuracy and validation accuracy 0.8049235939979553 0.686073362827301\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.4373 - accuracy: 0.6861\n",
      "test_accuracy 0.686073362827301\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301]\n",
      "0.7489740923047066\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S9_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S9_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.5181 - accuracy: 0.1361\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.25596, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 369ms/step - loss: 4.5058 - accuracy: 0.1364 - val_loss: 3.4328 - val_accuracy: 0.2560 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.9827 - accuracy: 0.2683\n",
      "Epoch 00002: val_accuracy improved from 0.25596 to 0.37292, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 2.9816 - accuracy: 0.2690 - val_loss: 2.5230 - val_accuracy: 0.3729 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.5764 - accuracy: 0.3501\n",
      "Epoch 00003: val_accuracy improved from 0.37292 to 0.44601, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 2.5734 - accuracy: 0.3504 - val_loss: 2.2416 - val_accuracy: 0.4460 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.3124 - accuracy: 0.4131\n",
      "Epoch 00004: val_accuracy improved from 0.44601 to 0.48987, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 2.3126 - accuracy: 0.4129 - val_loss: 2.0500 - val_accuracy: 0.4899 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1569 - accuracy: 0.4482\n",
      "Epoch 00005: val_accuracy improved from 0.48987 to 0.52501, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 2.1554 - accuracy: 0.4487 - val_loss: 1.9510 - val_accuracy: 0.5250 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0179 - accuracy: 0.4828\n",
      "Epoch 00006: val_accuracy improved from 0.52501 to 0.55091, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 2.0164 - accuracy: 0.4830 - val_loss: 1.8388 - val_accuracy: 0.5509 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9263 - accuracy: 0.5068\n",
      "Epoch 00007: val_accuracy improved from 0.55091 to 0.57707, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 1.9272 - accuracy: 0.5070 - val_loss: 1.7272 - val_accuracy: 0.5771 - lr: 0.0010\n",
      "Epoch 8/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.8553 - accuracy: 0.5259\n",
      "Epoch 00008: val_accuracy improved from 0.57707 to 0.58066, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 1.8553 - accuracy: 0.5259 - val_loss: 1.6897 - val_accuracy: 0.5807 - lr: 0.0010\n",
      "Epoch 9/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.7860 - accuracy: 0.5441\n",
      "Epoch 00009: val_accuracy improved from 0.58066 to 0.59605, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 1.7860 - accuracy: 0.5441 - val_loss: 1.6372 - val_accuracy: 0.5961 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7138 - accuracy: 0.5651\n",
      "Epoch 00010: val_accuracy improved from 0.59605 to 0.59682, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 1.7143 - accuracy: 0.5653 - val_loss: 1.6059 - val_accuracy: 0.5968 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6651 - accuracy: 0.5811\n",
      "Epoch 00011: val_accuracy improved from 0.59682 to 0.60554, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 1.6641 - accuracy: 0.5813 - val_loss: 1.5646 - val_accuracy: 0.6055 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6082 - accuracy: 0.5906\n",
      "Epoch 00012: val_accuracy improved from 0.60554 to 0.61759, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 1.6073 - accuracy: 0.5909 - val_loss: 1.5048 - val_accuracy: 0.6176 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5521 - accuracy: 0.6058\n",
      "Epoch 00013: val_accuracy improved from 0.61759 to 0.62426, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.5564 - accuracy: 0.6040 - val_loss: 1.5056 - val_accuracy: 0.6243 - lr: 0.0010\n",
      "Epoch 14/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.5674 - accuracy: 0.6051\n",
      "Epoch 00014: val_accuracy improved from 0.62426 to 0.62888, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.5674 - accuracy: 0.6051 - val_loss: 1.4889 - val_accuracy: 0.6289 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5105 - accuracy: 0.6206\n",
      "Epoch 00015: val_accuracy improved from 0.62888 to 0.63016, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.5115 - accuracy: 0.6204 - val_loss: 1.4563 - val_accuracy: 0.6302 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4849 - accuracy: 0.6246\n",
      "Epoch 00016: val_accuracy improved from 0.63016 to 0.64401, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 1.4864 - accuracy: 0.6242 - val_loss: 1.4241 - val_accuracy: 0.6440 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4449 - accuracy: 0.6317\n",
      "Epoch 00017: val_accuracy improved from 0.64401 to 0.65299, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 1.4478 - accuracy: 0.6303 - val_loss: 1.3834 - val_accuracy: 0.6530 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4173 - accuracy: 0.6412\n",
      "Epoch 00018: val_accuracy improved from 0.65299 to 0.65530, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.4175 - accuracy: 0.6412 - val_loss: 1.3786 - val_accuracy: 0.6553 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3877 - accuracy: 0.6480\n",
      "Epoch 00019: val_accuracy did not improve from 0.65530\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 1.3908 - accuracy: 0.6482 - val_loss: 1.3739 - val_accuracy: 0.6548 - lr: 0.0010\n",
      "Epoch 20/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3703 - accuracy: 0.6479\n",
      "Epoch 00020: val_accuracy improved from 0.65530 to 0.66248, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.3703 - accuracy: 0.6479 - val_loss: 1.3628 - val_accuracy: 0.6625 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3293 - accuracy: 0.6614\n",
      "Epoch 00021: val_accuracy did not improve from 0.66248\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.3300 - accuracy: 0.6614 - val_loss: 1.3722 - val_accuracy: 0.6609 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3245 - accuracy: 0.6704\n",
      "Epoch 00022: val_accuracy did not improve from 0.66248\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.3257 - accuracy: 0.6706 - val_loss: 1.3730 - val_accuracy: 0.6609 - lr: 0.0010\n",
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3097 - accuracy: 0.6686\n",
      "Epoch 00023: val_accuracy did not improve from 0.66248\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.3129 - accuracy: 0.6671 - val_loss: 1.3506 - val_accuracy: 0.6622 - lr: 0.0010\n",
      "Epoch 24/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3222 - accuracy: 0.6644\n",
      "Epoch 00024: val_accuracy improved from 0.66248 to 0.67222, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.3222 - accuracy: 0.6644 - val_loss: 1.3336 - val_accuracy: 0.6722 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2772 - accuracy: 0.6784\n",
      "Epoch 00025: val_accuracy improved from 0.67222 to 0.67428, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.2796 - accuracy: 0.6769 - val_loss: 1.3245 - val_accuracy: 0.6743 - lr: 0.0010\n",
      "Epoch 26/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2946 - accuracy: 0.6749\n",
      "Epoch 00026: val_accuracy improved from 0.67428 to 0.67453, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.2946 - accuracy: 0.6749 - val_loss: 1.3403 - val_accuracy: 0.6745 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2487 - accuracy: 0.6843\n",
      "Epoch 00027: val_accuracy did not improve from 0.67453\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.2502 - accuracy: 0.6844 - val_loss: 1.3336 - val_accuracy: 0.6715 - lr: 0.0010\n",
      "Epoch 28/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2492 - accuracy: 0.6829\n",
      "Epoch 00028: val_accuracy improved from 0.67453 to 0.67838, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.2492 - accuracy: 0.6829 - val_loss: 1.3234 - val_accuracy: 0.6784 - lr: 0.0010\n",
      "Epoch 29/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2206 - accuracy: 0.6926\n",
      "Epoch 00029: val_accuracy improved from 0.67838 to 0.68659, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.2206 - accuracy: 0.6926 - val_loss: 1.2936 - val_accuracy: 0.6866 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2096 - accuracy: 0.6981\n",
      "Epoch 00030: val_accuracy did not improve from 0.68659\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 1.2071 - accuracy: 0.6991 - val_loss: 1.3233 - val_accuracy: 0.6820 - lr: 0.0010\n",
      "Epoch 31/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1753 - accuracy: 0.7086\n",
      "Epoch 00031: val_accuracy improved from 0.68659 to 0.68864, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.1753 - accuracy: 0.7086 - val_loss: 1.2753 - val_accuracy: 0.6886 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1696 - accuracy: 0.7092\n",
      "Epoch 00032: val_accuracy improved from 0.68864 to 0.68889, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.1696 - accuracy: 0.7089 - val_loss: 1.2846 - val_accuracy: 0.6889 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1706 - accuracy: 0.7090\n",
      "Epoch 00033: val_accuracy did not improve from 0.68889\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.1719 - accuracy: 0.7083 - val_loss: 1.3038 - val_accuracy: 0.6789 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1549 - accuracy: 0.7118\n",
      "Epoch 00034: val_accuracy improved from 0.68889 to 0.69120, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.1547 - accuracy: 0.7121 - val_loss: 1.2958 - val_accuracy: 0.6912 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1467 - accuracy: 0.7112\n",
      "Epoch 00035: val_accuracy improved from 0.69120 to 0.69351, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.1481 - accuracy: 0.7110 - val_loss: 1.2851 - val_accuracy: 0.6935 - lr: 0.0010\n",
      "Epoch 36/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1247 - accuracy: 0.7152\n",
      "Epoch 00036: val_accuracy did not improve from 0.69351\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.1247 - accuracy: 0.7152 - val_loss: 1.2825 - val_accuracy: 0.6897 - lr: 0.0010\n",
      "Epoch 37/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1028 - accuracy: 0.7246\n",
      "Epoch 00037: val_accuracy improved from 0.69351 to 0.70762, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.1028 - accuracy: 0.7246 - val_loss: 1.2313 - val_accuracy: 0.7076 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1129 - accuracy: 0.7216\n",
      "Epoch 00038: val_accuracy did not improve from 0.70762\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.1125 - accuracy: 0.7224 - val_loss: 1.2213 - val_accuracy: 0.6951 - lr: 0.0010\n",
      "Epoch 39/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1180 - accuracy: 0.7216\n",
      "Epoch 00039: val_accuracy did not improve from 0.70762\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.1180 - accuracy: 0.7216 - val_loss: 1.2515 - val_accuracy: 0.6966 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0769 - accuracy: 0.7359\n",
      "Epoch 00040: val_accuracy did not improve from 0.70762\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.0773 - accuracy: 0.7355 - val_loss: 1.2104 - val_accuracy: 0.7058 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0228 - accuracy: 0.7497\n",
      "Epoch 00041: val_accuracy improved from 0.70762 to 0.71223, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.0217 - accuracy: 0.7500 - val_loss: 1.1945 - val_accuracy: 0.7122 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0125 - accuracy: 0.7534\n",
      "Epoch 00042: val_accuracy improved from 0.71223 to 0.71429, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 1.0125 - accuracy: 0.7534 - val_loss: 1.1801 - val_accuracy: 0.7143 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9991 - accuracy: 0.7559\n",
      "Epoch 00043: val_accuracy improved from 0.71429 to 0.71711, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.0004 - accuracy: 0.7554 - val_loss: 1.1760 - val_accuracy: 0.7171 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9933 - accuracy: 0.7550\n",
      "Epoch 00044: val_accuracy improved from 0.71711 to 0.71839, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 106ms/step - loss: 0.9968 - accuracy: 0.7544 - val_loss: 1.1684 - val_accuracy: 0.7184 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9757 - accuracy: 0.7631\n",
      "Epoch 00045: val_accuracy improved from 0.71839 to 0.71890, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.9757 - accuracy: 0.7629 - val_loss: 1.1624 - val_accuracy: 0.7189 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9781 - accuracy: 0.7607\n",
      "Epoch 00046: val_accuracy did not improve from 0.71890\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9800 - accuracy: 0.7596 - val_loss: 1.1646 - val_accuracy: 0.7189 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9716 - accuracy: 0.7683\n",
      "Epoch 00047: val_accuracy improved from 0.71890 to 0.72095, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.9742 - accuracy: 0.7672 - val_loss: 1.1614 - val_accuracy: 0.7210 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9622 - accuracy: 0.7648\n",
      "Epoch 00048: val_accuracy improved from 0.72095 to 0.72147, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.9634 - accuracy: 0.7648 - val_loss: 1.1660 - val_accuracy: 0.7215 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9475 - accuracy: 0.7676\n",
      "Epoch 00049: val_accuracy improved from 0.72147 to 0.72224, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 0.9488 - accuracy: 0.7676 - val_loss: 1.1593 - val_accuracy: 0.7222 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9517 - accuracy: 0.7659\n",
      "Epoch 00050: val_accuracy did not improve from 0.72224\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.9517 - accuracy: 0.7658 - val_loss: 1.1566 - val_accuracy: 0.7204 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9444 - accuracy: 0.7682\n",
      "Epoch 00051: val_accuracy did not improve from 0.72224\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9476 - accuracy: 0.7672 - val_loss: 1.1576 - val_accuracy: 0.7189 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9559 - accuracy: 0.7658\n",
      "Epoch 00052: val_accuracy did not improve from 0.72224\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9559 - accuracy: 0.7658 - val_loss: 1.1587 - val_accuracy: 0.7220 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9523 - accuracy: 0.7678\n",
      "Epoch 00053: val_accuracy did not improve from 0.72224\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9540 - accuracy: 0.7673 - val_loss: 1.1637 - val_accuracy: 0.7204 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9440 - accuracy: 0.7682\n",
      "Epoch 00054: val_accuracy improved from 0.72224 to 0.72429, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.9436 - accuracy: 0.7681 - val_loss: 1.1578 - val_accuracy: 0.7243 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9388 - accuracy: 0.7712\n",
      "Epoch 00055: val_accuracy did not improve from 0.72429\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.9404 - accuracy: 0.7706 - val_loss: 1.1570 - val_accuracy: 0.7189 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9312 - accuracy: 0.7752\n",
      "Epoch 00056: val_accuracy did not improve from 0.72429\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.9333 - accuracy: 0.7746 - val_loss: 1.1534 - val_accuracy: 0.7210 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9179 - accuracy: 0.7724\n",
      "Epoch 00057: val_accuracy did not improve from 0.72429\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9177 - accuracy: 0.7725 - val_loss: 1.1557 - val_accuracy: 0.7179 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9206 - accuracy: 0.7787\n",
      "Epoch 00058: val_accuracy did not improve from 0.72429\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.9207 - accuracy: 0.7784 - val_loss: 1.1512 - val_accuracy: 0.7230 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9290 - accuracy: 0.7768\n",
      "Epoch 00059: val_accuracy did not improve from 0.72429\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9297 - accuracy: 0.7765 - val_loss: 1.1523 - val_accuracy: 0.7235 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9279 - accuracy: 0.7710\n",
      "Epoch 00060: val_accuracy did not improve from 0.72429\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.9295 - accuracy: 0.7706 - val_loss: 1.1547 - val_accuracy: 0.7215 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9236 - accuracy: 0.7737\n",
      "Epoch 00061: val_accuracy did not improve from 0.72429\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.9256 - accuracy: 0.7732 - val_loss: 1.1548 - val_accuracy: 0.7204 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9057 - accuracy: 0.7819\n",
      "Epoch 00062: val_accuracy did not improve from 0.72429\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9075 - accuracy: 0.7812 - val_loss: 1.1586 - val_accuracy: 0.7222 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9152 - accuracy: 0.7757\n",
      "Epoch 00063: val_accuracy did not improve from 0.72429\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9161 - accuracy: 0.7755 - val_loss: 1.1561 - val_accuracy: 0.7230 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9066 - accuracy: 0.7791\n",
      "Epoch 00064: val_accuracy improved from 0.72429 to 0.72583, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 0.9076 - accuracy: 0.7788 - val_loss: 1.1506 - val_accuracy: 0.7258 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9102 - accuracy: 0.7758\n",
      "Epoch 00065: val_accuracy did not improve from 0.72583\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.9122 - accuracy: 0.7754 - val_loss: 1.1523 - val_accuracy: 0.7235 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9055 - accuracy: 0.7781\n",
      "Epoch 00066: val_accuracy did not improve from 0.72583\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9081 - accuracy: 0.7776 - val_loss: 1.1537 - val_accuracy: 0.7238 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9032 - accuracy: 0.7783\n",
      "Epoch 00067: val_accuracy improved from 0.72583 to 0.72814, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.9081 - accuracy: 0.7774 - val_loss: 1.1482 - val_accuracy: 0.7281 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9016 - accuracy: 0.7828\n",
      "Epoch 00068: val_accuracy did not improve from 0.72814\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9026 - accuracy: 0.7825 - val_loss: 1.1500 - val_accuracy: 0.7238 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9027 - accuracy: 0.7847\n",
      "Epoch 00069: val_accuracy did not improve from 0.72814\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9055 - accuracy: 0.7836 - val_loss: 1.1553 - val_accuracy: 0.7256 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8900 - accuracy: 0.7833\n",
      "Epoch 00070: val_accuracy did not improve from 0.72814\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.8922 - accuracy: 0.7826 - val_loss: 1.1530 - val_accuracy: 0.7227 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8961 - accuracy: 0.7833\n",
      "Epoch 00071: val_accuracy did not improve from 0.72814\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8961 - accuracy: 0.7833 - val_loss: 1.1580 - val_accuracy: 0.7217 - lr: 1.0000e-04\n",
      "Epoch 72/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8920 - accuracy: 0.7831\n",
      "Epoch 00072: val_accuracy did not improve from 0.72814\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.8920 - accuracy: 0.7831 - val_loss: 1.1524 - val_accuracy: 0.7261 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9053 - accuracy: 0.7833\n",
      "Epoch 00073: val_accuracy did not improve from 0.72814\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9061 - accuracy: 0.7836 - val_loss: 1.1508 - val_accuracy: 0.7269 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8837 - accuracy: 0.7848\n",
      "Epoch 00074: val_accuracy did not improve from 0.72814\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8850 - accuracy: 0.7845 - val_loss: 1.1534 - val_accuracy: 0.7251 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8855 - accuracy: 0.7813\n",
      "Epoch 00075: val_accuracy did not improve from 0.72814\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.8848 - accuracy: 0.7815 - val_loss: 1.1457 - val_accuracy: 0.7261 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8817 - accuracy: 0.7851\n",
      "Epoch 00076: val_accuracy improved from 0.72814 to 0.72865, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.8836 - accuracy: 0.7844 - val_loss: 1.1446 - val_accuracy: 0.7286 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8816 - accuracy: 0.7824\n",
      "Epoch 00077: val_accuracy did not improve from 0.72865\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 0.8824 - accuracy: 0.7826 - val_loss: 1.1435 - val_accuracy: 0.7281 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8723 - accuracy: 0.7907\n",
      "Epoch 00078: val_accuracy improved from 0.72865 to 0.73019, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.8765 - accuracy: 0.7898 - val_loss: 1.1370 - val_accuracy: 0.7302 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8850 - accuracy: 0.7900\n",
      "Epoch 00079: val_accuracy did not improve from 0.73019\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8850 - accuracy: 0.7900 - val_loss: 1.1431 - val_accuracy: 0.7253 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8720 - accuracy: 0.7909\n",
      "Epoch 00080: val_accuracy did not improve from 0.73019\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8713 - accuracy: 0.7909 - val_loss: 1.1448 - val_accuracy: 0.7269 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8730 - accuracy: 0.7890\n",
      "Epoch 00081: val_accuracy did not improve from 0.73019\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8755 - accuracy: 0.7884 - val_loss: 1.1382 - val_accuracy: 0.7286 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8702 - accuracy: 0.7913\n",
      "Epoch 00082: val_accuracy did not improve from 0.73019\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8723 - accuracy: 0.7906 - val_loss: 1.1411 - val_accuracy: 0.7286 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8593 - accuracy: 0.7944\n",
      "Epoch 00083: val_accuracy did not improve from 0.73019\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8597 - accuracy: 0.7942 - val_loss: 1.1401 - val_accuracy: 0.7289 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8603 - accuracy: 0.7927\n",
      "Epoch 00084: val_accuracy did not improve from 0.73019\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8624 - accuracy: 0.7924 - val_loss: 1.1395 - val_accuracy: 0.7286 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8624 - accuracy: 0.7938\n",
      "Epoch 00085: val_accuracy did not improve from 0.73019\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.8624 - accuracy: 0.7938 - val_loss: 1.1364 - val_accuracy: 0.7294 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8620 - accuracy: 0.7944\n",
      "Epoch 00086: val_accuracy did not improve from 0.73019\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.8613 - accuracy: 0.7947 - val_loss: 1.1401 - val_accuracy: 0.7302 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8673 - accuracy: 0.7906\n",
      "Epoch 00087: val_accuracy did not improve from 0.73019\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8741 - accuracy: 0.7882 - val_loss: 1.1388 - val_accuracy: 0.7292 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8591 - accuracy: 0.7960\n",
      "Epoch 00088: val_accuracy did not improve from 0.73019\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8591 - accuracy: 0.7960 - val_loss: 1.1392 - val_accuracy: 0.7299 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8737 - accuracy: 0.7879\n",
      "Epoch 00089: val_accuracy did not improve from 0.73019\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8737 - accuracy: 0.7879 - val_loss: 1.1378 - val_accuracy: 0.7289 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8631 - accuracy: 0.7909\n",
      "Epoch 00090: val_accuracy improved from 0.73019 to 0.73224, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 0.8649 - accuracy: 0.7905 - val_loss: 1.1413 - val_accuracy: 0.7322 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8593 - accuracy: 0.7915\n",
      "Epoch 00091: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.8593 - accuracy: 0.7915 - val_loss: 1.1399 - val_accuracy: 0.7297 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8505 - accuracy: 0.7949\n",
      "Epoch 00092: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8521 - accuracy: 0.7946 - val_loss: 1.1386 - val_accuracy: 0.7281 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8589 - accuracy: 0.7973\n",
      "Epoch 00093: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.8593 - accuracy: 0.7971 - val_loss: 1.1362 - val_accuracy: 0.7292 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8603 - accuracy: 0.7911\n",
      "Epoch 00094: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.8654 - accuracy: 0.7898 - val_loss: 1.1359 - val_accuracy: 0.7304 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8549 - accuracy: 0.8016\n",
      "Epoch 00095: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8570 - accuracy: 0.8010 - val_loss: 1.1360 - val_accuracy: 0.7284 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8586 - accuracy: 0.7899\n",
      "Epoch 00096: val_accuracy did not improve from 0.73224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 85ms/step - loss: 0.8618 - accuracy: 0.7893 - val_loss: 1.1325 - val_accuracy: 0.7286 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8577 - accuracy: 0.7964\n",
      "Epoch 00097: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.8577 - accuracy: 0.7964 - val_loss: 1.1346 - val_accuracy: 0.7302 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8474 - accuracy: 0.7934\n",
      "Epoch 00098: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.8519 - accuracy: 0.7921 - val_loss: 1.1340 - val_accuracy: 0.7279 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8521 - accuracy: 0.7927\n",
      "Epoch 00099: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8540 - accuracy: 0.7921 - val_loss: 1.1340 - val_accuracy: 0.7284 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8711 - accuracy: 0.7888\n",
      "Epoch 00100: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8737 - accuracy: 0.7879 - val_loss: 1.1364 - val_accuracy: 0.7299 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8490 - accuracy: 0.7984\n",
      "Epoch 00101: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8525 - accuracy: 0.7971 - val_loss: 1.1373 - val_accuracy: 0.7297 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8549 - accuracy: 0.7928\n",
      "Epoch 00102: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8547 - accuracy: 0.7929 - val_loss: 1.1339 - val_accuracy: 0.7299 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8466 - accuracy: 0.7966\n",
      "Epoch 00103: val_accuracy improved from 0.73224 to 0.73275, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.8479 - accuracy: 0.7961 - val_loss: 1.1328 - val_accuracy: 0.7328 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8591 - accuracy: 0.7907\n",
      "Epoch 00104: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8591 - accuracy: 0.7907 - val_loss: 1.1347 - val_accuracy: 0.7299 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8552 - accuracy: 0.7921\n",
      "Epoch 00105: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8552 - accuracy: 0.7921 - val_loss: 1.1349 - val_accuracy: 0.7292 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8632 - accuracy: 0.7927\n",
      "Epoch 00106: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8630 - accuracy: 0.7922 - val_loss: 1.1336 - val_accuracy: 0.7304 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8600 - accuracy: 0.7944\n",
      "Epoch 00107: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8614 - accuracy: 0.7942 - val_loss: 1.1357 - val_accuracy: 0.7322 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8548 - accuracy: 0.7944\n",
      "Epoch 00108: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8548 - accuracy: 0.7944 - val_loss: 1.1368 - val_accuracy: 0.7294 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8568 - accuracy: 0.7912\n",
      "Epoch 00109: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8572 - accuracy: 0.7917 - val_loss: 1.1330 - val_accuracy: 0.7307 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8428 - accuracy: 0.7961\n",
      "Epoch 00110: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8431 - accuracy: 0.7958 - val_loss: 1.1331 - val_accuracy: 0.7310 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8564 - accuracy: 0.7940\n",
      "Epoch 00111: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8564 - accuracy: 0.7940 - val_loss: 1.1343 - val_accuracy: 0.7310 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8463 - accuracy: 0.7942\n",
      "Epoch 00112: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8487 - accuracy: 0.7931 - val_loss: 1.1338 - val_accuracy: 0.7312 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8564 - accuracy: 0.7959\n",
      "Epoch 00113: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8564 - accuracy: 0.7959 - val_loss: 1.1326 - val_accuracy: 0.7317 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8593 - accuracy: 0.7884\n",
      "Epoch 00114: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8593 - accuracy: 0.7884 - val_loss: 1.1343 - val_accuracy: 0.7325 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8529 - accuracy: 0.7914\n",
      "Epoch 00115: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.8548 - accuracy: 0.7909 - val_loss: 1.1349 - val_accuracy: 0.7304 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8550 - accuracy: 0.7933\n",
      "Epoch 00116: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8550 - accuracy: 0.7933 - val_loss: 1.1342 - val_accuracy: 0.7297 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8561 - accuracy: 0.7928\n",
      "Epoch 00117: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8561 - accuracy: 0.7928 - val_loss: 1.1328 - val_accuracy: 0.7320 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8385 - accuracy: 0.7988\n",
      "Epoch 00118: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.8385 - accuracy: 0.7988 - val_loss: 1.1313 - val_accuracy: 0.7302 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8515 - accuracy: 0.7902\n",
      "Epoch 00119: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8519 - accuracy: 0.7903 - val_loss: 1.1331 - val_accuracy: 0.7317 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8576 - accuracy: 0.7896\n",
      "Epoch 00120: val_accuracy did not improve from 0.73275\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.8576 - accuracy: 0.7896 - val_loss: 1.1347 - val_accuracy: 0.7322 - lr: 1.0000e-06\n",
      "epoch_number 103\n",
      "train accuracy and validation accuracy 0.796131432056427 0.7327519655227661\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.1328 - accuracy: 0.7328\n",
      "test_accuracy 0.7327519655227661\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661]\n",
      "0.7471716337733798\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S10_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S10_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.3611 - accuracy: 0.1747\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.34214, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 371ms/step - loss: 4.3472 - accuracy: 0.1754 - val_loss: 2.9462 - val_accuracy: 0.3421 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.7279 - accuracy: 0.3253\n",
      "Epoch 00002: val_accuracy improved from 0.34214 to 0.44960, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 126ms/step - loss: 2.7253 - accuracy: 0.3263 - val_loss: 2.2409 - val_accuracy: 0.4496 - lr: 0.0010\n",
      "Epoch 3/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.2866 - accuracy: 0.4298\n",
      "Epoch 00003: val_accuracy improved from 0.44960 to 0.53732, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 127ms/step - loss: 2.2866 - accuracy: 0.4298 - val_loss: 1.8998 - val_accuracy: 0.5373 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0387 - accuracy: 0.4862\n",
      "Epoch 00004: val_accuracy improved from 0.53732 to 0.58682, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 2.0389 - accuracy: 0.4861 - val_loss: 1.7142 - val_accuracy: 0.5868 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8613 - accuracy: 0.5303\n",
      "Epoch 00005: val_accuracy improved from 0.58682 to 0.61477, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.8579 - accuracy: 0.5313 - val_loss: 1.5883 - val_accuracy: 0.6148 - lr: 0.0010\n",
      "Epoch 6/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.7083 - accuracy: 0.5724\n",
      "Epoch 00006: val_accuracy improved from 0.61477 to 0.65171, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.7083 - accuracy: 0.5724 - val_loss: 1.4412 - val_accuracy: 0.6517 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5965 - accuracy: 0.6008\n",
      "Epoch 00007: val_accuracy improved from 0.65171 to 0.66248, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 1.5976 - accuracy: 0.6009 - val_loss: 1.3904 - val_accuracy: 0.6625 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5056 - accuracy: 0.6221\n",
      "Epoch 00008: val_accuracy improved from 0.66248 to 0.68684, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 1.5056 - accuracy: 0.6222 - val_loss: 1.2906 - val_accuracy: 0.6868 - lr: 0.0010\n",
      "Epoch 9/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.4134 - accuracy: 0.6460\n",
      "Epoch 00009: val_accuracy improved from 0.68684 to 0.69685, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 1.4134 - accuracy: 0.6460 - val_loss: 1.2323 - val_accuracy: 0.6968 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3883 - accuracy: 0.6549\n",
      "Epoch 00010: val_accuracy improved from 0.69685 to 0.70839, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 1.3912 - accuracy: 0.6542 - val_loss: 1.1986 - val_accuracy: 0.7084 - lr: 0.0010\n",
      "Epoch 11/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3355 - accuracy: 0.6593\n",
      "Epoch 00011: val_accuracy improved from 0.70839 to 0.71377, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.3355 - accuracy: 0.6593 - val_loss: 1.1625 - val_accuracy: 0.7138 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2767 - accuracy: 0.6794\n",
      "Epoch 00012: val_accuracy improved from 0.71377 to 0.72121, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 1.2776 - accuracy: 0.6796 - val_loss: 1.1350 - val_accuracy: 0.7212 - lr: 0.0010\n",
      "Epoch 13/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2208 - accuracy: 0.6968\n",
      "Epoch 00013: val_accuracy improved from 0.72121 to 0.72378, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 1.2208 - accuracy: 0.6968 - val_loss: 1.0962 - val_accuracy: 0.7238 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2170 - accuracy: 0.6962\n",
      "Epoch 00014: val_accuracy improved from 0.72378 to 0.73044, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 1.2150 - accuracy: 0.6965 - val_loss: 1.1000 - val_accuracy: 0.7304 - lr: 0.0010\n",
      "Epoch 15/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1687 - accuracy: 0.7132\n",
      "Epoch 00015: val_accuracy improved from 0.73044 to 0.75712, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 1.1687 - accuracy: 0.7132 - val_loss: 1.0358 - val_accuracy: 0.7571 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1197 - accuracy: 0.7253\n",
      "Epoch 00016: val_accuracy did not improve from 0.75712\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.1219 - accuracy: 0.7241 - val_loss: 1.0455 - val_accuracy: 0.7476 - lr: 0.0010\n",
      "Epoch 17/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1047 - accuracy: 0.7315\n",
      "Epoch 00017: val_accuracy did not improve from 0.75712\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.1047 - accuracy: 0.7315 - val_loss: 1.0582 - val_accuracy: 0.7456 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0972 - accuracy: 0.7340\n",
      "Epoch 00018: val_accuracy did not improve from 0.75712\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 1.0992 - accuracy: 0.7336 - val_loss: 1.0319 - val_accuracy: 0.7492 - lr: 0.0010\n",
      "Epoch 19/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0524 - accuracy: 0.7479\n",
      "Epoch 00019: val_accuracy did not improve from 0.75712\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 1.0524 - accuracy: 0.7479 - val_loss: 1.0271 - val_accuracy: 0.7476 - lr: 0.0010\n",
      "Epoch 20/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0263 - accuracy: 0.7523\n",
      "Epoch 00020: val_accuracy did not improve from 0.75712\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1.0263 - accuracy: 0.7523 - val_loss: 0.9859 - val_accuracy: 0.7569 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0172 - accuracy: 0.7527\n",
      "Epoch 00021: val_accuracy did not improve from 0.75712\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0159 - accuracy: 0.7533 - val_loss: 1.0004 - val_accuracy: 0.7528 - lr: 0.0010\n",
      "Epoch 22/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9856 - accuracy: 0.7613\n",
      "Epoch 00022: val_accuracy improved from 0.75712 to 0.77328, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.9856 - accuracy: 0.7613 - val_loss: 0.9490 - val_accuracy: 0.7733 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9512 - accuracy: 0.7746\n",
      "Epoch 00023: val_accuracy did not improve from 0.77328\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9535 - accuracy: 0.7740 - val_loss: 0.9534 - val_accuracy: 0.7707 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9396 - accuracy: 0.7748\n",
      "Epoch 00024: val_accuracy did not improve from 0.77328\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9423 - accuracy: 0.7742 - val_loss: 0.9627 - val_accuracy: 0.7625 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9525 - accuracy: 0.7671\n",
      "Epoch 00025: val_accuracy did not improve from 0.77328\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 0.9524 - accuracy: 0.7673 - val_loss: 0.9395 - val_accuracy: 0.7687 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8990 - accuracy: 0.7848\n",
      "Epoch 00026: val_accuracy improved from 0.77328 to 0.77969, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.8995 - accuracy: 0.7847 - val_loss: 0.9219 - val_accuracy: 0.7797 - lr: 0.0010\n",
      "Epoch 27/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8931 - accuracy: 0.7899\n",
      "Epoch 00027: val_accuracy did not improve from 0.77969\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8931 - accuracy: 0.7899 - val_loss: 0.9395 - val_accuracy: 0.7784 - lr: 0.0010\n",
      "Epoch 28/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8916 - accuracy: 0.7849\n",
      "Epoch 00028: val_accuracy improved from 0.77969 to 0.78225, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 0.8916 - accuracy: 0.7849 - val_loss: 0.9143 - val_accuracy: 0.7823 - lr: 0.0010\n",
      "Epoch 29/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8673 - accuracy: 0.8002\n",
      "Epoch 00029: val_accuracy improved from 0.78225 to 0.78815, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.8673 - accuracy: 0.8002 - val_loss: 0.8992 - val_accuracy: 0.7882 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8774 - accuracy: 0.7928\n",
      "Epoch 00030: val_accuracy improved from 0.78815 to 0.79020, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 0.8781 - accuracy: 0.7924 - val_loss: 0.8817 - val_accuracy: 0.7902 - lr: 0.0010\n",
      "Epoch 31/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8518 - accuracy: 0.7994\n",
      "Epoch 00031: val_accuracy did not improve from 0.79020\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8518 - accuracy: 0.7994 - val_loss: 0.9112 - val_accuracy: 0.7879 - lr: 0.0010\n",
      "Epoch 32/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8346 - accuracy: 0.8021\n",
      "Epoch 00032: val_accuracy did not improve from 0.79020\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.8346 - accuracy: 0.8021 - val_loss: 0.9106 - val_accuracy: 0.7743 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8139 - accuracy: 0.8060\n",
      "Epoch 00033: val_accuracy did not improve from 0.79020\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8154 - accuracy: 0.8053 - val_loss: 0.9287 - val_accuracy: 0.7846 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8206 - accuracy: 0.8069\n",
      "Epoch 00034: val_accuracy improved from 0.79020 to 0.79302, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.8206 - accuracy: 0.8070 - val_loss: 0.8901 - val_accuracy: 0.7930 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8207 - accuracy: 0.8067\n",
      "Epoch 00035: val_accuracy did not improve from 0.79302\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 0.8219 - accuracy: 0.8059 - val_loss: 0.8793 - val_accuracy: 0.7892 - lr: 0.0010\n",
      "Epoch 36/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8082 - accuracy: 0.8092\n",
      "Epoch 00036: val_accuracy did not improve from 0.79302\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.8082 - accuracy: 0.8092 - val_loss: 0.9049 - val_accuracy: 0.7892 - lr: 0.0010\n",
      "Epoch 37/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7853 - accuracy: 0.8167\n",
      "Epoch 00037: val_accuracy did not improve from 0.79302\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 0.7853 - accuracy: 0.8167 - val_loss: 0.8785 - val_accuracy: 0.7930 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7790 - accuracy: 0.8167\n",
      "Epoch 00038: val_accuracy improved from 0.79302 to 0.79585, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 0.7805 - accuracy: 0.8157 - val_loss: 0.8743 - val_accuracy: 0.7958 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7790 - accuracy: 0.8221\n",
      "Epoch 00039: val_accuracy did not improve from 0.79585\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7813 - accuracy: 0.8212 - val_loss: 0.8853 - val_accuracy: 0.7930 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7529 - accuracy: 0.8261\n",
      "Epoch 00040: val_accuracy improved from 0.79585 to 0.79738, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.7542 - accuracy: 0.8257 - val_loss: 0.8609 - val_accuracy: 0.7974 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7269 - accuracy: 0.8332\n",
      "Epoch 00041: val_accuracy improved from 0.79738 to 0.80354, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.7269 - accuracy: 0.8332 - val_loss: 0.8390 - val_accuracy: 0.8035 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7148 - accuracy: 0.8367\n",
      "Epoch 00042: val_accuracy improved from 0.80354 to 0.80687, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.7152 - accuracy: 0.8369 - val_loss: 0.8355 - val_accuracy: 0.8069 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6957 - accuracy: 0.8462\n",
      "Epoch 00043: val_accuracy improved from 0.80687 to 0.80739, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 0.6997 - accuracy: 0.8454 - val_loss: 0.8342 - val_accuracy: 0.8074 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6814 - accuracy: 0.8509\n",
      "Epoch 00044: val_accuracy improved from 0.80739 to 0.80841, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 0.6821 - accuracy: 0.8509 - val_loss: 0.8332 - val_accuracy: 0.8084 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6721 - accuracy: 0.8500\n",
      "Epoch 00045: val_accuracy improved from 0.80841 to 0.80995, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.6728 - accuracy: 0.8498 - val_loss: 0.8275 - val_accuracy: 0.8100 - lr: 1.0000e-04\n",
      "Epoch 46/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6852 - accuracy: 0.8464\n",
      "Epoch 00046: val_accuracy did not improve from 0.80995\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6853 - accuracy: 0.8466 - val_loss: 0.8311 - val_accuracy: 0.8076 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6711 - accuracy: 0.8533\n",
      "Epoch 00047: val_accuracy did not improve from 0.80995\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.6724 - accuracy: 0.8527 - val_loss: 0.8261 - val_accuracy: 0.8092 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6727 - accuracy: 0.8504\n",
      "Epoch 00048: val_accuracy did not improve from 0.80995\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.6731 - accuracy: 0.8505 - val_loss: 0.8274 - val_accuracy: 0.8092 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6687 - accuracy: 0.8522\n",
      "Epoch 00049: val_accuracy improved from 0.80995 to 0.81046, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.6690 - accuracy: 0.8521 - val_loss: 0.8279 - val_accuracy: 0.8105 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6569 - accuracy: 0.8547\n",
      "Epoch 00050: val_accuracy improved from 0.81046 to 0.81072, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.6569 - accuracy: 0.8547 - val_loss: 0.8235 - val_accuracy: 0.8107 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6495 - accuracy: 0.8580\n",
      "Epoch 00051: val_accuracy did not improve from 0.81072\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.6519 - accuracy: 0.8573 - val_loss: 0.8189 - val_accuracy: 0.8102 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6506 - accuracy: 0.8607\n",
      "Epoch 00052: val_accuracy improved from 0.81072 to 0.81226, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.6523 - accuracy: 0.8598 - val_loss: 0.8134 - val_accuracy: 0.8123 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6491 - accuracy: 0.8572\n",
      "Epoch 00053: val_accuracy did not improve from 0.81226\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.6480 - accuracy: 0.8575 - val_loss: 0.8188 - val_accuracy: 0.8102 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6510 - accuracy: 0.8543\n",
      "Epoch 00054: val_accuracy did not improve from 0.81226\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.6510 - accuracy: 0.8543 - val_loss: 0.8156 - val_accuracy: 0.8115 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6448 - accuracy: 0.8589\n",
      "Epoch 00055: val_accuracy improved from 0.81226 to 0.81457, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.6448 - accuracy: 0.8589 - val_loss: 0.8138 - val_accuracy: 0.8146 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6331 - accuracy: 0.8657\n",
      "Epoch 00056: val_accuracy did not improve from 0.81457\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.6331 - accuracy: 0.8655 - val_loss: 0.8217 - val_accuracy: 0.8107 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6394 - accuracy: 0.8602\n",
      "Epoch 00057: val_accuracy did not improve from 0.81457\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.6422 - accuracy: 0.8595 - val_loss: 0.8156 - val_accuracy: 0.8117 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6257 - accuracy: 0.8656\n",
      "Epoch 00058: val_accuracy did not improve from 0.81457\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.6261 - accuracy: 0.8654 - val_loss: 0.8165 - val_accuracy: 0.8130 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6360 - accuracy: 0.8581\n",
      "Epoch 00059: val_accuracy did not improve from 0.81457\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.6360 - accuracy: 0.8581 - val_loss: 0.8181 - val_accuracy: 0.8079 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6290 - accuracy: 0.8610\n",
      "Epoch 00060: val_accuracy did not improve from 0.81457\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.6297 - accuracy: 0.8612 - val_loss: 0.8140 - val_accuracy: 0.8105 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6226 - accuracy: 0.8665\n",
      "Epoch 00061: val_accuracy did not improve from 0.81457\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.6226 - accuracy: 0.8665 - val_loss: 0.8155 - val_accuracy: 0.8074 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6234 - accuracy: 0.8669\n",
      "Epoch 00062: val_accuracy did not improve from 0.81457\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.6234 - accuracy: 0.8669 - val_loss: 0.8099 - val_accuracy: 0.8105 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6179 - accuracy: 0.8642\n",
      "Epoch 00063: val_accuracy did not improve from 0.81457\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6179 - accuracy: 0.8642 - val_loss: 0.8114 - val_accuracy: 0.8133 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6176 - accuracy: 0.8682\n",
      "Epoch 00064: val_accuracy did not improve from 0.81457\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6176 - accuracy: 0.8682 - val_loss: 0.8137 - val_accuracy: 0.8143 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6211 - accuracy: 0.8678\n",
      "Epoch 00065: val_accuracy did not improve from 0.81457\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.6222 - accuracy: 0.8670 - val_loss: 0.8141 - val_accuracy: 0.8141 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6204 - accuracy: 0.8658\n",
      "Epoch 00066: val_accuracy did not improve from 0.81457\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.6215 - accuracy: 0.8652 - val_loss: 0.8155 - val_accuracy: 0.8128 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6058 - accuracy: 0.8712\n",
      "Epoch 00067: val_accuracy did not improve from 0.81457\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.6064 - accuracy: 0.8711 - val_loss: 0.8147 - val_accuracy: 0.8117 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6049 - accuracy: 0.8718\n",
      "Epoch 00068: val_accuracy did not improve from 0.81457\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6058 - accuracy: 0.8712 - val_loss: 0.8146 - val_accuracy: 0.8107 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6038 - accuracy: 0.8687\n",
      "Epoch 00069: val_accuracy did not improve from 0.81457\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.6051 - accuracy: 0.8684 - val_loss: 0.8147 - val_accuracy: 0.8117 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6037 - accuracy: 0.8694\n",
      "Epoch 00070: val_accuracy did not improve from 0.81457\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6035 - accuracy: 0.8697 - val_loss: 0.8118 - val_accuracy: 0.8117 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6094 - accuracy: 0.8694\n",
      "Epoch 00071: val_accuracy did not improve from 0.81457\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6097 - accuracy: 0.8697 - val_loss: 0.8123 - val_accuracy: 0.8120 - lr: 1.0000e-04\n",
      "Epoch 72/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5982 - accuracy: 0.8737\n",
      "Epoch 00072: val_accuracy improved from 0.81457 to 0.81508, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.5997 - accuracy: 0.8731 - val_loss: 0.8010 - val_accuracy: 0.8151 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6017 - accuracy: 0.8742\n",
      "Epoch 00073: val_accuracy did not improve from 0.81508\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.6017 - accuracy: 0.8742 - val_loss: 0.8068 - val_accuracy: 0.8138 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6058 - accuracy: 0.8732\n",
      "Epoch 00074: val_accuracy improved from 0.81508 to 0.81816, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.6058 - accuracy: 0.8732 - val_loss: 0.8030 - val_accuracy: 0.8182 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5891 - accuracy: 0.8742\n",
      "Epoch 00075: val_accuracy did not improve from 0.81816\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.5910 - accuracy: 0.8738 - val_loss: 0.8021 - val_accuracy: 0.8141 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5950 - accuracy: 0.8741\n",
      "Epoch 00076: val_accuracy did not improve from 0.81816\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.5950 - accuracy: 0.8741 - val_loss: 0.8034 - val_accuracy: 0.8143 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5891 - accuracy: 0.8734\n",
      "Epoch 00077: val_accuracy did not improve from 0.81816\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.5891 - accuracy: 0.8734 - val_loss: 0.8044 - val_accuracy: 0.8171 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5872 - accuracy: 0.8741\n",
      "Epoch 00078: val_accuracy did not improve from 0.81816\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.5872 - accuracy: 0.8741 - val_loss: 0.8093 - val_accuracy: 0.8143 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5841 - accuracy: 0.8714\n",
      "Epoch 00079: val_accuracy did not improve from 0.81816\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.5864 - accuracy: 0.8705 - val_loss: 0.8035 - val_accuracy: 0.8171 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5833 - accuracy: 0.8742\n",
      "Epoch 00080: val_accuracy did not improve from 0.81816\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.5865 - accuracy: 0.8741 - val_loss: 0.8035 - val_accuracy: 0.8179 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5834 - accuracy: 0.8729\n",
      "Epoch 00081: val_accuracy improved from 0.81816 to 0.81867, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 0.5832 - accuracy: 0.8731 - val_loss: 0.8055 - val_accuracy: 0.8187 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5921 - accuracy: 0.8738\n",
      "Epoch 00082: val_accuracy did not improve from 0.81867\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.5921 - accuracy: 0.8738 - val_loss: 0.8034 - val_accuracy: 0.8179 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5774 - accuracy: 0.8762\n",
      "Epoch 00083: val_accuracy did not improve from 0.81867\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.5788 - accuracy: 0.8757 - val_loss: 0.8028 - val_accuracy: 0.8174 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5724 - accuracy: 0.8853\n",
      "Epoch 00084: val_accuracy improved from 0.81867 to 0.81893, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 0.5724 - accuracy: 0.8853 - val_loss: 0.8019 - val_accuracy: 0.8189 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5764 - accuracy: 0.8745\n",
      "Epoch 00085: val_accuracy did not improve from 0.81893\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.5764 - accuracy: 0.8745 - val_loss: 0.8015 - val_accuracy: 0.8179 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5785 - accuracy: 0.8738\n",
      "Epoch 00086: val_accuracy did not improve from 0.81893\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5788 - accuracy: 0.8737 - val_loss: 0.8087 - val_accuracy: 0.8161 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5804 - accuracy: 0.8737\n",
      "Epoch 00087: val_accuracy did not improve from 0.81893\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.5818 - accuracy: 0.8734 - val_loss: 0.8068 - val_accuracy: 0.8141 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5797 - accuracy: 0.8772\n",
      "Epoch 00088: val_accuracy did not improve from 0.81893\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.5805 - accuracy: 0.8772 - val_loss: 0.8025 - val_accuracy: 0.8164 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5833 - accuracy: 0.8764\n",
      "Epoch 00089: val_accuracy did not improve from 0.81893\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.5833 - accuracy: 0.8764 - val_loss: 0.8033 - val_accuracy: 0.8182 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5741 - accuracy: 0.8799\n",
      "Epoch 00090: val_accuracy improved from 0.81893 to 0.81918, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 0.5741 - accuracy: 0.8799 - val_loss: 0.7987 - val_accuracy: 0.8192 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5724 - accuracy: 0.8798\n",
      "Epoch 00091: val_accuracy did not improve from 0.81918\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.5737 - accuracy: 0.8794 - val_loss: 0.8034 - val_accuracy: 0.8141 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5820 - accuracy: 0.8776\n",
      "Epoch 00092: val_accuracy did not improve from 0.81918\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 0.5820 - accuracy: 0.8776 - val_loss: 0.7986 - val_accuracy: 0.8166 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5818 - accuracy: 0.8728\n",
      "Epoch 00093: val_accuracy did not improve from 0.81918\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.5833 - accuracy: 0.8727 - val_loss: 0.7985 - val_accuracy: 0.8174 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5731 - accuracy: 0.8737\n",
      "Epoch 00094: val_accuracy did not improve from 0.81918\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.5731 - accuracy: 0.8737 - val_loss: 0.8008 - val_accuracy: 0.8164 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5715 - accuracy: 0.8787\n",
      "Epoch 00095: val_accuracy did not improve from 0.81918\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.5715 - accuracy: 0.8787 - val_loss: 0.7979 - val_accuracy: 0.8174 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5804 - accuracy: 0.8761\n",
      "Epoch 00096: val_accuracy did not improve from 0.81918\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.5816 - accuracy: 0.8759 - val_loss: 0.8014 - val_accuracy: 0.8169 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5805 - accuracy: 0.8730\n",
      "Epoch 00097: val_accuracy did not improve from 0.81918\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5801 - accuracy: 0.8733 - val_loss: 0.8020 - val_accuracy: 0.8151 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5796 - accuracy: 0.8737\n",
      "Epoch 00098: val_accuracy did not improve from 0.81918\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5803 - accuracy: 0.8730 - val_loss: 0.7999 - val_accuracy: 0.8171 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5766 - accuracy: 0.8791\n",
      "Epoch 00099: val_accuracy improved from 0.81918 to 0.81944, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.5780 - accuracy: 0.8787 - val_loss: 0.7993 - val_accuracy: 0.8194 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5596 - accuracy: 0.8840\n",
      "Epoch 00100: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.5604 - accuracy: 0.8833 - val_loss: 0.8016 - val_accuracy: 0.8166 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5688 - accuracy: 0.8783\n",
      "Epoch 00101: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5709 - accuracy: 0.8777 - val_loss: 0.8031 - val_accuracy: 0.8153 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5753 - accuracy: 0.8813\n",
      "Epoch 00102: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.5753 - accuracy: 0.8813 - val_loss: 0.8029 - val_accuracy: 0.8138 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5668 - accuracy: 0.8797\n",
      "Epoch 00103: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.5668 - accuracy: 0.8797 - val_loss: 0.8004 - val_accuracy: 0.8189 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5754 - accuracy: 0.8744\n",
      "Epoch 00104: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.5754 - accuracy: 0.8744 - val_loss: 0.7940 - val_accuracy: 0.8192 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5701 - accuracy: 0.8797\n",
      "Epoch 00105: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5712 - accuracy: 0.8792 - val_loss: 0.7969 - val_accuracy: 0.8192 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5796 - accuracy: 0.8757\n",
      "Epoch 00106: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.5793 - accuracy: 0.8758 - val_loss: 0.8011 - val_accuracy: 0.8146 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5732 - accuracy: 0.8797\n",
      "Epoch 00107: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.5745 - accuracy: 0.8788 - val_loss: 0.8021 - val_accuracy: 0.8153 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5754 - accuracy: 0.8756\n",
      "Epoch 00108: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.5754 - accuracy: 0.8756 - val_loss: 0.8013 - val_accuracy: 0.8166 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5716 - accuracy: 0.8789\n",
      "Epoch 00109: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.5716 - accuracy: 0.8789 - val_loss: 0.8047 - val_accuracy: 0.8159 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5810 - accuracy: 0.8757\n",
      "Epoch 00110: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.5803 - accuracy: 0.8757 - val_loss: 0.8015 - val_accuracy: 0.8187 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5662 - accuracy: 0.8802\n",
      "Epoch 00111: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5655 - accuracy: 0.8806 - val_loss: 0.7995 - val_accuracy: 0.8174 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5713 - accuracy: 0.8803\n",
      "Epoch 00112: val_accuracy did not improve from 0.81944\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5760 - accuracy: 0.8792 - val_loss: 0.7998 - val_accuracy: 0.8184 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5726 - accuracy: 0.8774\n",
      "Epoch 00113: val_accuracy improved from 0.81944 to 0.82021, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.5743 - accuracy: 0.8768 - val_loss: 0.7965 - val_accuracy: 0.8202 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5763 - accuracy: 0.8763\n",
      "Epoch 00114: val_accuracy improved from 0.82021 to 0.82124, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.5767 - accuracy: 0.8766 - val_loss: 0.8011 - val_accuracy: 0.8212 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5689 - accuracy: 0.8777\n",
      "Epoch 00115: val_accuracy did not improve from 0.82124\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.5693 - accuracy: 0.8778 - val_loss: 0.7992 - val_accuracy: 0.8205 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5675 - accuracy: 0.8747\n",
      "Epoch 00116: val_accuracy did not improve from 0.82124\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5681 - accuracy: 0.8747 - val_loss: 0.8018 - val_accuracy: 0.8169 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5708 - accuracy: 0.8780\n",
      "Epoch 00117: val_accuracy did not improve from 0.82124\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5725 - accuracy: 0.8771 - val_loss: 0.8025 - val_accuracy: 0.8148 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5629 - accuracy: 0.8791\n",
      "Epoch 00118: val_accuracy did not improve from 0.82124\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5637 - accuracy: 0.8787 - val_loss: 0.8004 - val_accuracy: 0.8164 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5604 - accuracy: 0.8833\n",
      "Epoch 00119: val_accuracy did not improve from 0.82124\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.5604 - accuracy: 0.8833 - val_loss: 0.8002 - val_accuracy: 0.8135 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5640 - accuracy: 0.8800\n",
      "Epoch 00120: val_accuracy did not improve from 0.82124\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.5645 - accuracy: 0.8795 - val_loss: 0.7987 - val_accuracy: 0.8153 - lr: 1.0000e-06\n",
      "epoch_number 114\n",
      "train accuracy and validation accuracy 0.8765798211097717 0.8212361931800842\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.8011 - accuracy: 0.8212\n",
      "test_accuracy 0.8212361931800842\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842]\n",
      "0.7545780897140503\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S11_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S11_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.0578 - accuracy: 0.1756\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.31162, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 361ms/step - loss: 4.0457 - accuracy: 0.1768 - val_loss: 2.8921 - val_accuracy: 0.3116 - lr: 0.0010\n",
      "Epoch 2/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.7438 - accuracy: 0.3041\n",
      "Epoch 00002: val_accuracy improved from 0.31162 to 0.39087, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 2.7438 - accuracy: 0.3041 - val_loss: 2.2899 - val_accuracy: 0.3909 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4035 - accuracy: 0.3730\n",
      "Epoch 00003: val_accuracy improved from 0.39087 to 0.45319, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 2.4052 - accuracy: 0.3734 - val_loss: 2.0740 - val_accuracy: 0.4532 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1979 - accuracy: 0.4226\n",
      "Epoch 00004: val_accuracy improved from 0.45319 to 0.51654, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 2.1986 - accuracy: 0.4227 - val_loss: 1.9002 - val_accuracy: 0.5165 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0460 - accuracy: 0.4599\n",
      "Epoch 00005: val_accuracy improved from 0.51654 to 0.55886, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 2.0477 - accuracy: 0.4596 - val_loss: 1.7799 - val_accuracy: 0.5589 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9138 - accuracy: 0.4974\n",
      "Epoch 00006: val_accuracy improved from 0.55886 to 0.56527, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.9130 - accuracy: 0.4974 - val_loss: 1.7045 - val_accuracy: 0.5653 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8150 - accuracy: 0.5253\n",
      "Epoch 00007: val_accuracy improved from 0.56527 to 0.59785, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 1.8160 - accuracy: 0.5250 - val_loss: 1.6032 - val_accuracy: 0.5978 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7547 - accuracy: 0.5389\n",
      "Epoch 00008: val_accuracy improved from 0.59785 to 0.61426, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.7548 - accuracy: 0.5392 - val_loss: 1.5595 - val_accuracy: 0.6143 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6915 - accuracy: 0.5568\n",
      "Epoch 00009: val_accuracy did not improve from 0.61426\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.6891 - accuracy: 0.5573 - val_loss: 1.5047 - val_accuracy: 0.6140 - lr: 0.0010\n",
      "Epoch 10/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.6157 - accuracy: 0.5763\n",
      "Epoch 00010: val_accuracy improved from 0.61426 to 0.63632, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.6157 - accuracy: 0.5763 - val_loss: 1.4435 - val_accuracy: 0.6363 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5654 - accuracy: 0.5872\n",
      "Epoch 00011: val_accuracy improved from 0.63632 to 0.65222, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 1.5676 - accuracy: 0.5864 - val_loss: 1.4091 - val_accuracy: 0.6522 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5358 - accuracy: 0.5944\n",
      "Epoch 00012: val_accuracy improved from 0.65222 to 0.65735, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 1.5359 - accuracy: 0.5949 - val_loss: 1.3683 - val_accuracy: 0.6573 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4669 - accuracy: 0.6123\n",
      "Epoch 00013: val_accuracy improved from 0.65735 to 0.66940, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.4663 - accuracy: 0.6123 - val_loss: 1.3423 - val_accuracy: 0.6694 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4423 - accuracy: 0.6242\n",
      "Epoch 00014: val_accuracy did not improve from 0.66940\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.4415 - accuracy: 0.6241 - val_loss: 1.3238 - val_accuracy: 0.6620 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4069 - accuracy: 0.6280\n",
      "Epoch 00015: val_accuracy improved from 0.66940 to 0.67787, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.4045 - accuracy: 0.6285 - val_loss: 1.2996 - val_accuracy: 0.6779 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3469 - accuracy: 0.6478\n",
      "Epoch 00016: val_accuracy improved from 0.67787 to 0.68864, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 1.3517 - accuracy: 0.6470 - val_loss: 1.2756 - val_accuracy: 0.6886 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3730 - accuracy: 0.6393\n",
      "Epoch 00017: val_accuracy improved from 0.68864 to 0.69043, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.3740 - accuracy: 0.6392 - val_loss: 1.2493 - val_accuracy: 0.6904 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3341 - accuracy: 0.6523\n",
      "Epoch 00018: val_accuracy improved from 0.69043 to 0.69377, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.3339 - accuracy: 0.6522 - val_loss: 1.2512 - val_accuracy: 0.6938 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3016 - accuracy: 0.6600\n",
      "Epoch 00019: val_accuracy improved from 0.69377 to 0.69941, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.3041 - accuracy: 0.6591 - val_loss: 1.2327 - val_accuracy: 0.6994 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2871 - accuracy: 0.6679\n",
      "Epoch 00020: val_accuracy did not improve from 0.69941\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.2905 - accuracy: 0.6663 - val_loss: 1.2281 - val_accuracy: 0.6945 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2893 - accuracy: 0.6699\n",
      "Epoch 00021: val_accuracy did not improve from 0.69941\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.2926 - accuracy: 0.6685 - val_loss: 1.2360 - val_accuracy: 0.6938 - lr: 0.0010\n",
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2332 - accuracy: 0.6843\n",
      "Epoch 00022: val_accuracy improved from 0.69941 to 0.70633, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 107ms/step - loss: 1.2343 - accuracy: 0.6840 - val_loss: 1.1837 - val_accuracy: 0.7063 - lr: 0.0010\n",
      "Epoch 23/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2120 - accuracy: 0.6900\n",
      "Epoch 00023: val_accuracy did not improve from 0.70633\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.2120 - accuracy: 0.6900 - val_loss: 1.1736 - val_accuracy: 0.7038 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1996 - accuracy: 0.6931\n",
      "Epoch 00024: val_accuracy improved from 0.70633 to 0.71608, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.2001 - accuracy: 0.6927 - val_loss: 1.1715 - val_accuracy: 0.7161 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2099 - accuracy: 0.6904\n",
      "Epoch 00025: val_accuracy did not improve from 0.71608\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.2117 - accuracy: 0.6903 - val_loss: 1.1898 - val_accuracy: 0.7110 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1872 - accuracy: 0.6992\n",
      "Epoch 00026: val_accuracy did not improve from 0.71608\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.1884 - accuracy: 0.6985 - val_loss: 1.1741 - val_accuracy: 0.7112 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1629 - accuracy: 0.7008\n",
      "Epoch 00027: val_accuracy improved from 0.71608 to 0.71942, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.1638 - accuracy: 0.7005 - val_loss: 1.1616 - val_accuracy: 0.7194 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1494 - accuracy: 0.7071\n",
      "Epoch 00028: val_accuracy improved from 0.71942 to 0.72737, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.1519 - accuracy: 0.7061 - val_loss: 1.1282 - val_accuracy: 0.7274 - lr: 0.0010\n",
      "Epoch 29/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1239 - accuracy: 0.7176\n",
      "Epoch 00029: val_accuracy did not improve from 0.72737\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.1239 - accuracy: 0.7176 - val_loss: 1.1601 - val_accuracy: 0.7194 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1569 - accuracy: 0.7067\n",
      "Epoch 00030: val_accuracy did not improve from 0.72737\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.1591 - accuracy: 0.7057 - val_loss: 1.1447 - val_accuracy: 0.7115 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1110 - accuracy: 0.7191\n",
      "Epoch 00031: val_accuracy improved from 0.72737 to 0.73096, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.1145 - accuracy: 0.7182 - val_loss: 1.1199 - val_accuracy: 0.7310 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1041 - accuracy: 0.7204\n",
      "Epoch 00032: val_accuracy did not improve from 0.73096\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.1054 - accuracy: 0.7199 - val_loss: 1.1129 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0790 - accuracy: 0.7274\n",
      "Epoch 00033: val_accuracy improved from 0.73096 to 0.74019, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.0799 - accuracy: 0.7271 - val_loss: 1.0818 - val_accuracy: 0.7402 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0666 - accuracy: 0.7306\n",
      "Epoch 00034: val_accuracy did not improve from 0.74019\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0698 - accuracy: 0.7294 - val_loss: 1.0895 - val_accuracy: 0.7292 - lr: 0.0010\n",
      "Epoch 35/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0811 - accuracy: 0.7257\n",
      "Epoch 00035: val_accuracy did not improve from 0.74019\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0811 - accuracy: 0.7257 - val_loss: 1.1381 - val_accuracy: 0.7240 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0587 - accuracy: 0.7327\n",
      "Epoch 00036: val_accuracy did not improve from 0.74019\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0595 - accuracy: 0.7322 - val_loss: 1.0894 - val_accuracy: 0.7333 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0293 - accuracy: 0.7424\n",
      "Epoch 00037: val_accuracy improved from 0.74019 to 0.74199, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.0318 - accuracy: 0.7415 - val_loss: 1.0792 - val_accuracy: 0.7420 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0354 - accuracy: 0.7419\n",
      "Epoch 00038: val_accuracy did not improve from 0.74199\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.0361 - accuracy: 0.7418 - val_loss: 1.0792 - val_accuracy: 0.7397 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0277 - accuracy: 0.7431\n",
      "Epoch 00039: val_accuracy did not improve from 0.74199\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.0287 - accuracy: 0.7430 - val_loss: 1.0716 - val_accuracy: 0.7415 - lr: 0.0010\n",
      "Epoch 40/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9933 - accuracy: 0.7527\n",
      "Epoch 00040: val_accuracy improved from 0.74199 to 0.74711, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.9933 - accuracy: 0.7527 - val_loss: 1.0567 - val_accuracy: 0.7471 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9754 - accuracy: 0.7621\n",
      "Epoch 00041: val_accuracy improved from 0.74711 to 0.74942, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.9776 - accuracy: 0.7612 - val_loss: 1.0412 - val_accuracy: 0.7494 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9520 - accuracy: 0.7640\n",
      "Epoch 00042: val_accuracy did not improve from 0.74942\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.9499 - accuracy: 0.7647 - val_loss: 1.0394 - val_accuracy: 0.7487 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9425 - accuracy: 0.7684\n",
      "Epoch 00043: val_accuracy did not improve from 0.74942\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.9432 - accuracy: 0.7679 - val_loss: 1.0333 - val_accuracy: 0.7492 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9512 - accuracy: 0.7613\n",
      "Epoch 00044: val_accuracy improved from 0.74942 to 0.75019, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 0.9521 - accuracy: 0.7609 - val_loss: 1.0243 - val_accuracy: 0.7502 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9214 - accuracy: 0.7760\n",
      "Epoch 00045: val_accuracy improved from 0.75019 to 0.75096, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 0.9214 - accuracy: 0.7760 - val_loss: 1.0246 - val_accuracy: 0.7510 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9194 - accuracy: 0.7758\n",
      "Epoch 00046: val_accuracy improved from 0.75096 to 0.75353, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 111ms/step - loss: 0.9194 - accuracy: 0.7758 - val_loss: 1.0214 - val_accuracy: 0.7535 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9189 - accuracy: 0.7740\n",
      "Epoch 00047: val_accuracy improved from 0.75353 to 0.75814, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 0.9181 - accuracy: 0.7745 - val_loss: 1.0155 - val_accuracy: 0.7581 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9113 - accuracy: 0.7779\n",
      "Epoch 00048: val_accuracy improved from 0.75814 to 0.75994, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.9126 - accuracy: 0.7772 - val_loss: 1.0114 - val_accuracy: 0.7599 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8977 - accuracy: 0.7770\n",
      "Epoch 00049: val_accuracy did not improve from 0.75994\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9023 - accuracy: 0.7753 - val_loss: 1.0137 - val_accuracy: 0.7558 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8965 - accuracy: 0.7823\n",
      "Epoch 00050: val_accuracy did not improve from 0.75994\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8980 - accuracy: 0.7815 - val_loss: 1.0156 - val_accuracy: 0.7556 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8957 - accuracy: 0.7806\n",
      "Epoch 00051: val_accuracy did not improve from 0.75994\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8989 - accuracy: 0.7799 - val_loss: 1.0240 - val_accuracy: 0.7451 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9058 - accuracy: 0.7757\n",
      "Epoch 00052: val_accuracy did not improve from 0.75994\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.9072 - accuracy: 0.7757 - val_loss: 1.0044 - val_accuracy: 0.7551 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8771 - accuracy: 0.7823\n",
      "Epoch 00053: val_accuracy did not improve from 0.75994\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8809 - accuracy: 0.7811 - val_loss: 1.0045 - val_accuracy: 0.7553 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8774 - accuracy: 0.7854\n",
      "Epoch 00054: val_accuracy did not improve from 0.75994\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8778 - accuracy: 0.7848 - val_loss: 1.0083 - val_accuracy: 0.7584 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8808 - accuracy: 0.7841\n",
      "Epoch 00055: val_accuracy improved from 0.75994 to 0.76302, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 0.8826 - accuracy: 0.7837 - val_loss: 0.9941 - val_accuracy: 0.7630 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8753 - accuracy: 0.7849\n",
      "Epoch 00056: val_accuracy did not improve from 0.76302\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8753 - accuracy: 0.7849 - val_loss: 1.0014 - val_accuracy: 0.7551 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8625 - accuracy: 0.7873\n",
      "Epoch 00057: val_accuracy did not improve from 0.76302\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8630 - accuracy: 0.7871 - val_loss: 0.9984 - val_accuracy: 0.7574 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8733 - accuracy: 0.7827\n",
      "Epoch 00058: val_accuracy did not improve from 0.76302\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8739 - accuracy: 0.7822 - val_loss: 1.0010 - val_accuracy: 0.7569 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8672 - accuracy: 0.7833\n",
      "Epoch 00059: val_accuracy did not improve from 0.76302\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8710 - accuracy: 0.7822 - val_loss: 0.9921 - val_accuracy: 0.7589 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8707 - accuracy: 0.7889\n",
      "Epoch 00060: val_accuracy did not improve from 0.76302\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.8713 - accuracy: 0.7890 - val_loss: 0.9914 - val_accuracy: 0.7592 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8579 - accuracy: 0.7888\n",
      "Epoch 00061: val_accuracy did not improve from 0.76302\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8587 - accuracy: 0.7884 - val_loss: 0.9921 - val_accuracy: 0.7574 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8636 - accuracy: 0.7896\n",
      "Epoch 00062: val_accuracy did not improve from 0.76302\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8651 - accuracy: 0.7887 - val_loss: 0.9925 - val_accuracy: 0.7615 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8458 - accuracy: 0.7960\n",
      "Epoch 00063: val_accuracy did not improve from 0.76302\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.8495 - accuracy: 0.7946 - val_loss: 0.9897 - val_accuracy: 0.7587 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8462 - accuracy: 0.7951\n",
      "Epoch 00064: val_accuracy did not improve from 0.76302\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.8481 - accuracy: 0.7944 - val_loss: 0.9874 - val_accuracy: 0.7566 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8508 - accuracy: 0.7956\n",
      "Epoch 00065: val_accuracy did not improve from 0.76302\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.8508 - accuracy: 0.7956 - val_loss: 0.9845 - val_accuracy: 0.7581 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8414 - accuracy: 0.7939\n",
      "Epoch 00066: val_accuracy improved from 0.76302 to 0.76327, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.8427 - accuracy: 0.7934 - val_loss: 0.9905 - val_accuracy: 0.7633 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8405 - accuracy: 0.7976\n",
      "Epoch 00067: val_accuracy did not improve from 0.76327\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8423 - accuracy: 0.7972 - val_loss: 0.9904 - val_accuracy: 0.7610 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8405 - accuracy: 0.7951\n",
      "Epoch 00068: val_accuracy did not improve from 0.76327\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8393 - accuracy: 0.7955 - val_loss: 0.9872 - val_accuracy: 0.7517 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8483 - accuracy: 0.7910\n",
      "Epoch 00069: val_accuracy did not improve from 0.76327\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 0.8483 - accuracy: 0.7910 - val_loss: 0.9831 - val_accuracy: 0.7571 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8302 - accuracy: 0.8015\n",
      "Epoch 00070: val_accuracy did not improve from 0.76327\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.8302 - accuracy: 0.8015 - val_loss: 0.9794 - val_accuracy: 0.7622 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8399 - accuracy: 0.7928\n",
      "Epoch 00071: val_accuracy did not improve from 0.76327\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8389 - accuracy: 0.7934 - val_loss: 0.9828 - val_accuracy: 0.7602 - lr: 1.0000e-04\n",
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8355 - accuracy: 0.7921\n",
      "Epoch 00072: val_accuracy did not improve from 0.76327\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8367 - accuracy: 0.7916 - val_loss: 0.9818 - val_accuracy: 0.7633 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8324 - accuracy: 0.7981\n",
      "Epoch 00073: val_accuracy did not improve from 0.76327\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.8324 - accuracy: 0.7978 - val_loss: 0.9748 - val_accuracy: 0.7610 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8324 - accuracy: 0.7950\n",
      "Epoch 00074: val_accuracy did not improve from 0.76327\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.8357 - accuracy: 0.7943 - val_loss: 0.9707 - val_accuracy: 0.7622 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8249 - accuracy: 0.7987\n",
      "Epoch 00075: val_accuracy improved from 0.76327 to 0.76686, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 0.8239 - accuracy: 0.7991 - val_loss: 0.9698 - val_accuracy: 0.7669 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8214 - accuracy: 0.8018\n",
      "Epoch 00076: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.8225 - accuracy: 0.8005 - val_loss: 0.9687 - val_accuracy: 0.7656 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8276 - accuracy: 0.7975\n",
      "Epoch 00077: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8276 - accuracy: 0.7975 - val_loss: 0.9756 - val_accuracy: 0.7617 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8096 - accuracy: 0.8039\n",
      "Epoch 00078: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8118 - accuracy: 0.8032 - val_loss: 0.9704 - val_accuracy: 0.7615 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8279 - accuracy: 0.7953\n",
      "Epoch 00079: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8310 - accuracy: 0.7945 - val_loss: 0.9754 - val_accuracy: 0.7584 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8217 - accuracy: 0.7990\n",
      "Epoch 00080: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8220 - accuracy: 0.7991 - val_loss: 0.9765 - val_accuracy: 0.7607 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8125 - accuracy: 0.8013\n",
      "Epoch 00081: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8108 - accuracy: 0.8017 - val_loss: 0.9805 - val_accuracy: 0.7617 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8084 - accuracy: 0.8058\n",
      "Epoch 00082: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8104 - accuracy: 0.8051 - val_loss: 0.9752 - val_accuracy: 0.7620 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8116 - accuracy: 0.8021\n",
      "Epoch 00083: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8137 - accuracy: 0.8012 - val_loss: 0.9748 - val_accuracy: 0.7607 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8115 - accuracy: 0.8014\n",
      "Epoch 00084: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8140 - accuracy: 0.8013 - val_loss: 0.9724 - val_accuracy: 0.7628 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8093 - accuracy: 0.8018\n",
      "Epoch 00085: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8102 - accuracy: 0.8015 - val_loss: 0.9725 - val_accuracy: 0.7625 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8158 - accuracy: 0.7987\n",
      "Epoch 00086: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8155 - accuracy: 0.7990 - val_loss: 0.9722 - val_accuracy: 0.7622 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8119 - accuracy: 0.8001\n",
      "Epoch 00087: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8119 - accuracy: 0.8001 - val_loss: 0.9743 - val_accuracy: 0.7569 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8100 - accuracy: 0.8058\n",
      "Epoch 00088: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8143 - accuracy: 0.8046 - val_loss: 0.9702 - val_accuracy: 0.7617 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8080 - accuracy: 0.8040\n",
      "Epoch 00089: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8084 - accuracy: 0.8040 - val_loss: 0.9685 - val_accuracy: 0.7628 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8095 - accuracy: 0.8043\n",
      "Epoch 00090: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8114 - accuracy: 0.8039 - val_loss: 0.9687 - val_accuracy: 0.7617 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8250 - accuracy: 0.7983\n",
      "Epoch 00091: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8260 - accuracy: 0.7979 - val_loss: 0.9712 - val_accuracy: 0.7625 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7993 - accuracy: 0.8050\n",
      "Epoch 00092: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8009 - accuracy: 0.8044 - val_loss: 0.9739 - val_accuracy: 0.7597 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8107 - accuracy: 0.8014\n",
      "Epoch 00093: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8151 - accuracy: 0.8010 - val_loss: 0.9727 - val_accuracy: 0.7610 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7962 - accuracy: 0.8066\n",
      "Epoch 00094: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.7962 - accuracy: 0.8066 - val_loss: 0.9656 - val_accuracy: 0.7622 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8067 - accuracy: 0.8042\n",
      "Epoch 00095: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8072 - accuracy: 0.8039 - val_loss: 0.9655 - val_accuracy: 0.7633 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8150 - accuracy: 0.8033\n",
      "Epoch 00096: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8175 - accuracy: 0.8026 - val_loss: 0.9657 - val_accuracy: 0.7597 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8089 - accuracy: 0.8026\n",
      "Epoch 00097: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8104 - accuracy: 0.8017 - val_loss: 0.9679 - val_accuracy: 0.7605 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8058 - accuracy: 0.8062\n",
      "Epoch 00098: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8068 - accuracy: 0.8060 - val_loss: 0.9738 - val_accuracy: 0.7615 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8088 - accuracy: 0.8039\n",
      "Epoch 00099: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8102 - accuracy: 0.8034 - val_loss: 0.9711 - val_accuracy: 0.7630 - lr: 1.0000e-05\n",
      "Epoch 100/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 0.8071 - accuracy: 0.8015\n",
      "Epoch 00100: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8071 - accuracy: 0.8015 - val_loss: 0.9694 - val_accuracy: 0.7602 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8096 - accuracy: 0.8038\n",
      "Epoch 00101: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8096 - accuracy: 0.8038 - val_loss: 0.9709 - val_accuracy: 0.7633 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7993 - accuracy: 0.8073\n",
      "Epoch 00102: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7993 - accuracy: 0.8073 - val_loss: 0.9676 - val_accuracy: 0.7622 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8071 - accuracy: 0.8022\n",
      "Epoch 00103: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8076 - accuracy: 0.8017 - val_loss: 0.9723 - val_accuracy: 0.7628 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8100 - accuracy: 0.8040\n",
      "Epoch 00104: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8117 - accuracy: 0.8035 - val_loss: 0.9709 - val_accuracy: 0.7638 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8135 - accuracy: 0.8000\n",
      "Epoch 00105: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8135 - accuracy: 0.8000 - val_loss: 0.9666 - val_accuracy: 0.7638 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7990 - accuracy: 0.8092\n",
      "Epoch 00106: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.8026 - accuracy: 0.8080 - val_loss: 0.9602 - val_accuracy: 0.7625 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8103 - accuracy: 0.8095\n",
      "Epoch 00107: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8103 - accuracy: 0.8095 - val_loss: 0.9660 - val_accuracy: 0.7610 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7947 - accuracy: 0.8046\n",
      "Epoch 00108: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7960 - accuracy: 0.8043 - val_loss: 0.9651 - val_accuracy: 0.7610 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8077 - accuracy: 0.8018\n",
      "Epoch 00109: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8120 - accuracy: 0.8010 - val_loss: 0.9704 - val_accuracy: 0.7599 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8137 - accuracy: 0.8009\n",
      "Epoch 00110: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8160 - accuracy: 0.8001 - val_loss: 0.9654 - val_accuracy: 0.7620 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7976 - accuracy: 0.8039\n",
      "Epoch 00111: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7999 - accuracy: 0.8033 - val_loss: 0.9686 - val_accuracy: 0.7605 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8044 - accuracy: 0.8044\n",
      "Epoch 00112: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8044 - accuracy: 0.8044 - val_loss: 0.9672 - val_accuracy: 0.7620 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7925 - accuracy: 0.8081\n",
      "Epoch 00113: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7946 - accuracy: 0.8075 - val_loss: 0.9665 - val_accuracy: 0.7615 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7906 - accuracy: 0.8143\n",
      "Epoch 00114: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7911 - accuracy: 0.8140 - val_loss: 0.9665 - val_accuracy: 0.7605 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8045 - accuracy: 0.8036\n",
      "Epoch 00115: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8062 - accuracy: 0.8033 - val_loss: 0.9664 - val_accuracy: 0.7615 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8030 - accuracy: 0.8035\n",
      "Epoch 00116: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8030 - accuracy: 0.8035 - val_loss: 0.9681 - val_accuracy: 0.7594 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7990 - accuracy: 0.8053\n",
      "Epoch 00117: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7990 - accuracy: 0.8053 - val_loss: 0.9641 - val_accuracy: 0.7610 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7995 - accuracy: 0.8076\n",
      "Epoch 00118: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8000 - accuracy: 0.8070 - val_loss: 0.9686 - val_accuracy: 0.7620 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8008 - accuracy: 0.8042\n",
      "Epoch 00119: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8023 - accuracy: 0.8038 - val_loss: 0.9668 - val_accuracy: 0.7620 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7901 - accuracy: 0.8079\n",
      "Epoch 00120: val_accuracy did not improve from 0.76686\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.7906 - accuracy: 0.8073 - val_loss: 0.9631 - val_accuracy: 0.7635 - lr: 1.0000e-06\n",
      "epoch_number 75\n",
      "train accuracy and validation accuracy 0.799098789691925 0.7668632864952087\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.9698 - accuracy: 0.7669\n",
      "test_accuracy 0.7668632864952087\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087]\n",
      "0.7556949257850647\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S12_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S12_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 5.0080 - accuracy: 0.0896\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.19826, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 385ms/step - loss: 4.9969 - accuracy: 0.0897 - val_loss: 3.9434 - val_accuracy: 0.1983 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.3054 - accuracy: 0.2133\n",
      "Epoch 00002: val_accuracy improved from 0.19826 to 0.30033, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 3.3058 - accuracy: 0.2142 - val_loss: 2.7863 - val_accuracy: 0.3003 - lr: 0.0010\n",
      "Epoch 3/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.8374 - accuracy: 0.2920\n",
      "Epoch 00003: val_accuracy improved from 0.30033 to 0.39318, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 2.8374 - accuracy: 0.2920 - val_loss: 2.3878 - val_accuracy: 0.3932 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.5500 - accuracy: 0.3571\n",
      "Epoch 00004: val_accuracy improved from 0.39318 to 0.45012, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 2.5503 - accuracy: 0.3570 - val_loss: 2.1610 - val_accuracy: 0.4501 - lr: 0.0010\n",
      "Epoch 5/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.3421 - accuracy: 0.4049\n",
      "Epoch 00005: val_accuracy improved from 0.45012 to 0.48012, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 2.3421 - accuracy: 0.4049 - val_loss: 2.0360 - val_accuracy: 0.4801 - lr: 0.0010\n",
      "Epoch 6/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.1924 - accuracy: 0.4377\n",
      "Epoch 00006: val_accuracy improved from 0.48012 to 0.52142, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 2.1924 - accuracy: 0.4377 - val_loss: 1.8817 - val_accuracy: 0.5214 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0403 - accuracy: 0.4741\n",
      "Epoch 00007: val_accuracy improved from 0.52142 to 0.55860, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 2.0377 - accuracy: 0.4750 - val_loss: 1.7512 - val_accuracy: 0.5586 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9020 - accuracy: 0.5077\n",
      "Epoch 00008: val_accuracy improved from 0.55860 to 0.57451, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.8961 - accuracy: 0.5091 - val_loss: 1.6781 - val_accuracy: 0.5745 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8152 - accuracy: 0.5269\n",
      "Epoch 00009: val_accuracy improved from 0.57451 to 0.59913, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.8138 - accuracy: 0.5275 - val_loss: 1.5716 - val_accuracy: 0.5991 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7331 - accuracy: 0.5541\n",
      "Epoch 00010: val_accuracy improved from 0.59913 to 0.61708, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.7318 - accuracy: 0.5546 - val_loss: 1.5253 - val_accuracy: 0.6171 - lr: 0.0010\n",
      "Epoch 11/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.6653 - accuracy: 0.5681\n",
      "Epoch 00011: val_accuracy improved from 0.61708 to 0.63811, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 1.6653 - accuracy: 0.5681 - val_loss: 1.4553 - val_accuracy: 0.6381 - lr: 0.0010\n",
      "Epoch 12/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.6085 - accuracy: 0.5875\n",
      "Epoch 00012: val_accuracy improved from 0.63811 to 0.65017, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 1.6085 - accuracy: 0.5875 - val_loss: 1.4083 - val_accuracy: 0.6502 - lr: 0.0010\n",
      "Epoch 13/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.5438 - accuracy: 0.6038\n",
      "Epoch 00013: val_accuracy improved from 0.65017 to 0.65453, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.5438 - accuracy: 0.6038 - val_loss: 1.3632 - val_accuracy: 0.6545 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4917 - accuracy: 0.6150\n",
      "Epoch 00014: val_accuracy improved from 0.65453 to 0.66581, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 1.4943 - accuracy: 0.6148 - val_loss: 1.3314 - val_accuracy: 0.6658 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4738 - accuracy: 0.6203\n",
      "Epoch 00015: val_accuracy did not improve from 0.66581\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.4749 - accuracy: 0.6207 - val_loss: 1.3530 - val_accuracy: 0.6548 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4265 - accuracy: 0.6342\n",
      "Epoch 00016: val_accuracy improved from 0.66581 to 0.66889, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.4266 - accuracy: 0.6338 - val_loss: 1.3080 - val_accuracy: 0.6689 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3915 - accuracy: 0.6449\n",
      "Epoch 00017: val_accuracy improved from 0.66889 to 0.69146, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.3935 - accuracy: 0.6447 - val_loss: 1.2647 - val_accuracy: 0.6915 - lr: 0.0010\n",
      "Epoch 18/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3372 - accuracy: 0.6606\n",
      "Epoch 00018: val_accuracy did not improve from 0.69146\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1.3372 - accuracy: 0.6606 - val_loss: 1.2642 - val_accuracy: 0.6791 - lr: 0.0010\n",
      "Epoch 19/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3100 - accuracy: 0.6673\n",
      "Epoch 00019: val_accuracy improved from 0.69146 to 0.69890, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 1.3100 - accuracy: 0.6673 - val_loss: 1.2406 - val_accuracy: 0.6989 - lr: 0.0010\n",
      "Epoch 20/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3036 - accuracy: 0.6684\n",
      "Epoch 00020: val_accuracy did not improve from 0.69890\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1.3036 - accuracy: 0.6684 - val_loss: 1.2246 - val_accuracy: 0.6889 - lr: 0.0010\n",
      "Epoch 21/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2748 - accuracy: 0.6748\n",
      "Epoch 00021: val_accuracy did not improve from 0.69890\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1.2748 - accuracy: 0.6748 - val_loss: 1.1973 - val_accuracy: 0.6984 - lr: 0.0010\n",
      "Epoch 22/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2238 - accuracy: 0.6960\n",
      "Epoch 00022: val_accuracy improved from 0.69890 to 0.71557, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 107ms/step - loss: 1.2238 - accuracy: 0.6960 - val_loss: 1.1729 - val_accuracy: 0.7156 - lr: 0.0010\n",
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2018 - accuracy: 0.7000\n",
      "Epoch 00023: val_accuracy did not improve from 0.71557\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.2040 - accuracy: 0.6989 - val_loss: 1.1702 - val_accuracy: 0.7102 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1607 - accuracy: 0.7046\n",
      "Epoch 00024: val_accuracy improved from 0.71557 to 0.71659, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.1635 - accuracy: 0.7036 - val_loss: 1.1681 - val_accuracy: 0.7166 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1979 - accuracy: 0.6982\n",
      "Epoch 00025: val_accuracy did not improve from 0.71659\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.1999 - accuracy: 0.6972 - val_loss: 1.1480 - val_accuracy: 0.7117 - lr: 0.0010\n",
      "Epoch 26/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1449 - accuracy: 0.7139\n",
      "Epoch 00026: val_accuracy improved from 0.71659 to 0.72198, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.1449 - accuracy: 0.7139 - val_loss: 1.1343 - val_accuracy: 0.7220 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1177 - accuracy: 0.7261\n",
      "Epoch 00027: val_accuracy improved from 0.72198 to 0.73147, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.1200 - accuracy: 0.7246 - val_loss: 1.1118 - val_accuracy: 0.7315 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1100 - accuracy: 0.7274\n",
      "Epoch 00028: val_accuracy did not improve from 0.73147\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.1114 - accuracy: 0.7274 - val_loss: 1.1361 - val_accuracy: 0.7210 - lr: 0.0010\n",
      "Epoch 29/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1111 - accuracy: 0.7263\n",
      "Epoch 00029: val_accuracy did not improve from 0.73147\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.1111 - accuracy: 0.7263 - val_loss: 1.1388 - val_accuracy: 0.7227 - lr: 0.0010\n",
      "Epoch 30/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1053 - accuracy: 0.7222\n",
      "Epoch 00030: val_accuracy did not improve from 0.73147\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.1053 - accuracy: 0.7222 - val_loss: 1.1212 - val_accuracy: 0.7233 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0727 - accuracy: 0.7354\n",
      "Epoch 00031: val_accuracy did not improve from 0.73147\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 1.0754 - accuracy: 0.7352 - val_loss: 1.0999 - val_accuracy: 0.7279 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0672 - accuracy: 0.7322\n",
      "Epoch 00032: val_accuracy did not improve from 0.73147\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0673 - accuracy: 0.7322 - val_loss: 1.1064 - val_accuracy: 0.7245 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0346 - accuracy: 0.7451\n",
      "Epoch 00033: val_accuracy improved from 0.73147 to 0.73737, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.0345 - accuracy: 0.7455 - val_loss: 1.0731 - val_accuracy: 0.7374 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0084 - accuracy: 0.7546\n",
      "Epoch 00034: val_accuracy improved from 0.73737 to 0.73788, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.0095 - accuracy: 0.7536 - val_loss: 1.0625 - val_accuracy: 0.7379 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0074 - accuracy: 0.7560\n",
      "Epoch 00035: val_accuracy improved from 0.73788 to 0.73916, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.0068 - accuracy: 0.7556 - val_loss: 1.0873 - val_accuracy: 0.7392 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9960 - accuracy: 0.7514\n",
      "Epoch 00036: val_accuracy did not improve from 0.73916\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9966 - accuracy: 0.7515 - val_loss: 1.0829 - val_accuracy: 0.7292 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9701 - accuracy: 0.7642\n",
      "Epoch 00037: val_accuracy improved from 0.73916 to 0.74711, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.9776 - accuracy: 0.7621 - val_loss: 1.0337 - val_accuracy: 0.7471 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9678 - accuracy: 0.7640\n",
      "Epoch 00038: val_accuracy did not improve from 0.74711\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.9683 - accuracy: 0.7641 - val_loss: 1.0279 - val_accuracy: 0.7402 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9643 - accuracy: 0.7643\n",
      "Epoch 00039: val_accuracy did not improve from 0.74711\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9632 - accuracy: 0.7650 - val_loss: 1.0395 - val_accuracy: 0.7448 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9477 - accuracy: 0.7670\n",
      "Epoch 00040: val_accuracy improved from 0.74711 to 0.75276, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.9503 - accuracy: 0.7665 - val_loss: 1.0118 - val_accuracy: 0.7528 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9063 - accuracy: 0.7828\n",
      "Epoch 00041: val_accuracy improved from 0.75276 to 0.75404, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.9058 - accuracy: 0.7827 - val_loss: 1.0093 - val_accuracy: 0.7540 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8883 - accuracy: 0.7851\n",
      "Epoch 00042: val_accuracy improved from 0.75404 to 0.75968, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.8880 - accuracy: 0.7853 - val_loss: 0.9987 - val_accuracy: 0.7597 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8718 - accuracy: 0.7953\n",
      "Epoch 00043: val_accuracy did not improve from 0.75968\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.8718 - accuracy: 0.7953 - val_loss: 0.9920 - val_accuracy: 0.7566 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8558 - accuracy: 0.7951\n",
      "Epoch 00044: val_accuracy did not improve from 0.75968\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.8558 - accuracy: 0.7951 - val_loss: 0.9910 - val_accuracy: 0.7571 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8691 - accuracy: 0.7906\n",
      "Epoch 00045: val_accuracy did not improve from 0.75968\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 0.8691 - accuracy: 0.7906 - val_loss: 0.9870 - val_accuracy: 0.7597 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8451 - accuracy: 0.7974\n",
      "Epoch 00046: val_accuracy improved from 0.75968 to 0.76122, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 108ms/step - loss: 0.8479 - accuracy: 0.7969 - val_loss: 0.9836 - val_accuracy: 0.7612 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8567 - accuracy: 0.7986\n",
      "Epoch 00047: val_accuracy improved from 0.76122 to 0.76327, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.8561 - accuracy: 0.7984 - val_loss: 0.9798 - val_accuracy: 0.7633 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8421 - accuracy: 0.7989\n",
      "Epoch 00048: val_accuracy did not improve from 0.76327\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8418 - accuracy: 0.7987 - val_loss: 0.9789 - val_accuracy: 0.7625 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8389 - accuracy: 0.8037\n",
      "Epoch 00049: val_accuracy improved from 0.76327 to 0.76712, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.8406 - accuracy: 0.8033 - val_loss: 0.9739 - val_accuracy: 0.7671 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8335 - accuracy: 0.8081\n",
      "Epoch 00050: val_accuracy did not improve from 0.76712\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8338 - accuracy: 0.8077 - val_loss: 0.9778 - val_accuracy: 0.7643 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8242 - accuracy: 0.8050\n",
      "Epoch 00051: val_accuracy did not improve from 0.76712\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8242 - accuracy: 0.8050 - val_loss: 0.9747 - val_accuracy: 0.7622 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8384 - accuracy: 0.8031\n",
      "Epoch 00052: val_accuracy did not improve from 0.76712\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8384 - accuracy: 0.8031 - val_loss: 0.9730 - val_accuracy: 0.7638 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8193 - accuracy: 0.8072\n",
      "Epoch 00053: val_accuracy did not improve from 0.76712\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8189 - accuracy: 0.8076 - val_loss: 0.9671 - val_accuracy: 0.7651 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8113 - accuracy: 0.8088\n",
      "Epoch 00054: val_accuracy did not improve from 0.76712\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.8113 - accuracy: 0.8088 - val_loss: 0.9666 - val_accuracy: 0.7651 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8171 - accuracy: 0.8038\n",
      "Epoch 00055: val_accuracy did not improve from 0.76712\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8199 - accuracy: 0.8033 - val_loss: 0.9662 - val_accuracy: 0.7658 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8165 - accuracy: 0.8021\n",
      "Epoch 00056: val_accuracy did not improve from 0.76712\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.8165 - accuracy: 0.8021 - val_loss: 0.9705 - val_accuracy: 0.7640 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8074 - accuracy: 0.8067\n",
      "Epoch 00057: val_accuracy did not improve from 0.76712\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.8074 - accuracy: 0.8067 - val_loss: 0.9696 - val_accuracy: 0.7648 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8158 - accuracy: 0.8110\n",
      "Epoch 00058: val_accuracy did not improve from 0.76712\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.8158 - accuracy: 0.8110 - val_loss: 0.9707 - val_accuracy: 0.7630 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7993 - accuracy: 0.8158\n",
      "Epoch 00059: val_accuracy did not improve from 0.76712\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 0.8004 - accuracy: 0.8159 - val_loss: 0.9641 - val_accuracy: 0.7651 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8000 - accuracy: 0.8139\n",
      "Epoch 00060: val_accuracy did not improve from 0.76712\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8000 - accuracy: 0.8139 - val_loss: 0.9668 - val_accuracy: 0.7638 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7952 - accuracy: 0.8110\n",
      "Epoch 00061: val_accuracy did not improve from 0.76712\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.7952 - accuracy: 0.8110 - val_loss: 0.9636 - val_accuracy: 0.7630 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7896 - accuracy: 0.8210\n",
      "Epoch 00062: val_accuracy did not improve from 0.76712\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 0.7928 - accuracy: 0.8201 - val_loss: 0.9603 - val_accuracy: 0.7628 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7876 - accuracy: 0.8173\n",
      "Epoch 00063: val_accuracy did not improve from 0.76712\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.7874 - accuracy: 0.8173 - val_loss: 0.9589 - val_accuracy: 0.7648 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7931 - accuracy: 0.8144\n",
      "Epoch 00064: val_accuracy did not improve from 0.76712\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7960 - accuracy: 0.8136 - val_loss: 0.9593 - val_accuracy: 0.7664 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7926 - accuracy: 0.8176\n",
      "Epoch 00065: val_accuracy improved from 0.76712 to 0.76917, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 0.7926 - accuracy: 0.8176 - val_loss: 0.9617 - val_accuracy: 0.7692 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7819 - accuracy: 0.8126\n",
      "Epoch 00066: val_accuracy did not improve from 0.76917\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.7819 - accuracy: 0.8126 - val_loss: 0.9610 - val_accuracy: 0.7638 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7818 - accuracy: 0.8177\n",
      "Epoch 00067: val_accuracy did not improve from 0.76917\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7831 - accuracy: 0.8173 - val_loss: 0.9639 - val_accuracy: 0.7638 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7721 - accuracy: 0.8218\n",
      "Epoch 00068: val_accuracy did not improve from 0.76917\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7721 - accuracy: 0.8218 - val_loss: 0.9592 - val_accuracy: 0.7681 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7723 - accuracy: 0.8201\n",
      "Epoch 00069: val_accuracy did not improve from 0.76917\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.7707 - accuracy: 0.8205 - val_loss: 0.9588 - val_accuracy: 0.7648 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7721 - accuracy: 0.8213\n",
      "Epoch 00070: val_accuracy did not improve from 0.76917\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.7721 - accuracy: 0.8213 - val_loss: 0.9618 - val_accuracy: 0.7640 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7840 - accuracy: 0.8122\n",
      "Epoch 00071: val_accuracy did not improve from 0.76917\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.7834 - accuracy: 0.8121 - val_loss: 0.9591 - val_accuracy: 0.7648 - lr: 1.0000e-04\n",
      "Epoch 72/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7649 - accuracy: 0.8198\n",
      "Epoch 00072: val_accuracy did not improve from 0.76917\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.7649 - accuracy: 0.8198 - val_loss: 0.9539 - val_accuracy: 0.7692 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7791 - accuracy: 0.8173\n",
      "Epoch 00073: val_accuracy improved from 0.76917 to 0.76943, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 0.7791 - accuracy: 0.8173 - val_loss: 0.9527 - val_accuracy: 0.7694 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7631 - accuracy: 0.8207\n",
      "Epoch 00074: val_accuracy did not improve from 0.76943\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7624 - accuracy: 0.8210 - val_loss: 0.9541 - val_accuracy: 0.7674 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7603 - accuracy: 0.8229\n",
      "Epoch 00075: val_accuracy did not improve from 0.76943\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7649 - accuracy: 0.8221 - val_loss: 0.9580 - val_accuracy: 0.7643 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7619 - accuracy: 0.8230\n",
      "Epoch 00076: val_accuracy did not improve from 0.76943\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7631 - accuracy: 0.8226 - val_loss: 0.9559 - val_accuracy: 0.7674 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7678 - accuracy: 0.8228\n",
      "Epoch 00077: val_accuracy improved from 0.76943 to 0.77199, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.7678 - accuracy: 0.8228 - val_loss: 0.9474 - val_accuracy: 0.7720 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7517 - accuracy: 0.8241\n",
      "Epoch 00078: val_accuracy did not improve from 0.77199\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.7554 - accuracy: 0.8225 - val_loss: 0.9584 - val_accuracy: 0.7679 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7568 - accuracy: 0.8270\n",
      "Epoch 00079: val_accuracy did not improve from 0.77199\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.7568 - accuracy: 0.8270 - val_loss: 0.9541 - val_accuracy: 0.7681 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7407 - accuracy: 0.8251\n",
      "Epoch 00080: val_accuracy did not improve from 0.77199\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7401 - accuracy: 0.8255 - val_loss: 0.9532 - val_accuracy: 0.7692 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7519 - accuracy: 0.8248\n",
      "Epoch 00081: val_accuracy did not improve from 0.77199\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7527 - accuracy: 0.8247 - val_loss: 0.9520 - val_accuracy: 0.7717 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7493 - accuracy: 0.8289\n",
      "Epoch 00082: val_accuracy improved from 0.77199 to 0.77225, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.7502 - accuracy: 0.8284 - val_loss: 0.9475 - val_accuracy: 0.7722 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7588 - accuracy: 0.8233\n",
      "Epoch 00083: val_accuracy did not improve from 0.77225\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.7586 - accuracy: 0.8234 - val_loss: 0.9459 - val_accuracy: 0.7707 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7488 - accuracy: 0.8259\n",
      "Epoch 00084: val_accuracy did not improve from 0.77225\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7488 - accuracy: 0.8259 - val_loss: 0.9484 - val_accuracy: 0.7702 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7468 - accuracy: 0.8278\n",
      "Epoch 00085: val_accuracy did not improve from 0.77225\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7477 - accuracy: 0.8272 - val_loss: 0.9476 - val_accuracy: 0.7712 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7546 - accuracy: 0.8228\n",
      "Epoch 00086: val_accuracy did not improve from 0.77225\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7579 - accuracy: 0.8222 - val_loss: 0.9552 - val_accuracy: 0.7692 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7462 - accuracy: 0.8257\n",
      "Epoch 00087: val_accuracy did not improve from 0.77225\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7469 - accuracy: 0.8253 - val_loss: 0.9540 - val_accuracy: 0.7697 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7457 - accuracy: 0.8241\n",
      "Epoch 00088: val_accuracy did not improve from 0.77225\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7472 - accuracy: 0.8237 - val_loss: 0.9528 - val_accuracy: 0.7679 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7460 - accuracy: 0.8256\n",
      "Epoch 00089: val_accuracy did not improve from 0.77225\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7480 - accuracy: 0.8248 - val_loss: 0.9535 - val_accuracy: 0.7694 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7312 - accuracy: 0.8337\n",
      "Epoch 00090: val_accuracy did not improve from 0.77225\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7352 - accuracy: 0.8326 - val_loss: 0.9478 - val_accuracy: 0.7702 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7519 - accuracy: 0.8255\n",
      "Epoch 00091: val_accuracy did not improve from 0.77225\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7519 - accuracy: 0.8255 - val_loss: 0.9463 - val_accuracy: 0.7717 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7427 - accuracy: 0.8311\n",
      "Epoch 00092: val_accuracy improved from 0.77225 to 0.77251, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.7436 - accuracy: 0.8305 - val_loss: 0.9441 - val_accuracy: 0.7725 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7418 - accuracy: 0.8314\n",
      "Epoch 00093: val_accuracy did not improve from 0.77251\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7418 - accuracy: 0.8314 - val_loss: 0.9513 - val_accuracy: 0.7705 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7365 - accuracy: 0.8288\n",
      "Epoch 00094: val_accuracy did not improve from 0.77251\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7358 - accuracy: 0.8290 - val_loss: 0.9469 - val_accuracy: 0.7715 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7368 - accuracy: 0.8271\n",
      "Epoch 00095: val_accuracy improved from 0.77251 to 0.77328, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 0.7384 - accuracy: 0.8269 - val_loss: 0.9416 - val_accuracy: 0.7733 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7311 - accuracy: 0.8301\n",
      "Epoch 00096: val_accuracy improved from 0.77328 to 0.77456, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.7343 - accuracy: 0.8291 - val_loss: 0.9412 - val_accuracy: 0.7746 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7431 - accuracy: 0.8271\n",
      "Epoch 00097: val_accuracy did not improve from 0.77456\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7446 - accuracy: 0.8264 - val_loss: 0.9449 - val_accuracy: 0.7717 - lr: 1.0000e-05\n",
      "Epoch 98/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7462 - accuracy: 0.8283\n",
      "Epoch 00098: val_accuracy did not improve from 0.77456\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7473 - accuracy: 0.8279 - val_loss: 0.9456 - val_accuracy: 0.7715 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7369 - accuracy: 0.8306\n",
      "Epoch 00099: val_accuracy did not improve from 0.77456\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7363 - accuracy: 0.8302 - val_loss: 0.9453 - val_accuracy: 0.7722 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7378 - accuracy: 0.8282\n",
      "Epoch 00100: val_accuracy did not improve from 0.77456\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7378 - accuracy: 0.8282 - val_loss: 0.9430 - val_accuracy: 0.7707 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7349 - accuracy: 0.8299\n",
      "Epoch 00101: val_accuracy improved from 0.77456 to 0.77481, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 0.7349 - accuracy: 0.8299 - val_loss: 0.9400 - val_accuracy: 0.7748 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7556 - accuracy: 0.8198\n",
      "Epoch 00102: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7556 - accuracy: 0.8198 - val_loss: 0.9441 - val_accuracy: 0.7720 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7441 - accuracy: 0.8223\n",
      "Epoch 00103: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7449 - accuracy: 0.8222 - val_loss: 0.9420 - val_accuracy: 0.7735 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7449 - accuracy: 0.8273\n",
      "Epoch 00104: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7491 - accuracy: 0.8259 - val_loss: 0.9450 - val_accuracy: 0.7717 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7319 - accuracy: 0.8318\n",
      "Epoch 00105: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.7312 - accuracy: 0.8320 - val_loss: 0.9435 - val_accuracy: 0.7720 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7306 - accuracy: 0.8333\n",
      "Epoch 00106: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7318 - accuracy: 0.8328 - val_loss: 0.9432 - val_accuracy: 0.7705 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7370 - accuracy: 0.8293\n",
      "Epoch 00107: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.7378 - accuracy: 0.8289 - val_loss: 0.9442 - val_accuracy: 0.7715 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7458 - accuracy: 0.8271\n",
      "Epoch 00108: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7458 - accuracy: 0.8271 - val_loss: 0.9450 - val_accuracy: 0.7712 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7390 - accuracy: 0.8256\n",
      "Epoch 00109: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7425 - accuracy: 0.8242 - val_loss: 0.9440 - val_accuracy: 0.7743 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7438 - accuracy: 0.8261\n",
      "Epoch 00110: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7438 - accuracy: 0.8261 - val_loss: 0.9411 - val_accuracy: 0.7735 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7364 - accuracy: 0.8277\n",
      "Epoch 00111: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.7369 - accuracy: 0.8282 - val_loss: 0.9451 - val_accuracy: 0.7722 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7362 - accuracy: 0.8278\n",
      "Epoch 00112: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7374 - accuracy: 0.8273 - val_loss: 0.9456 - val_accuracy: 0.7707 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7499 - accuracy: 0.8248\n",
      "Epoch 00113: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7503 - accuracy: 0.8243 - val_loss: 0.9455 - val_accuracy: 0.7720 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7301 - accuracy: 0.8339\n",
      "Epoch 00114: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7301 - accuracy: 0.8339 - val_loss: 0.9430 - val_accuracy: 0.7743 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7316 - accuracy: 0.8330\n",
      "Epoch 00115: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7327 - accuracy: 0.8321 - val_loss: 0.9454 - val_accuracy: 0.7722 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7470 - accuracy: 0.8278\n",
      "Epoch 00116: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7480 - accuracy: 0.8275 - val_loss: 0.9448 - val_accuracy: 0.7725 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7432 - accuracy: 0.8270\n",
      "Epoch 00117: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7442 - accuracy: 0.8267 - val_loss: 0.9425 - val_accuracy: 0.7730 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7521 - accuracy: 0.8222\n",
      "Epoch 00118: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7522 - accuracy: 0.8226 - val_loss: 0.9415 - val_accuracy: 0.7728 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7184 - accuracy: 0.8372\n",
      "Epoch 00119: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.7187 - accuracy: 0.8370 - val_loss: 0.9445 - val_accuracy: 0.7712 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7291 - accuracy: 0.8301\n",
      "Epoch 00120: val_accuracy did not improve from 0.77481\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7320 - accuracy: 0.8295 - val_loss: 0.9402 - val_accuracy: 0.7717 - lr: 1.0000e-06\n",
      "epoch_number 101\n",
      "train accuracy and validation accuracy 0.829871416091919 0.7748140692710876\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.9400 - accuracy: 0.7748\n",
      "test_accuracy 0.7748140692710876\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087, 0.7748140692710876]\n",
      "0.7572881877422333\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S13_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S13_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 5.1417 - accuracy: 0.0684\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.15029, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 397ms/step - loss: 5.1272 - accuracy: 0.0686 - val_loss: 4.0504 - val_accuracy: 0.1503 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.5696 - accuracy: 0.1496\n",
      "Epoch 00002: val_accuracy improved from 0.15029 to 0.20826, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 3.5771 - accuracy: 0.1495 - val_loss: 3.2420 - val_accuracy: 0.2083 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.1295 - accuracy: 0.2272\n",
      "Epoch 00003: val_accuracy improved from 0.20826 to 0.28033, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 3.1330 - accuracy: 0.2268 - val_loss: 2.9037 - val_accuracy: 0.2803 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.8274 - accuracy: 0.2909\n",
      "Epoch 00004: val_accuracy improved from 0.28033 to 0.35727, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 2.8213 - accuracy: 0.2921 - val_loss: 2.6205 - val_accuracy: 0.3573 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.5638 - accuracy: 0.3497\n",
      "Epoch 00005: val_accuracy improved from 0.35727 to 0.41883, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 2.5642 - accuracy: 0.3494 - val_loss: 2.3706 - val_accuracy: 0.4188 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.3532 - accuracy: 0.4051\n",
      "Epoch 00006: val_accuracy improved from 0.41883 to 0.46422, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 2.3508 - accuracy: 0.4054 - val_loss: 2.1609 - val_accuracy: 0.4642 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2137 - accuracy: 0.4364\n",
      "Epoch 00007: val_accuracy improved from 0.46422 to 0.51193, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 2.2159 - accuracy: 0.4361 - val_loss: 2.0130 - val_accuracy: 0.5119 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0809 - accuracy: 0.4748\n",
      "Epoch 00008: val_accuracy improved from 0.51193 to 0.53988, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 2.0806 - accuracy: 0.4747 - val_loss: 1.8987 - val_accuracy: 0.5399 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9678 - accuracy: 0.4991\n",
      "Epoch 00009: val_accuracy improved from 0.53988 to 0.55604, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 1.9711 - accuracy: 0.4988 - val_loss: 1.8122 - val_accuracy: 0.5560 - lr: 0.0010\n",
      "Epoch 10/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.8889 - accuracy: 0.5154\n",
      "Epoch 00010: val_accuracy improved from 0.55604 to 0.58348, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 1.8889 - accuracy: 0.5154 - val_loss: 1.7385 - val_accuracy: 0.5835 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8182 - accuracy: 0.5341\n",
      "Epoch 00011: val_accuracy improved from 0.58348 to 0.59015, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.8163 - accuracy: 0.5342 - val_loss: 1.6958 - val_accuracy: 0.5902 - lr: 0.0010\n",
      "Epoch 12/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.7688 - accuracy: 0.5497\n",
      "Epoch 00012: val_accuracy improved from 0.59015 to 0.60759, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.7688 - accuracy: 0.5497 - val_loss: 1.6491 - val_accuracy: 0.6076 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7228 - accuracy: 0.5551\n",
      "Epoch 00013: val_accuracy improved from 0.60759 to 0.61246, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.7218 - accuracy: 0.5553 - val_loss: 1.5822 - val_accuracy: 0.6125 - lr: 0.0010\n",
      "Epoch 14/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.6592 - accuracy: 0.5779\n",
      "Epoch 00014: val_accuracy improved from 0.61246 to 0.62478, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 1.6592 - accuracy: 0.5779 - val_loss: 1.5698 - val_accuracy: 0.6248 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5956 - accuracy: 0.5909\n",
      "Epoch 00015: val_accuracy did not improve from 0.62478\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.5965 - accuracy: 0.5909 - val_loss: 1.5304 - val_accuracy: 0.6235 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5446 - accuracy: 0.5988\n",
      "Epoch 00016: val_accuracy improved from 0.62478 to 0.62837, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.5454 - accuracy: 0.5986 - val_loss: 1.5142 - val_accuracy: 0.6284 - lr: 0.0010\n",
      "Epoch 17/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.5374 - accuracy: 0.6052\n",
      "Epoch 00017: val_accuracy improved from 0.62837 to 0.63734, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.5374 - accuracy: 0.6052 - val_loss: 1.5126 - val_accuracy: 0.6373 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4984 - accuracy: 0.6087\n",
      "Epoch 00018: val_accuracy did not improve from 0.63734\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1.4983 - accuracy: 0.6090 - val_loss: 1.5065 - val_accuracy: 0.6291 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4596 - accuracy: 0.6230\n",
      "Epoch 00019: val_accuracy improved from 0.63734 to 0.64940, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.4603 - accuracy: 0.6226 - val_loss: 1.4424 - val_accuracy: 0.6494 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4204 - accuracy: 0.6310\n",
      "Epoch 00020: val_accuracy did not improve from 0.64940\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.4208 - accuracy: 0.6304 - val_loss: 1.4232 - val_accuracy: 0.6489 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4119 - accuracy: 0.6369\n",
      "Epoch 00021: val_accuracy improved from 0.64940 to 0.65068, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.4145 - accuracy: 0.6360 - val_loss: 1.4097 - val_accuracy: 0.6507 - lr: 0.0010\n",
      "Epoch 22/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3875 - accuracy: 0.6402\n",
      "Epoch 00022: val_accuracy did not improve from 0.65068\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 1.3904 - accuracy: 0.6399 - val_loss: 1.4300 - val_accuracy: 0.6461 - lr: 0.0010\n",
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3741 - accuracy: 0.6451\n",
      "Epoch 00023: val_accuracy improved from 0.65068 to 0.65786, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 1.3733 - accuracy: 0.6452 - val_loss: 1.3924 - val_accuracy: 0.6579 - lr: 0.0010\n",
      "Epoch 24/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3503 - accuracy: 0.6506\n",
      "Epoch 00024: val_accuracy did not improve from 0.65786\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.3503 - accuracy: 0.6506 - val_loss: 1.4213 - val_accuracy: 0.6458 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3116 - accuracy: 0.6596\n",
      "Epoch 00025: val_accuracy improved from 0.65786 to 0.66222, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.3148 - accuracy: 0.6589 - val_loss: 1.3875 - val_accuracy: 0.6622 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2987 - accuracy: 0.6608\n",
      "Epoch 00026: val_accuracy did not improve from 0.66222\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.2996 - accuracy: 0.6603 - val_loss: 1.3807 - val_accuracy: 0.6607 - lr: 0.0010\n",
      "Epoch 27/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2765 - accuracy: 0.6734\n",
      "Epoch 00027: val_accuracy did not improve from 0.66222\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.2765 - accuracy: 0.6734 - val_loss: 1.3719 - val_accuracy: 0.6609 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2551 - accuracy: 0.6730\n",
      "Epoch 00028: val_accuracy did not improve from 0.66222\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.2561 - accuracy: 0.6727 - val_loss: 1.3692 - val_accuracy: 0.6622 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2413 - accuracy: 0.6829\n",
      "Epoch 00029: val_accuracy improved from 0.66222 to 0.67838, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.2434 - accuracy: 0.6819 - val_loss: 1.3298 - val_accuracy: 0.6784 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2281 - accuracy: 0.6810\n",
      "Epoch 00030: val_accuracy did not improve from 0.67838\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.2276 - accuracy: 0.6807 - val_loss: 1.3566 - val_accuracy: 0.6681 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2224 - accuracy: 0.6837\n",
      "Epoch 00031: val_accuracy did not improve from 0.67838\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.2233 - accuracy: 0.6830 - val_loss: 1.3271 - val_accuracy: 0.6727 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1894 - accuracy: 0.6927\n",
      "Epoch 00032: val_accuracy did not improve from 0.67838\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.1881 - accuracy: 0.6934 - val_loss: 1.3454 - val_accuracy: 0.6774 - lr: 0.0010\n",
      "Epoch 33/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1797 - accuracy: 0.7006\n",
      "Epoch 00033: val_accuracy improved from 0.67838 to 0.67966, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.1797 - accuracy: 0.7006 - val_loss: 1.3436 - val_accuracy: 0.6797 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1652 - accuracy: 0.7028\n",
      "Epoch 00034: val_accuracy did not improve from 0.67966\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.1650 - accuracy: 0.7029 - val_loss: 1.3210 - val_accuracy: 0.6776 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1351 - accuracy: 0.7099\n",
      "Epoch 00035: val_accuracy improved from 0.67966 to 0.68274, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.1399 - accuracy: 0.7086 - val_loss: 1.3238 - val_accuracy: 0.6827 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1390 - accuracy: 0.7069\n",
      "Epoch 00036: val_accuracy did not improve from 0.68274\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.1411 - accuracy: 0.7061 - val_loss: 1.3426 - val_accuracy: 0.6797 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1426 - accuracy: 0.7044\n",
      "Epoch 00037: val_accuracy did not improve from 0.68274\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 1.1433 - accuracy: 0.7041 - val_loss: 1.3293 - val_accuracy: 0.6735 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1085 - accuracy: 0.7172\n",
      "Epoch 00038: val_accuracy did not improve from 0.68274\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.1097 - accuracy: 0.7167 - val_loss: 1.3357 - val_accuracy: 0.6776 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1144 - accuracy: 0.7150\n",
      "Epoch 00039: val_accuracy improved from 0.68274 to 0.68300, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 1.1149 - accuracy: 0.7150 - val_loss: 1.3115 - val_accuracy: 0.6830 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0790 - accuracy: 0.7244\n",
      "Epoch 00040: val_accuracy improved from 0.68300 to 0.68736, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 1.0800 - accuracy: 0.7243 - val_loss: 1.2833 - val_accuracy: 0.6874 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0482 - accuracy: 0.7389\n",
      "Epoch 00041: val_accuracy did not improve from 0.68736\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.0487 - accuracy: 0.7388 - val_loss: 1.2830 - val_accuracy: 0.6863 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0370 - accuracy: 0.7362\n",
      "Epoch 00042: val_accuracy improved from 0.68736 to 0.68915, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.0370 - accuracy: 0.7362 - val_loss: 1.2811 - val_accuracy: 0.6892 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0173 - accuracy: 0.7447\n",
      "Epoch 00043: val_accuracy improved from 0.68915 to 0.69146, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.0179 - accuracy: 0.7448 - val_loss: 1.2709 - val_accuracy: 0.6915 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0102 - accuracy: 0.7520\n",
      "Epoch 00044: val_accuracy improved from 0.69146 to 0.69659, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.0091 - accuracy: 0.7524 - val_loss: 1.2630 - val_accuracy: 0.6966 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9934 - accuracy: 0.7514\n",
      "Epoch 00045: val_accuracy did not improve from 0.69659\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9927 - accuracy: 0.7518 - val_loss: 1.2634 - val_accuracy: 0.6953 - lr: 1.0000e-04\n",
      "Epoch 46/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9926 - accuracy: 0.7502\n",
      "Epoch 00046: val_accuracy did not improve from 0.69659\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.9957 - accuracy: 0.7491 - val_loss: 1.2670 - val_accuracy: 0.6940 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9908 - accuracy: 0.7527\n",
      "Epoch 00047: val_accuracy improved from 0.69659 to 0.69864, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.9908 - accuracy: 0.7527 - val_loss: 1.2653 - val_accuracy: 0.6986 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9860 - accuracy: 0.7479\n",
      "Epoch 00048: val_accuracy did not improve from 0.69864\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.9875 - accuracy: 0.7477 - val_loss: 1.2621 - val_accuracy: 0.6963 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9794 - accuracy: 0.7519\n",
      "Epoch 00049: val_accuracy did not improve from 0.69864\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9779 - accuracy: 0.7523 - val_loss: 1.2684 - val_accuracy: 0.6933 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9733 - accuracy: 0.7553\n",
      "Epoch 00050: val_accuracy did not improve from 0.69864\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9763 - accuracy: 0.7537 - val_loss: 1.2685 - val_accuracy: 0.6938 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9658 - accuracy: 0.7546\n",
      "Epoch 00051: val_accuracy improved from 0.69864 to 0.69967, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.9657 - accuracy: 0.7550 - val_loss: 1.2647 - val_accuracy: 0.6997 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9617 - accuracy: 0.7588\n",
      "Epoch 00052: val_accuracy did not improve from 0.69967\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.9684 - accuracy: 0.7568 - val_loss: 1.2650 - val_accuracy: 0.6971 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9587 - accuracy: 0.7620\n",
      "Epoch 00053: val_accuracy did not improve from 0.69967\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9615 - accuracy: 0.7610 - val_loss: 1.2706 - val_accuracy: 0.6963 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9568 - accuracy: 0.7607\n",
      "Epoch 00054: val_accuracy did not improve from 0.69967\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9572 - accuracy: 0.7609 - val_loss: 1.2645 - val_accuracy: 0.6976 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9483 - accuracy: 0.7633\n",
      "Epoch 00055: val_accuracy did not improve from 0.69967\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.9519 - accuracy: 0.7623 - val_loss: 1.2591 - val_accuracy: 0.6966 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9474 - accuracy: 0.7611\n",
      "Epoch 00056: val_accuracy did not improve from 0.69967\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.9502 - accuracy: 0.7603 - val_loss: 1.2583 - val_accuracy: 0.6966 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9544 - accuracy: 0.7614\n",
      "Epoch 00057: val_accuracy did not improve from 0.69967\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.9543 - accuracy: 0.7612 - val_loss: 1.2540 - val_accuracy: 0.6940 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9464 - accuracy: 0.7616\n",
      "Epoch 00058: val_accuracy did not improve from 0.69967\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9467 - accuracy: 0.7617 - val_loss: 1.2615 - val_accuracy: 0.6979 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9402 - accuracy: 0.7692\n",
      "Epoch 00059: val_accuracy improved from 0.69967 to 0.70044, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.9403 - accuracy: 0.7689 - val_loss: 1.2640 - val_accuracy: 0.7004 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9294 - accuracy: 0.7677\n",
      "Epoch 00060: val_accuracy improved from 0.70044 to 0.70069, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.9307 - accuracy: 0.7673 - val_loss: 1.2601 - val_accuracy: 0.7007 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9273 - accuracy: 0.7689\n",
      "Epoch 00061: val_accuracy improved from 0.70069 to 0.70326, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 0.9296 - accuracy: 0.7683 - val_loss: 1.2545 - val_accuracy: 0.7033 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9404 - accuracy: 0.7654\n",
      "Epoch 00062: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.9407 - accuracy: 0.7650 - val_loss: 1.2534 - val_accuracy: 0.6992 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9309 - accuracy: 0.7707\n",
      "Epoch 00063: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.9302 - accuracy: 0.7713 - val_loss: 1.2473 - val_accuracy: 0.7020 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9292 - accuracy: 0.7697\n",
      "Epoch 00064: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9292 - accuracy: 0.7698 - val_loss: 1.2498 - val_accuracy: 0.6994 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9266 - accuracy: 0.7690\n",
      "Epoch 00065: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9266 - accuracy: 0.7687 - val_loss: 1.2474 - val_accuracy: 0.6999 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9258 - accuracy: 0.7652\n",
      "Epoch 00066: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9258 - accuracy: 0.7652 - val_loss: 1.2497 - val_accuracy: 0.7009 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9287 - accuracy: 0.7646\n",
      "Epoch 00067: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9290 - accuracy: 0.7646 - val_loss: 1.2543 - val_accuracy: 0.6994 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9021 - accuracy: 0.7759\n",
      "Epoch 00068: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9045 - accuracy: 0.7755 - val_loss: 1.2547 - val_accuracy: 0.6994 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9042 - accuracy: 0.7731\n",
      "Epoch 00069: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9081 - accuracy: 0.7722 - val_loss: 1.2677 - val_accuracy: 0.6956 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9081 - accuracy: 0.7682\n",
      "Epoch 00070: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9081 - accuracy: 0.7682 - val_loss: 1.2556 - val_accuracy: 0.6989 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8894 - accuracy: 0.7771\n",
      "Epoch 00071: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8900 - accuracy: 0.7762 - val_loss: 1.2536 - val_accuracy: 0.7004 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8983 - accuracy: 0.7751\n",
      "Epoch 00072: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8996 - accuracy: 0.7746 - val_loss: 1.2597 - val_accuracy: 0.6992 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8953 - accuracy: 0.7769\n",
      "Epoch 00073: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8966 - accuracy: 0.7761 - val_loss: 1.2571 - val_accuracy: 0.6994 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8949 - accuracy: 0.7759\n",
      "Epoch 00074: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8970 - accuracy: 0.7749 - val_loss: 1.2508 - val_accuracy: 0.7017 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9048 - accuracy: 0.7677\n",
      "Epoch 00075: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9048 - accuracy: 0.7677 - val_loss: 1.2613 - val_accuracy: 0.6968 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8991 - accuracy: 0.7762\n",
      "Epoch 00076: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8997 - accuracy: 0.7762 - val_loss: 1.2520 - val_accuracy: 0.7017 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8920 - accuracy: 0.7786\n",
      "Epoch 00077: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8937 - accuracy: 0.7781 - val_loss: 1.2612 - val_accuracy: 0.7022 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8987 - accuracy: 0.7717\n",
      "Epoch 00078: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8979 - accuracy: 0.7721 - val_loss: 1.2533 - val_accuracy: 0.6999 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8913 - accuracy: 0.7787\n",
      "Epoch 00079: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8913 - accuracy: 0.7787 - val_loss: 1.2559 - val_accuracy: 0.6981 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8780 - accuracy: 0.7768\n",
      "Epoch 00080: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8798 - accuracy: 0.7761 - val_loss: 1.2590 - val_accuracy: 0.6984 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8883 - accuracy: 0.7771\n",
      "Epoch 00081: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8873 - accuracy: 0.7771 - val_loss: 1.2664 - val_accuracy: 0.6956 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8846 - accuracy: 0.7781\n",
      "Epoch 00082: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8846 - accuracy: 0.7781 - val_loss: 1.2589 - val_accuracy: 0.6974 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8691 - accuracy: 0.7851\n",
      "Epoch 00083: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8705 - accuracy: 0.7843 - val_loss: 1.2516 - val_accuracy: 0.6994 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8704 - accuracy: 0.7837\n",
      "Epoch 00084: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8697 - accuracy: 0.7835 - val_loss: 1.2523 - val_accuracy: 0.6997 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8787 - accuracy: 0.7836\n",
      "Epoch 00085: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8802 - accuracy: 0.7825 - val_loss: 1.2516 - val_accuracy: 0.6992 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8868 - accuracy: 0.7849\n",
      "Epoch 00086: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8868 - accuracy: 0.7849 - val_loss: 1.2484 - val_accuracy: 0.7004 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8663 - accuracy: 0.7843\n",
      "Epoch 00087: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8675 - accuracy: 0.7847 - val_loss: 1.2493 - val_accuracy: 0.6979 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8703 - accuracy: 0.7839\n",
      "Epoch 00088: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8712 - accuracy: 0.7832 - val_loss: 1.2499 - val_accuracy: 0.6979 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8573 - accuracy: 0.7886\n",
      "Epoch 00089: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8603 - accuracy: 0.7874 - val_loss: 1.2494 - val_accuracy: 0.6976 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8850 - accuracy: 0.7766\n",
      "Epoch 00090: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8850 - accuracy: 0.7766 - val_loss: 1.2506 - val_accuracy: 0.6986 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8720 - accuracy: 0.7830\n",
      "Epoch 00091: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.8754 - accuracy: 0.7818 - val_loss: 1.2441 - val_accuracy: 0.6992 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8777 - accuracy: 0.7780\n",
      "Epoch 00092: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8780 - accuracy: 0.7779 - val_loss: 1.2451 - val_accuracy: 0.6986 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8894 - accuracy: 0.7824\n",
      "Epoch 00093: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.8898 - accuracy: 0.7821 - val_loss: 1.2472 - val_accuracy: 0.6997 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8747 - accuracy: 0.7823\n",
      "Epoch 00094: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8758 - accuracy: 0.7820 - val_loss: 1.2501 - val_accuracy: 0.6986 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8674 - accuracy: 0.7881\n",
      "Epoch 00095: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8676 - accuracy: 0.7881 - val_loss: 1.2446 - val_accuracy: 0.7007 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8784 - accuracy: 0.7772\n",
      "Epoch 00096: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8762 - accuracy: 0.7779 - val_loss: 1.2508 - val_accuracy: 0.6992 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8722 - accuracy: 0.7807\n",
      "Epoch 00097: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8722 - accuracy: 0.7807 - val_loss: 1.2474 - val_accuracy: 0.7007 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8780 - accuracy: 0.7798\n",
      "Epoch 00098: val_accuracy did not improve from 0.70326\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8780 - accuracy: 0.7798 - val_loss: 1.2479 - val_accuracy: 0.7025 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8883 - accuracy: 0.7812\n",
      "Epoch 00099: val_accuracy did not improve from 0.70326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8883 - accuracy: 0.7812 - val_loss: 1.2422 - val_accuracy: 0.7022 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8693 - accuracy: 0.7831\n",
      "Epoch 00100: val_accuracy improved from 0.70326 to 0.70351, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.8702 - accuracy: 0.7826 - val_loss: 1.2444 - val_accuracy: 0.7035 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8712 - accuracy: 0.7829\n",
      "Epoch 00101: val_accuracy did not improve from 0.70351\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8722 - accuracy: 0.7825 - val_loss: 1.2474 - val_accuracy: 0.6999 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8817 - accuracy: 0.7797\n",
      "Epoch 00102: val_accuracy did not improve from 0.70351\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8834 - accuracy: 0.7793 - val_loss: 1.2487 - val_accuracy: 0.7020 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8717 - accuracy: 0.7846\n",
      "Epoch 00103: val_accuracy did not improve from 0.70351\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8730 - accuracy: 0.7838 - val_loss: 1.2546 - val_accuracy: 0.7025 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8821 - accuracy: 0.7786\n",
      "Epoch 00104: val_accuracy did not improve from 0.70351\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8843 - accuracy: 0.7782 - val_loss: 1.2482 - val_accuracy: 0.7004 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8717 - accuracy: 0.7838\n",
      "Epoch 00105: val_accuracy did not improve from 0.70351\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8726 - accuracy: 0.7835 - val_loss: 1.2503 - val_accuracy: 0.7009 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8720 - accuracy: 0.7814\n",
      "Epoch 00106: val_accuracy did not improve from 0.70351\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8729 - accuracy: 0.7812 - val_loss: 1.2466 - val_accuracy: 0.7015 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8693 - accuracy: 0.7808\n",
      "Epoch 00107: val_accuracy did not improve from 0.70351\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8714 - accuracy: 0.7801 - val_loss: 1.2447 - val_accuracy: 0.7025 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8739 - accuracy: 0.7814\n",
      "Epoch 00108: val_accuracy did not improve from 0.70351\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8739 - accuracy: 0.7814 - val_loss: 1.2460 - val_accuracy: 0.7002 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8570 - accuracy: 0.7873\n",
      "Epoch 00109: val_accuracy improved from 0.70351 to 0.70377, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 0.8585 - accuracy: 0.7870 - val_loss: 1.2497 - val_accuracy: 0.7038 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8729 - accuracy: 0.7789\n",
      "Epoch 00110: val_accuracy did not improve from 0.70377\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8729 - accuracy: 0.7789 - val_loss: 1.2511 - val_accuracy: 0.7030 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8905 - accuracy: 0.7721\n",
      "Epoch 00111: val_accuracy did not improve from 0.70377\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8907 - accuracy: 0.7720 - val_loss: 1.2555 - val_accuracy: 0.7012 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8681 - accuracy: 0.7849\n",
      "Epoch 00112: val_accuracy did not improve from 0.70377\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8700 - accuracy: 0.7846 - val_loss: 1.2531 - val_accuracy: 0.7030 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8847 - accuracy: 0.7740\n",
      "Epoch 00113: val_accuracy improved from 0.70377 to 0.70557, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 0.8844 - accuracy: 0.7740 - val_loss: 1.2476 - val_accuracy: 0.7056 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8717 - accuracy: 0.7847\n",
      "Epoch 00114: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8727 - accuracy: 0.7844 - val_loss: 1.2480 - val_accuracy: 0.7027 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8874 - accuracy: 0.7797\n",
      "Epoch 00115: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8894 - accuracy: 0.7794 - val_loss: 1.2450 - val_accuracy: 0.7045 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8662 - accuracy: 0.7836\n",
      "Epoch 00116: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8698 - accuracy: 0.7826 - val_loss: 1.2464 - val_accuracy: 0.7025 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8717 - accuracy: 0.7831\n",
      "Epoch 00117: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8732 - accuracy: 0.7827 - val_loss: 1.2504 - val_accuracy: 0.7035 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8584 - accuracy: 0.7872\n",
      "Epoch 00118: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.8616 - accuracy: 0.7865 - val_loss: 1.2516 - val_accuracy: 0.7015 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8703 - accuracy: 0.7843\n",
      "Epoch 00119: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8736 - accuracy: 0.7833 - val_loss: 1.2683 - val_accuracy: 0.6986 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8745 - accuracy: 0.7843\n",
      "Epoch 00120: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8779 - accuracy: 0.7833 - val_loss: 1.2547 - val_accuracy: 0.6999 - lr: 1.0000e-06\n",
      "epoch_number 113\n",
      "train accuracy and validation accuracy 0.7740411162376404 0.7055655121803284\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.2476 - accuracy: 0.7056\n",
      "test_accuracy 0.7055655121803284\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087, 0.7748140692710876, 0.7055655121803284]\n",
      "0.7533095203913175\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S14_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S14_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.7196 - accuracy: 0.1236\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.24827, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 367ms/step - loss: 4.7078 - accuracy: 0.1232 - val_loss: 3.5014 - val_accuracy: 0.2483 - lr: 0.0010\n",
      "Epoch 2/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 3.1361 - accuracy: 0.2299\n",
      "Epoch 00002: val_accuracy improved from 0.24827 to 0.33265, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 3.1361 - accuracy: 0.2299 - val_loss: 2.7050 - val_accuracy: 0.3326 - lr: 0.0010\n",
      "Epoch 3/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.6952 - accuracy: 0.3156\n",
      "Epoch 00003: val_accuracy improved from 0.33265 to 0.43780, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 2.6952 - accuracy: 0.3156 - val_loss: 2.3081 - val_accuracy: 0.4378 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4083 - accuracy: 0.3826\n",
      "Epoch 00004: val_accuracy improved from 0.43780 to 0.48756, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 2.4079 - accuracy: 0.3821 - val_loss: 2.1211 - val_accuracy: 0.4876 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2141 - accuracy: 0.4400\n",
      "Epoch 00005: val_accuracy improved from 0.48756 to 0.51295, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 2.2190 - accuracy: 0.4383 - val_loss: 1.9568 - val_accuracy: 0.5130 - lr: 0.0010\n",
      "Epoch 6/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.0847 - accuracy: 0.4624\n",
      "Epoch 00006: val_accuracy improved from 0.51295 to 0.54193, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 2.0847 - accuracy: 0.4624 - val_loss: 1.8588 - val_accuracy: 0.5419 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9868 - accuracy: 0.4899\n",
      "Epoch 00007: val_accuracy improved from 0.54193 to 0.54373, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.9887 - accuracy: 0.4893 - val_loss: 1.8154 - val_accuracy: 0.5437 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8913 - accuracy: 0.5024\n",
      "Epoch 00008: val_accuracy improved from 0.54373 to 0.57374, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 1.8918 - accuracy: 0.5017 - val_loss: 1.7268 - val_accuracy: 0.5737 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8040 - accuracy: 0.5348\n",
      "Epoch 00009: val_accuracy improved from 0.57374 to 0.58836, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.8060 - accuracy: 0.5343 - val_loss: 1.6466 - val_accuracy: 0.5884 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7425 - accuracy: 0.5469\n",
      "Epoch 00010: val_accuracy improved from 0.58836 to 0.60118, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.7430 - accuracy: 0.5468 - val_loss: 1.5844 - val_accuracy: 0.6012 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6863 - accuracy: 0.5666\n",
      "Epoch 00011: val_accuracy improved from 0.60118 to 0.61221, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.6889 - accuracy: 0.5667 - val_loss: 1.5745 - val_accuracy: 0.6122 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6550 - accuracy: 0.5729\n",
      "Epoch 00012: val_accuracy improved from 0.61221 to 0.61785, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.6547 - accuracy: 0.5731 - val_loss: 1.5205 - val_accuracy: 0.6179 - lr: 0.0010\n",
      "Epoch 13/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.5617 - accuracy: 0.5900\n",
      "Epoch 00013: val_accuracy improved from 0.61785 to 0.63016, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.5617 - accuracy: 0.5900 - val_loss: 1.4950 - val_accuracy: 0.6302 - lr: 0.0010\n",
      "Epoch 14/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.5407 - accuracy: 0.5975\n",
      "Epoch 00014: val_accuracy did not improve from 0.63016\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.5407 - accuracy: 0.5975 - val_loss: 1.4724 - val_accuracy: 0.6237 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5260 - accuracy: 0.6037\n",
      "Epoch 00015: val_accuracy improved from 0.63016 to 0.64222, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.5282 - accuracy: 0.6034 - val_loss: 1.4509 - val_accuracy: 0.6422 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4796 - accuracy: 0.6137\n",
      "Epoch 00016: val_accuracy did not improve from 0.64222\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.4812 - accuracy: 0.6145 - val_loss: 1.4467 - val_accuracy: 0.6381 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4624 - accuracy: 0.6194\n",
      "Epoch 00017: val_accuracy did not improve from 0.64222\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.4615 - accuracy: 0.6201 - val_loss: 1.4376 - val_accuracy: 0.6394 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4405 - accuracy: 0.6270\n",
      "Epoch 00018: val_accuracy improved from 0.64222 to 0.64324, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.4407 - accuracy: 0.6266 - val_loss: 1.4219 - val_accuracy: 0.6432 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3897 - accuracy: 0.6357\n",
      "Epoch 00019: val_accuracy improved from 0.64324 to 0.64760, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.3906 - accuracy: 0.6353 - val_loss: 1.4065 - val_accuracy: 0.6476 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3620 - accuracy: 0.6477\n",
      "Epoch 00020: val_accuracy improved from 0.64760 to 0.65119, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.3641 - accuracy: 0.6468 - val_loss: 1.4100 - val_accuracy: 0.6512 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3483 - accuracy: 0.6520\n",
      "Epoch 00021: val_accuracy improved from 0.65119 to 0.65735, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.3525 - accuracy: 0.6507 - val_loss: 1.3913 - val_accuracy: 0.6573 - lr: 0.0010\n",
      "Epoch 22/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3511 - accuracy: 0.6507\n",
      "Epoch 00022: val_accuracy did not improve from 0.65735\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.3493 - accuracy: 0.6508 - val_loss: 1.3793 - val_accuracy: 0.6553 - lr: 0.0010\n",
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3112 - accuracy: 0.6569\n",
      "Epoch 00023: val_accuracy did not improve from 0.65735\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.3087 - accuracy: 0.6579 - val_loss: 1.3945 - val_accuracy: 0.6556 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3002 - accuracy: 0.6591\n",
      "Epoch 00024: val_accuracy improved from 0.65735 to 0.66530, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.3005 - accuracy: 0.6592 - val_loss: 1.3483 - val_accuracy: 0.6653 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2663 - accuracy: 0.6739\n",
      "Epoch 00025: val_accuracy improved from 0.66530 to 0.67145, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.2690 - accuracy: 0.6733 - val_loss: 1.3369 - val_accuracy: 0.6715 - lr: 0.0010\n",
      "Epoch 26/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2718 - accuracy: 0.6685\n",
      "Epoch 00026: val_accuracy did not improve from 0.67145\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.2718 - accuracy: 0.6685 - val_loss: 1.3453 - val_accuracy: 0.6674 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2480 - accuracy: 0.6739\n",
      "Epoch 00027: val_accuracy did not improve from 0.67145\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.2480 - accuracy: 0.6735 - val_loss: 1.3312 - val_accuracy: 0.6656 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2142 - accuracy: 0.6878\n",
      "Epoch 00028: val_accuracy did not improve from 0.67145\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.2155 - accuracy: 0.6870 - val_loss: 1.3262 - val_accuracy: 0.6709 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2037 - accuracy: 0.6852\n",
      "Epoch 00029: val_accuracy improved from 0.67145 to 0.67428, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 1.2036 - accuracy: 0.6852 - val_loss: 1.3405 - val_accuracy: 0.6743 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2031 - accuracy: 0.6954\n",
      "Epoch 00030: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.2016 - accuracy: 0.6957 - val_loss: 1.3228 - val_accuracy: 0.6686 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1893 - accuracy: 0.6882\n",
      "Epoch 00031: val_accuracy improved from 0.67428 to 0.68094, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.1893 - accuracy: 0.6884 - val_loss: 1.3149 - val_accuracy: 0.6809 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1370 - accuracy: 0.7024\n",
      "Epoch 00032: val_accuracy did not improve from 0.68094\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.1395 - accuracy: 0.7021 - val_loss: 1.3268 - val_accuracy: 0.6758 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1520 - accuracy: 0.6984\n",
      "Epoch 00033: val_accuracy improved from 0.68094 to 0.68351, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.1528 - accuracy: 0.6985 - val_loss: 1.3057 - val_accuracy: 0.6835 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1270 - accuracy: 0.7103\n",
      "Epoch 00034: val_accuracy did not improve from 0.68351\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 1.1287 - accuracy: 0.7096 - val_loss: 1.3283 - val_accuracy: 0.6715 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1269 - accuracy: 0.7042\n",
      "Epoch 00035: val_accuracy did not improve from 0.68351\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.1301 - accuracy: 0.7035 - val_loss: 1.3112 - val_accuracy: 0.6827 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1315 - accuracy: 0.7080\n",
      "Epoch 00036: val_accuracy did not improve from 0.68351\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.1321 - accuracy: 0.7078 - val_loss: 1.3134 - val_accuracy: 0.6763 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1192 - accuracy: 0.7080\n",
      "Epoch 00037: val_accuracy did not improve from 0.68351\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.1193 - accuracy: 0.7073 - val_loss: 1.3035 - val_accuracy: 0.6833 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0823 - accuracy: 0.7217\n",
      "Epoch 00038: val_accuracy improved from 0.68351 to 0.68377, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.0820 - accuracy: 0.7216 - val_loss: 1.2885 - val_accuracy: 0.6838 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0856 - accuracy: 0.7172\n",
      "Epoch 00039: val_accuracy improved from 0.68377 to 0.68530, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 1.0856 - accuracy: 0.7173 - val_loss: 1.3183 - val_accuracy: 0.6853 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0521 - accuracy: 0.7279\n",
      "Epoch 00040: val_accuracy improved from 0.68530 to 0.68915, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.0520 - accuracy: 0.7273 - val_loss: 1.2790 - val_accuracy: 0.6892 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0281 - accuracy: 0.7403\n",
      "Epoch 00041: val_accuracy improved from 0.68915 to 0.69377, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.0296 - accuracy: 0.7399 - val_loss: 1.2685 - val_accuracy: 0.6938 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9978 - accuracy: 0.7453\n",
      "Epoch 00042: val_accuracy improved from 0.69377 to 0.69890, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 2s 150ms/step - loss: 1.0024 - accuracy: 0.7446 - val_loss: 1.2620 - val_accuracy: 0.6989 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9944 - accuracy: 0.7482\n",
      "Epoch 00043: val_accuracy improved from 0.69890 to 0.70044, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.9930 - accuracy: 0.7485 - val_loss: 1.2626 - val_accuracy: 0.7004 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9959 - accuracy: 0.7444\n",
      "Epoch 00044: val_accuracy did not improve from 0.70044\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.9959 - accuracy: 0.7444 - val_loss: 1.2557 - val_accuracy: 0.7004 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9625 - accuracy: 0.7590\n",
      "Epoch 00045: val_accuracy did not improve from 0.70044\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.9645 - accuracy: 0.7585 - val_loss: 1.2499 - val_accuracy: 0.6994 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9803 - accuracy: 0.7451\n",
      "Epoch 00046: val_accuracy did not improve from 0.70044\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.9816 - accuracy: 0.7449 - val_loss: 1.2497 - val_accuracy: 0.6997 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9616 - accuracy: 0.7561\n",
      "Epoch 00047: val_accuracy improved from 0.70044 to 0.70557, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.9632 - accuracy: 0.7559 - val_loss: 1.2436 - val_accuracy: 0.7056 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9568 - accuracy: 0.7552\n",
      "Epoch 00048: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.9573 - accuracy: 0.7550 - val_loss: 1.2431 - val_accuracy: 0.7048 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9614 - accuracy: 0.7547\n",
      "Epoch 00049: val_accuracy improved from 0.70557 to 0.70736, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.9638 - accuracy: 0.7541 - val_loss: 1.2455 - val_accuracy: 0.7074 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9465 - accuracy: 0.7568\n",
      "Epoch 00050: val_accuracy did not improve from 0.70736\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.9477 - accuracy: 0.7566 - val_loss: 1.2393 - val_accuracy: 0.7051 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9493 - accuracy: 0.7559\n",
      "Epoch 00051: val_accuracy did not improve from 0.70736\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.9492 - accuracy: 0.7562 - val_loss: 1.2391 - val_accuracy: 0.7045 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9472 - accuracy: 0.7587\n",
      "Epoch 00052: val_accuracy did not improve from 0.70736\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.9467 - accuracy: 0.7589 - val_loss: 1.2358 - val_accuracy: 0.7071 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9573 - accuracy: 0.7588\n",
      "Epoch 00053: val_accuracy improved from 0.70736 to 0.70916, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.9577 - accuracy: 0.7584 - val_loss: 1.2392 - val_accuracy: 0.7092 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9358 - accuracy: 0.7623\n",
      "Epoch 00054: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9367 - accuracy: 0.7616 - val_loss: 1.2415 - val_accuracy: 0.7074 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9447 - accuracy: 0.7656\n",
      "Epoch 00055: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9447 - accuracy: 0.7656 - val_loss: 1.2383 - val_accuracy: 0.7086 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9292 - accuracy: 0.7699\n",
      "Epoch 00056: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9335 - accuracy: 0.7688 - val_loss: 1.2375 - val_accuracy: 0.7074 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9300 - accuracy: 0.7617\n",
      "Epoch 00057: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.9328 - accuracy: 0.7611 - val_loss: 1.2355 - val_accuracy: 0.7068 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9274 - accuracy: 0.7632\n",
      "Epoch 00058: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.9274 - accuracy: 0.7632 - val_loss: 1.2312 - val_accuracy: 0.7061 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9195 - accuracy: 0.7668\n",
      "Epoch 00059: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9204 - accuracy: 0.7665 - val_loss: 1.2322 - val_accuracy: 0.7076 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9108 - accuracy: 0.7688\n",
      "Epoch 00060: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9124 - accuracy: 0.7682 - val_loss: 1.2314 - val_accuracy: 0.7081 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9316 - accuracy: 0.7612\n",
      "Epoch 00061: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.9333 - accuracy: 0.7612 - val_loss: 1.2312 - val_accuracy: 0.7071 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9086 - accuracy: 0.7653\n",
      "Epoch 00062: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.9091 - accuracy: 0.7652 - val_loss: 1.2253 - val_accuracy: 0.7086 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9144 - accuracy: 0.7701\n",
      "Epoch 00063: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.9144 - accuracy: 0.7700 - val_loss: 1.2288 - val_accuracy: 0.7079 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9238 - accuracy: 0.7649\n",
      "Epoch 00064: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9247 - accuracy: 0.7648 - val_loss: 1.2288 - val_accuracy: 0.7038 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9085 - accuracy: 0.7703\n",
      "Epoch 00065: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9120 - accuracy: 0.7690 - val_loss: 1.2320 - val_accuracy: 0.7074 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8929 - accuracy: 0.7709\n",
      "Epoch 00066: val_accuracy did not improve from 0.70916\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8923 - accuracy: 0.7714 - val_loss: 1.2328 - val_accuracy: 0.7084 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9181 - accuracy: 0.7660\n",
      "Epoch 00067: val_accuracy improved from 0.70916 to 0.71044, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.9194 - accuracy: 0.7656 - val_loss: 1.2336 - val_accuracy: 0.7104 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8879 - accuracy: 0.7719\n",
      "Epoch 00068: val_accuracy did not improve from 0.71044\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8882 - accuracy: 0.7717 - val_loss: 1.2324 - val_accuracy: 0.7102 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9021 - accuracy: 0.7690\n",
      "Epoch 00069: val_accuracy improved from 0.71044 to 0.71070, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.9019 - accuracy: 0.7690 - val_loss: 1.2304 - val_accuracy: 0.7107 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9012 - accuracy: 0.7726\n",
      "Epoch 00070: val_accuracy did not improve from 0.71070\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9012 - accuracy: 0.7726 - val_loss: 1.2337 - val_accuracy: 0.7089 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9046 - accuracy: 0.7746\n",
      "Epoch 00071: val_accuracy did not improve from 0.71070\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9046 - accuracy: 0.7746 - val_loss: 1.2353 - val_accuracy: 0.7066 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8932 - accuracy: 0.7736\n",
      "Epoch 00072: val_accuracy did not improve from 0.71070\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8943 - accuracy: 0.7733 - val_loss: 1.2289 - val_accuracy: 0.7094 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8965 - accuracy: 0.7737\n",
      "Epoch 00073: val_accuracy did not improve from 0.71070\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8994 - accuracy: 0.7728 - val_loss: 1.2318 - val_accuracy: 0.7061 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8879 - accuracy: 0.7776\n",
      "Epoch 00074: val_accuracy did not improve from 0.71070\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8908 - accuracy: 0.7773 - val_loss: 1.2329 - val_accuracy: 0.7086 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8960 - accuracy: 0.7728\n",
      "Epoch 00075: val_accuracy did not improve from 0.71070\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8992 - accuracy: 0.7723 - val_loss: 1.2325 - val_accuracy: 0.7099 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8861 - accuracy: 0.7772\n",
      "Epoch 00076: val_accuracy did not improve from 0.71070\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8860 - accuracy: 0.7776 - val_loss: 1.2266 - val_accuracy: 0.7107 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8791 - accuracy: 0.7792\n",
      "Epoch 00077: val_accuracy improved from 0.71070 to 0.71249, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.8819 - accuracy: 0.7788 - val_loss: 1.2264 - val_accuracy: 0.7125 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8795 - accuracy: 0.7776\n",
      "Epoch 00078: val_accuracy improved from 0.71249 to 0.71352, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.8789 - accuracy: 0.7777 - val_loss: 1.2284 - val_accuracy: 0.7135 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8779 - accuracy: 0.7796\n",
      "Epoch 00079: val_accuracy did not improve from 0.71352\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8779 - accuracy: 0.7796 - val_loss: 1.2293 - val_accuracy: 0.7135 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8677 - accuracy: 0.7767\n",
      "Epoch 00080: val_accuracy improved from 0.71352 to 0.71608, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.8686 - accuracy: 0.7765 - val_loss: 1.2281 - val_accuracy: 0.7161 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8709 - accuracy: 0.7829\n",
      "Epoch 00081: val_accuracy did not improve from 0.71608\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8713 - accuracy: 0.7831 - val_loss: 1.2273 - val_accuracy: 0.7143 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8679 - accuracy: 0.7829\n",
      "Epoch 00082: val_accuracy did not improve from 0.71608\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8704 - accuracy: 0.7817 - val_loss: 1.2253 - val_accuracy: 0.7158 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8757 - accuracy: 0.7726\n",
      "Epoch 00083: val_accuracy did not improve from 0.71608\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8755 - accuracy: 0.7723 - val_loss: 1.2294 - val_accuracy: 0.7156 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8707 - accuracy: 0.7800\n",
      "Epoch 00084: val_accuracy did not improve from 0.71608\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.8701 - accuracy: 0.7799 - val_loss: 1.2265 - val_accuracy: 0.7130 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8705 - accuracy: 0.7829\n",
      "Epoch 00085: val_accuracy did not improve from 0.71608\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8705 - accuracy: 0.7829 - val_loss: 1.2261 - val_accuracy: 0.7138 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8678 - accuracy: 0.7806\n",
      "Epoch 00086: val_accuracy did not improve from 0.71608\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.8679 - accuracy: 0.7807 - val_loss: 1.2246 - val_accuracy: 0.7156 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8557 - accuracy: 0.7903\n",
      "Epoch 00087: val_accuracy did not improve from 0.71608\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8576 - accuracy: 0.7896 - val_loss: 1.2249 - val_accuracy: 0.7161 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8713 - accuracy: 0.7790\n",
      "Epoch 00088: val_accuracy did not improve from 0.71608\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8713 - accuracy: 0.7791 - val_loss: 1.2229 - val_accuracy: 0.7145 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8703 - accuracy: 0.7864\n",
      "Epoch 00089: val_accuracy did not improve from 0.71608\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8703 - accuracy: 0.7864 - val_loss: 1.2249 - val_accuracy: 0.7145 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8615 - accuracy: 0.7852\n",
      "Epoch 00090: val_accuracy did not improve from 0.71608\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8619 - accuracy: 0.7846 - val_loss: 1.2281 - val_accuracy: 0.7158 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8474 - accuracy: 0.7873\n",
      "Epoch 00091: val_accuracy improved from 0.71608 to 0.71634, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 0.8474 - accuracy: 0.7873 - val_loss: 1.2236 - val_accuracy: 0.7163 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8652 - accuracy: 0.7843\n",
      "Epoch 00092: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.8682 - accuracy: 0.7834 - val_loss: 1.2213 - val_accuracy: 0.7143 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8719 - accuracy: 0.7816\n",
      "Epoch 00093: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8719 - accuracy: 0.7816 - val_loss: 1.2224 - val_accuracy: 0.7145 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8640 - accuracy: 0.7840\n",
      "Epoch 00094: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8661 - accuracy: 0.7835 - val_loss: 1.2240 - val_accuracy: 0.7148 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8567 - accuracy: 0.7883\n",
      "Epoch 00095: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8577 - accuracy: 0.7880 - val_loss: 1.2244 - val_accuracy: 0.7161 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8569 - accuracy: 0.7790\n",
      "Epoch 00096: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8581 - accuracy: 0.7785 - val_loss: 1.2254 - val_accuracy: 0.7143 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8595 - accuracy: 0.7800\n",
      "Epoch 00097: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8603 - accuracy: 0.7795 - val_loss: 1.2278 - val_accuracy: 0.7151 - lr: 1.0000e-05\n",
      "Epoch 98/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8641 - accuracy: 0.7819\n",
      "Epoch 00098: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8635 - accuracy: 0.7822 - val_loss: 1.2276 - val_accuracy: 0.7145 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8607 - accuracy: 0.7882\n",
      "Epoch 00099: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8645 - accuracy: 0.7873 - val_loss: 1.2235 - val_accuracy: 0.7140 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8555 - accuracy: 0.7851\n",
      "Epoch 00100: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8592 - accuracy: 0.7837 - val_loss: 1.2233 - val_accuracy: 0.7148 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8695 - accuracy: 0.7813\n",
      "Epoch 00101: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.8684 - accuracy: 0.7815 - val_loss: 1.2201 - val_accuracy: 0.7151 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8482 - accuracy: 0.7859\n",
      "Epoch 00102: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8499 - accuracy: 0.7855 - val_loss: 1.2205 - val_accuracy: 0.7153 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8524 - accuracy: 0.7848\n",
      "Epoch 00103: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.8514 - accuracy: 0.7850 - val_loss: 1.2214 - val_accuracy: 0.7153 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8676 - accuracy: 0.7798\n",
      "Epoch 00104: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.8697 - accuracy: 0.7792 - val_loss: 1.2222 - val_accuracy: 0.7148 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8510 - accuracy: 0.7866\n",
      "Epoch 00105: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8522 - accuracy: 0.7865 - val_loss: 1.2230 - val_accuracy: 0.7153 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8564 - accuracy: 0.7838\n",
      "Epoch 00106: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8580 - accuracy: 0.7833 - val_loss: 1.2258 - val_accuracy: 0.7130 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8670 - accuracy: 0.7843\n",
      "Epoch 00107: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8697 - accuracy: 0.7833 - val_loss: 1.2242 - val_accuracy: 0.7143 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8605 - accuracy: 0.7828\n",
      "Epoch 00108: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8627 - accuracy: 0.7821 - val_loss: 1.2212 - val_accuracy: 0.7135 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8576 - accuracy: 0.7854\n",
      "Epoch 00109: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8577 - accuracy: 0.7855 - val_loss: 1.2247 - val_accuracy: 0.7130 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8574 - accuracy: 0.7888\n",
      "Epoch 00110: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8616 - accuracy: 0.7879 - val_loss: 1.2219 - val_accuracy: 0.7117 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8617 - accuracy: 0.7811\n",
      "Epoch 00111: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8617 - accuracy: 0.7811 - val_loss: 1.2227 - val_accuracy: 0.7140 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8637 - accuracy: 0.7773\n",
      "Epoch 00112: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.8653 - accuracy: 0.7767 - val_loss: 1.2196 - val_accuracy: 0.7148 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8634 - accuracy: 0.7794\n",
      "Epoch 00113: val_accuracy did not improve from 0.71634\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8661 - accuracy: 0.7788 - val_loss: 1.2221 - val_accuracy: 0.7140 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8664 - accuracy: 0.7817\n",
      "Epoch 00114: val_accuracy improved from 0.71634 to 0.71685, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 0.8661 - accuracy: 0.7817 - val_loss: 1.2201 - val_accuracy: 0.7169 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8464 - accuracy: 0.7876\n",
      "Epoch 00115: val_accuracy did not improve from 0.71685\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8480 - accuracy: 0.7872 - val_loss: 1.2204 - val_accuracy: 0.7156 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8676 - accuracy: 0.7828\n",
      "Epoch 00116: val_accuracy did not improve from 0.71685\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8674 - accuracy: 0.7825 - val_loss: 1.2225 - val_accuracy: 0.7158 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8573 - accuracy: 0.7865\n",
      "Epoch 00117: val_accuracy did not improve from 0.71685\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8573 - accuracy: 0.7865 - val_loss: 1.2246 - val_accuracy: 0.7148 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8546 - accuracy: 0.7853\n",
      "Epoch 00118: val_accuracy did not improve from 0.71685\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8559 - accuracy: 0.7854 - val_loss: 1.2246 - val_accuracy: 0.7151 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8628 - accuracy: 0.7850\n",
      "Epoch 00119: val_accuracy did not improve from 0.71685\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8622 - accuracy: 0.7853 - val_loss: 1.2221 - val_accuracy: 0.7135 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8569 - accuracy: 0.7854\n",
      "Epoch 00120: val_accuracy did not improve from 0.71685\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8596 - accuracy: 0.7842 - val_loss: 1.2221 - val_accuracy: 0.7148 - lr: 1.0000e-06\n",
      "epoch_number 114\n",
      "train accuracy and validation accuracy 0.7817342281341553 0.7168504595756531\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.2201 - accuracy: 0.7169\n",
      "test_accuracy 0.7168504595756531\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087, 0.7748140692710876, 0.7055655121803284, 0.7168504595756531]\n",
      "0.7507053017616272\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S15_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S15_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 4.7072 - accuracy: 0.1103\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.17774, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 381ms/step - loss: 4.7072 - accuracy: 0.1103 - val_loss: 3.8541 - val_accuracy: 0.1777 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.2167 - accuracy: 0.2044\n",
      "Epoch 00002: val_accuracy improved from 0.17774 to 0.29110, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 3.2232 - accuracy: 0.2039 - val_loss: 2.8096 - val_accuracy: 0.2911 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.8348 - accuracy: 0.2797\n",
      "Epoch 00003: val_accuracy improved from 0.29110 to 0.37163, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 118ms/step - loss: 2.8406 - accuracy: 0.2795 - val_loss: 2.4946 - val_accuracy: 0.3716 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.5866 - accuracy: 0.3342\n",
      "Epoch 00004: val_accuracy improved from 0.37163 to 0.41575, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 2.5894 - accuracy: 0.3340 - val_loss: 2.3080 - val_accuracy: 0.4157 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4234 - accuracy: 0.3729\n",
      "Epoch 00005: val_accuracy improved from 0.41575 to 0.45012, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 2.4206 - accuracy: 0.3733 - val_loss: 2.1964 - val_accuracy: 0.4501 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2681 - accuracy: 0.4116\n",
      "Epoch 00006: val_accuracy improved from 0.45012 to 0.48294, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 2.2694 - accuracy: 0.4109 - val_loss: 2.0412 - val_accuracy: 0.4829 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1581 - accuracy: 0.4388\n",
      "Epoch 00007: val_accuracy improved from 0.48294 to 0.51064, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 116ms/step - loss: 2.1622 - accuracy: 0.4375 - val_loss: 1.9499 - val_accuracy: 0.5106 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0562 - accuracy: 0.4594\n",
      "Epoch 00008: val_accuracy improved from 0.51064 to 0.53116, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 121ms/step - loss: 2.0557 - accuracy: 0.4599 - val_loss: 1.8763 - val_accuracy: 0.5312 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9758 - accuracy: 0.4818\n",
      "Epoch 00009: val_accuracy improved from 0.53116 to 0.54116, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 1.9784 - accuracy: 0.4814 - val_loss: 1.8280 - val_accuracy: 0.5412 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8681 - accuracy: 0.5104\n",
      "Epoch 00010: val_accuracy improved from 0.54116 to 0.54912, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 120ms/step - loss: 1.8706 - accuracy: 0.5099 - val_loss: 1.7698 - val_accuracy: 0.5491 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8341 - accuracy: 0.5176\n",
      "Epoch 00011: val_accuracy improved from 0.54912 to 0.56091, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 114ms/step - loss: 1.8358 - accuracy: 0.5174 - val_loss: 1.7179 - val_accuracy: 0.5609 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7538 - accuracy: 0.5354\n",
      "Epoch 00012: val_accuracy improved from 0.56091 to 0.57887, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 119ms/step - loss: 1.7545 - accuracy: 0.5357 - val_loss: 1.6630 - val_accuracy: 0.5789 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7035 - accuracy: 0.5508\n",
      "Epoch 00013: val_accuracy improved from 0.57887 to 0.59092, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.7044 - accuracy: 0.5505 - val_loss: 1.6299 - val_accuracy: 0.5909 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6754 - accuracy: 0.5629\n",
      "Epoch 00014: val_accuracy improved from 0.59092 to 0.59708, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 1.6747 - accuracy: 0.5630 - val_loss: 1.5969 - val_accuracy: 0.5971 - lr: 0.0010\n",
      "Epoch 15/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.6482 - accuracy: 0.5682\n",
      "Epoch 00015: val_accuracy improved from 0.59708 to 0.60580, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 124ms/step - loss: 1.6482 - accuracy: 0.5682 - val_loss: 1.5725 - val_accuracy: 0.6058 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5817 - accuracy: 0.5861\n",
      "Epoch 00016: val_accuracy improved from 0.60580 to 0.61144, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 122ms/step - loss: 1.5836 - accuracy: 0.5856 - val_loss: 1.5508 - val_accuracy: 0.6114 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5610 - accuracy: 0.5899\n",
      "Epoch 00017: val_accuracy improved from 0.61144 to 0.62016, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.5637 - accuracy: 0.5893 - val_loss: 1.5276 - val_accuracy: 0.6202 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5360 - accuracy: 0.5989\n",
      "Epoch 00018: val_accuracy did not improve from 0.62016\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 1.5383 - accuracy: 0.5976 - val_loss: 1.5264 - val_accuracy: 0.6202 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4907 - accuracy: 0.6052\n",
      "Epoch 00019: val_accuracy did not improve from 0.62016\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.4932 - accuracy: 0.6048 - val_loss: 1.5207 - val_accuracy: 0.6122 - lr: 0.0010\n",
      "Epoch 20/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.4960 - accuracy: 0.6136\n",
      "Epoch 00020: val_accuracy did not improve from 0.62016\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.4960 - accuracy: 0.6136 - val_loss: 1.5422 - val_accuracy: 0.6091 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4341 - accuracy: 0.6227\n",
      "Epoch 00021: val_accuracy did not improve from 0.62016\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.4349 - accuracy: 0.6223 - val_loss: 1.5360 - val_accuracy: 0.6168 - lr: 0.0010\n",
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4061 - accuracy: 0.6324\n",
      "Epoch 00022: val_accuracy improved from 0.62016 to 0.62298, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 111ms/step - loss: 1.4121 - accuracy: 0.6307 - val_loss: 1.4796 - val_accuracy: 0.6230 - lr: 0.0010\n",
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3701 - accuracy: 0.6377\n",
      "Epoch 00023: val_accuracy improved from 0.62298 to 0.63580, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 1.3722 - accuracy: 0.6375 - val_loss: 1.4607 - val_accuracy: 0.6358 - lr: 0.0010\n",
      "Epoch 24/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3610 - accuracy: 0.6456\n",
      "Epoch 00024: val_accuracy improved from 0.63580 to 0.64247, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 115ms/step - loss: 1.3610 - accuracy: 0.6456 - val_loss: 1.4450 - val_accuracy: 0.6425 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3285 - accuracy: 0.6557\n",
      "Epoch 00025: val_accuracy did not improve from 0.64247\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.3290 - accuracy: 0.6559 - val_loss: 1.4776 - val_accuracy: 0.6289 - lr: 0.0010\n",
      "Epoch 26/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2978 - accuracy: 0.6584\n",
      "Epoch 00026: val_accuracy improved from 0.64247 to 0.64581, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.2978 - accuracy: 0.6584 - val_loss: 1.4244 - val_accuracy: 0.6458 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3008 - accuracy: 0.6607\n",
      "Epoch 00027: val_accuracy improved from 0.64581 to 0.65247, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 1.3049 - accuracy: 0.6597 - val_loss: 1.4294 - val_accuracy: 0.6525 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2966 - accuracy: 0.6598\n",
      "Epoch 00028: val_accuracy improved from 0.65247 to 0.65709, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.2986 - accuracy: 0.6595 - val_loss: 1.4222 - val_accuracy: 0.6571 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2700 - accuracy: 0.6696\n",
      "Epoch 00029: val_accuracy did not improve from 0.65709\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.2699 - accuracy: 0.6694 - val_loss: 1.4184 - val_accuracy: 0.6520 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2567 - accuracy: 0.6756\n",
      "Epoch 00030: val_accuracy did not improve from 0.65709\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.2565 - accuracy: 0.6761 - val_loss: 1.4103 - val_accuracy: 0.6540 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2192 - accuracy: 0.6792\n",
      "Epoch 00031: val_accuracy improved from 0.65709 to 0.66709, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.2188 - accuracy: 0.6792 - val_loss: 1.3892 - val_accuracy: 0.6671 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2160 - accuracy: 0.6868\n",
      "Epoch 00032: val_accuracy did not improve from 0.66709\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.2150 - accuracy: 0.6870 - val_loss: 1.3998 - val_accuracy: 0.6568 - lr: 0.0010\n",
      "Epoch 33/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2077 - accuracy: 0.6845\n",
      "Epoch 00033: val_accuracy improved from 0.66709 to 0.66915, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.2077 - accuracy: 0.6845 - val_loss: 1.3796 - val_accuracy: 0.6691 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1696 - accuracy: 0.7020\n",
      "Epoch 00034: val_accuracy did not improve from 0.66915\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.1693 - accuracy: 0.7024 - val_loss: 1.3983 - val_accuracy: 0.6602 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1635 - accuracy: 0.7007\n",
      "Epoch 00035: val_accuracy did not improve from 0.66915\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 1.1645 - accuracy: 0.7006 - val_loss: 1.3986 - val_accuracy: 0.6589 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1575 - accuracy: 0.7017\n",
      "Epoch 00036: val_accuracy did not improve from 0.66915\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.1594 - accuracy: 0.7015 - val_loss: 1.4490 - val_accuracy: 0.6571 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1749 - accuracy: 0.6979\n",
      "Epoch 00037: val_accuracy improved from 0.66915 to 0.66940, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.1782 - accuracy: 0.6969 - val_loss: 1.3916 - val_accuracy: 0.6694 - lr: 0.0010\n",
      "Epoch 38/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1454 - accuracy: 0.7090\n",
      "Epoch 00038: val_accuracy did not improve from 0.66940\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.1454 - accuracy: 0.7090 - val_loss: 1.4110 - val_accuracy: 0.6681 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1314 - accuracy: 0.7118\n",
      "Epoch 00039: val_accuracy improved from 0.66940 to 0.67248, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.1322 - accuracy: 0.7115 - val_loss: 1.4059 - val_accuracy: 0.6725 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1028 - accuracy: 0.7121\n",
      "Epoch 00040: val_accuracy improved from 0.67248 to 0.67556, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.1019 - accuracy: 0.7126 - val_loss: 1.3767 - val_accuracy: 0.6756 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0727 - accuracy: 0.7261\n",
      "Epoch 00041: val_accuracy did not improve from 0.67556\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.0747 - accuracy: 0.7256 - val_loss: 1.3666 - val_accuracy: 0.6750 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0508 - accuracy: 0.7308\n",
      "Epoch 00042: val_accuracy improved from 0.67556 to 0.67684, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.0520 - accuracy: 0.7304 - val_loss: 1.3597 - val_accuracy: 0.6768 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0208 - accuracy: 0.7381\n",
      "Epoch 00043: val_accuracy improved from 0.67684 to 0.68146, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.0234 - accuracy: 0.7372 - val_loss: 1.3521 - val_accuracy: 0.6815 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0300 - accuracy: 0.7406\n",
      "Epoch 00044: val_accuracy did not improve from 0.68146\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.0293 - accuracy: 0.7410 - val_loss: 1.3470 - val_accuracy: 0.6802 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0116 - accuracy: 0.7433\n",
      "Epoch 00045: val_accuracy improved from 0.68146 to 0.68223, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 104ms/step - loss: 1.0114 - accuracy: 0.7429 - val_loss: 1.3430 - val_accuracy: 0.6822 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0052 - accuracy: 0.7426\n",
      "Epoch 00046: val_accuracy improved from 0.68223 to 0.68607, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.0071 - accuracy: 0.7419 - val_loss: 1.3411 - val_accuracy: 0.6861 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0009 - accuracy: 0.7469\n",
      "Epoch 00047: val_accuracy improved from 0.68607 to 0.68710, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.0025 - accuracy: 0.7468 - val_loss: 1.3357 - val_accuracy: 0.6871 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9978 - accuracy: 0.7469\n",
      "Epoch 00048: val_accuracy did not improve from 0.68710\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9983 - accuracy: 0.7458 - val_loss: 1.3375 - val_accuracy: 0.6866 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0093 - accuracy: 0.7417\n",
      "Epoch 00049: val_accuracy did not improve from 0.68710\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0080 - accuracy: 0.7418 - val_loss: 1.3392 - val_accuracy: 0.6827 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9967 - accuracy: 0.7490\n",
      "Epoch 00050: val_accuracy did not improve from 0.68710\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.9970 - accuracy: 0.7490 - val_loss: 1.3328 - val_accuracy: 0.6871 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9950 - accuracy: 0.7433\n",
      "Epoch 00051: val_accuracy improved from 0.68710 to 0.68889, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.9990 - accuracy: 0.7422 - val_loss: 1.3266 - val_accuracy: 0.6889 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9830 - accuracy: 0.7541\n",
      "Epoch 00052: val_accuracy did not improve from 0.68889\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9839 - accuracy: 0.7535 - val_loss: 1.3349 - val_accuracy: 0.6858 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9791 - accuracy: 0.7568\n",
      "Epoch 00053: val_accuracy did not improve from 0.68889\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.9807 - accuracy: 0.7563 - val_loss: 1.3230 - val_accuracy: 0.6868 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9724 - accuracy: 0.7519\n",
      "Epoch 00054: val_accuracy did not improve from 0.68889\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9761 - accuracy: 0.7510 - val_loss: 1.3281 - val_accuracy: 0.6871 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9833 - accuracy: 0.7514\n",
      "Epoch 00055: val_accuracy did not improve from 0.68889\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9837 - accuracy: 0.7518 - val_loss: 1.3306 - val_accuracy: 0.6876 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9574 - accuracy: 0.7659\n",
      "Epoch 00056: val_accuracy improved from 0.68889 to 0.68992, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.9592 - accuracy: 0.7649 - val_loss: 1.3235 - val_accuracy: 0.6899 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9672 - accuracy: 0.7590\n",
      "Epoch 00057: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.9672 - accuracy: 0.7590 - val_loss: 1.3222 - val_accuracy: 0.6892 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9626 - accuracy: 0.7554\n",
      "Epoch 00058: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.9639 - accuracy: 0.7554 - val_loss: 1.3325 - val_accuracy: 0.6876 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9647 - accuracy: 0.7596\n",
      "Epoch 00059: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.9653 - accuracy: 0.7593 - val_loss: 1.3218 - val_accuracy: 0.6868 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9479 - accuracy: 0.7611\n",
      "Epoch 00060: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9492 - accuracy: 0.7609 - val_loss: 1.3230 - val_accuracy: 0.6825 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9341 - accuracy: 0.7602\n",
      "Epoch 00061: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9351 - accuracy: 0.7600 - val_loss: 1.3253 - val_accuracy: 0.6861 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9564 - accuracy: 0.7599\n",
      "Epoch 00062: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9568 - accuracy: 0.7596 - val_loss: 1.3235 - val_accuracy: 0.6856 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9416 - accuracy: 0.7646\n",
      "Epoch 00063: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9426 - accuracy: 0.7646 - val_loss: 1.3232 - val_accuracy: 0.6825 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9559 - accuracy: 0.7544\n",
      "Epoch 00064: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.9559 - accuracy: 0.7544 - val_loss: 1.3213 - val_accuracy: 0.6868 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9382 - accuracy: 0.7646\n",
      "Epoch 00065: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.9392 - accuracy: 0.7644 - val_loss: 1.3271 - val_accuracy: 0.6861 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9336 - accuracy: 0.7589\n",
      "Epoch 00066: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.9347 - accuracy: 0.7584 - val_loss: 1.3208 - val_accuracy: 0.6884 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9380 - accuracy: 0.7627\n",
      "Epoch 00067: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9407 - accuracy: 0.7616 - val_loss: 1.3252 - val_accuracy: 0.6848 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9357 - accuracy: 0.7658\n",
      "Epoch 00068: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9376 - accuracy: 0.7647 - val_loss: 1.3219 - val_accuracy: 0.6822 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9327 - accuracy: 0.7604\n",
      "Epoch 00069: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9330 - accuracy: 0.7607 - val_loss: 1.3291 - val_accuracy: 0.6827 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9427 - accuracy: 0.7594\n",
      "Epoch 00070: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9422 - accuracy: 0.7594 - val_loss: 1.3320 - val_accuracy: 0.6825 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9144 - accuracy: 0.7681\n",
      "Epoch 00071: val_accuracy did not improve from 0.68992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 69ms/step - loss: 0.9141 - accuracy: 0.7682 - val_loss: 1.3234 - val_accuracy: 0.6845 - lr: 1.0000e-04\n",
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9123 - accuracy: 0.7689\n",
      "Epoch 00072: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9138 - accuracy: 0.7685 - val_loss: 1.3272 - val_accuracy: 0.6858 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9278 - accuracy: 0.7668\n",
      "Epoch 00073: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9262 - accuracy: 0.7676 - val_loss: 1.3280 - val_accuracy: 0.6881 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9092 - accuracy: 0.7732\n",
      "Epoch 00074: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.9091 - accuracy: 0.7732 - val_loss: 1.3205 - val_accuracy: 0.6874 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9067 - accuracy: 0.7744\n",
      "Epoch 00075: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9085 - accuracy: 0.7738 - val_loss: 1.3293 - val_accuracy: 0.6825 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9036 - accuracy: 0.7738\n",
      "Epoch 00076: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9036 - accuracy: 0.7738 - val_loss: 1.3259 - val_accuracy: 0.6881 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8979 - accuracy: 0.7699\n",
      "Epoch 00077: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.9000 - accuracy: 0.7684 - val_loss: 1.3199 - val_accuracy: 0.6886 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9159 - accuracy: 0.7680\n",
      "Epoch 00078: val_accuracy did not improve from 0.68992\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9169 - accuracy: 0.7677 - val_loss: 1.3247 - val_accuracy: 0.6899 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9146 - accuracy: 0.7710\n",
      "Epoch 00079: val_accuracy improved from 0.68992 to 0.69300, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 0.9162 - accuracy: 0.7703 - val_loss: 1.3188 - val_accuracy: 0.6930 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9135 - accuracy: 0.7692\n",
      "Epoch 00080: val_accuracy did not improve from 0.69300\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.9127 - accuracy: 0.7691 - val_loss: 1.3185 - val_accuracy: 0.6915 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9046 - accuracy: 0.7706\n",
      "Epoch 00081: val_accuracy did not improve from 0.69300\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.9048 - accuracy: 0.7706 - val_loss: 1.3126 - val_accuracy: 0.6912 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9072 - accuracy: 0.7693\n",
      "Epoch 00082: val_accuracy did not improve from 0.69300\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.9100 - accuracy: 0.7690 - val_loss: 1.3154 - val_accuracy: 0.6930 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9095 - accuracy: 0.7714\n",
      "Epoch 00083: val_accuracy did not improve from 0.69300\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9094 - accuracy: 0.7715 - val_loss: 1.3140 - val_accuracy: 0.6907 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8983 - accuracy: 0.7737\n",
      "Epoch 00084: val_accuracy did not improve from 0.69300\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8983 - accuracy: 0.7737 - val_loss: 1.3133 - val_accuracy: 0.6927 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8884 - accuracy: 0.7777\n",
      "Epoch 00085: val_accuracy improved from 0.69300 to 0.69377, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 0.8897 - accuracy: 0.7773 - val_loss: 1.3107 - val_accuracy: 0.6938 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8815 - accuracy: 0.7831\n",
      "Epoch 00086: val_accuracy improved from 0.69377 to 0.69582, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.8830 - accuracy: 0.7829 - val_loss: 1.3078 - val_accuracy: 0.6958 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8910 - accuracy: 0.7763\n",
      "Epoch 00087: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8925 - accuracy: 0.7753 - val_loss: 1.3128 - val_accuracy: 0.6927 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8930 - accuracy: 0.7733\n",
      "Epoch 00088: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8919 - accuracy: 0.7736 - val_loss: 1.3134 - val_accuracy: 0.6912 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8879 - accuracy: 0.7768\n",
      "Epoch 00089: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8877 - accuracy: 0.7770 - val_loss: 1.3109 - val_accuracy: 0.6935 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8881 - accuracy: 0.7739\n",
      "Epoch 00090: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8881 - accuracy: 0.7739 - val_loss: 1.3142 - val_accuracy: 0.6912 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8973 - accuracy: 0.7701\n",
      "Epoch 00091: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8973 - accuracy: 0.7701 - val_loss: 1.3169 - val_accuracy: 0.6938 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8741 - accuracy: 0.7847\n",
      "Epoch 00092: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8796 - accuracy: 0.7837 - val_loss: 1.3169 - val_accuracy: 0.6925 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8855 - accuracy: 0.7793\n",
      "Epoch 00093: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8885 - accuracy: 0.7785 - val_loss: 1.3167 - val_accuracy: 0.6881 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8863 - accuracy: 0.7786\n",
      "Epoch 00094: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8878 - accuracy: 0.7784 - val_loss: 1.3102 - val_accuracy: 0.6927 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9015 - accuracy: 0.7730\n",
      "Epoch 00095: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9003 - accuracy: 0.7732 - val_loss: 1.3130 - val_accuracy: 0.6922 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8815 - accuracy: 0.7814\n",
      "Epoch 00096: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.8830 - accuracy: 0.7809 - val_loss: 1.3149 - val_accuracy: 0.6922 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8803 - accuracy: 0.7756\n",
      "Epoch 00097: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8800 - accuracy: 0.7758 - val_loss: 1.3159 - val_accuracy: 0.6904 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8879 - accuracy: 0.7753\n",
      "Epoch 00098: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8880 - accuracy: 0.7750 - val_loss: 1.3136 - val_accuracy: 0.6912 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8800 - accuracy: 0.7817\n",
      "Epoch 00099: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8833 - accuracy: 0.7803 - val_loss: 1.3132 - val_accuracy: 0.6915 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8961 - accuracy: 0.7757\n",
      "Epoch 00100: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8961 - accuracy: 0.7757 - val_loss: 1.3146 - val_accuracy: 0.6912 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8825 - accuracy: 0.7806\n",
      "Epoch 00101: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8843 - accuracy: 0.7803 - val_loss: 1.3168 - val_accuracy: 0.6897 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8987 - accuracy: 0.7753\n",
      "Epoch 00102: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9001 - accuracy: 0.7748 - val_loss: 1.3159 - val_accuracy: 0.6935 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8940 - accuracy: 0.7799\n",
      "Epoch 00103: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8955 - accuracy: 0.7790 - val_loss: 1.3143 - val_accuracy: 0.6912 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8832 - accuracy: 0.7782\n",
      "Epoch 00104: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8867 - accuracy: 0.7772 - val_loss: 1.3128 - val_accuracy: 0.6897 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8776 - accuracy: 0.7783\n",
      "Epoch 00105: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8792 - accuracy: 0.7783 - val_loss: 1.3095 - val_accuracy: 0.6925 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8927 - accuracy: 0.7807\n",
      "Epoch 00106: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8923 - accuracy: 0.7804 - val_loss: 1.3076 - val_accuracy: 0.6933 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8822 - accuracy: 0.7777\n",
      "Epoch 00107: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8839 - accuracy: 0.7776 - val_loss: 1.3130 - val_accuracy: 0.6889 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8877 - accuracy: 0.7749\n",
      "Epoch 00108: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8877 - accuracy: 0.7749 - val_loss: 1.3129 - val_accuracy: 0.6922 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8863 - accuracy: 0.7769\n",
      "Epoch 00109: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8860 - accuracy: 0.7770 - val_loss: 1.3112 - val_accuracy: 0.6940 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8969 - accuracy: 0.7780\n",
      "Epoch 00110: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8968 - accuracy: 0.7779 - val_loss: 1.3121 - val_accuracy: 0.6886 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8971 - accuracy: 0.7739\n",
      "Epoch 00111: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9004 - accuracy: 0.7733 - val_loss: 1.3127 - val_accuracy: 0.6879 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8878 - accuracy: 0.7784\n",
      "Epoch 00112: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8901 - accuracy: 0.7779 - val_loss: 1.3128 - val_accuracy: 0.6904 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8804 - accuracy: 0.7814\n",
      "Epoch 00113: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8818 - accuracy: 0.7813 - val_loss: 1.3118 - val_accuracy: 0.6943 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8957 - accuracy: 0.7727\n",
      "Epoch 00114: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8975 - accuracy: 0.7725 - val_loss: 1.3127 - val_accuracy: 0.6879 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8754 - accuracy: 0.7762\n",
      "Epoch 00115: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8755 - accuracy: 0.7760 - val_loss: 1.3079 - val_accuracy: 0.6915 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8717 - accuracy: 0.7848\n",
      "Epoch 00116: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8750 - accuracy: 0.7834 - val_loss: 1.3133 - val_accuracy: 0.6892 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8817 - accuracy: 0.7770\n",
      "Epoch 00117: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8828 - accuracy: 0.7769 - val_loss: 1.3132 - val_accuracy: 0.6904 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8693 - accuracy: 0.7822\n",
      "Epoch 00118: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.8714 - accuracy: 0.7816 - val_loss: 1.3113 - val_accuracy: 0.6907 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8893 - accuracy: 0.7761\n",
      "Epoch 00119: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8893 - accuracy: 0.7761 - val_loss: 1.3117 - val_accuracy: 0.6927 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8851 - accuracy: 0.7793\n",
      "Epoch 00120: val_accuracy did not improve from 0.69582\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8867 - accuracy: 0.7788 - val_loss: 1.3118 - val_accuracy: 0.6909 - lr: 1.0000e-06\n",
      "epoch_number 86\n",
      "train accuracy and validation accuracy 0.7829431891441345 0.6958194375038147\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.3078 - accuracy: 0.6958\n",
      "test_accuracy 0.6958194375038147\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087, 0.7748140692710876, 0.7055655121803284, 0.7168504595756531, 0.6958194375038147]\n",
      "0.7470462441444397\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S16_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S16_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 5.2830 - accuracy: 0.0641\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.17697, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 383ms/step - loss: 5.2830 - accuracy: 0.0641 - val_loss: 4.0036 - val_accuracy: 0.1770 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.8186 - accuracy: 0.1228\n",
      "Epoch 00002: val_accuracy improved from 0.17697 to 0.21980, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 3.8158 - accuracy: 0.1228 - val_loss: 3.3348 - val_accuracy: 0.2198 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.4235 - accuracy: 0.1914\n",
      "Epoch 00003: val_accuracy improved from 0.21980 to 0.27315, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 3.4225 - accuracy: 0.1913 - val_loss: 3.0388 - val_accuracy: 0.2731 - lr: 0.0010\n",
      "Epoch 4/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 3.0031 - accuracy: 0.2685\n",
      "Epoch 00004: val_accuracy improved from 0.27315 to 0.35548, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 3.0031 - accuracy: 0.2685 - val_loss: 2.6717 - val_accuracy: 0.3555 - lr: 0.0010\n",
      "Epoch 5/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.6704 - accuracy: 0.3277\n",
      "Epoch 00005: val_accuracy improved from 0.35548 to 0.42729, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 2.6704 - accuracy: 0.3277 - val_loss: 2.3455 - val_accuracy: 0.4273 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4111 - accuracy: 0.3810\n",
      "Epoch 00006: val_accuracy improved from 0.42729 to 0.47884, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 2.4096 - accuracy: 0.3818 - val_loss: 2.1055 - val_accuracy: 0.4788 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2088 - accuracy: 0.4340\n",
      "Epoch 00007: val_accuracy improved from 0.47884 to 0.52834, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 2.2081 - accuracy: 0.4346 - val_loss: 1.9023 - val_accuracy: 0.5283 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0512 - accuracy: 0.4658\n",
      "Epoch 00008: val_accuracy improved from 0.52834 to 0.55758, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 2.0479 - accuracy: 0.4661 - val_loss: 1.7753 - val_accuracy: 0.5576 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9342 - accuracy: 0.4997\n",
      "Epoch 00009: val_accuracy improved from 0.55758 to 0.59272, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.9342 - accuracy: 0.4997 - val_loss: 1.6853 - val_accuracy: 0.5927 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8248 - accuracy: 0.5307\n",
      "Epoch 00010: val_accuracy improved from 0.59272 to 0.59810, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.8295 - accuracy: 0.5303 - val_loss: 1.6467 - val_accuracy: 0.5981 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7418 - accuracy: 0.5528\n",
      "Epoch 00011: val_accuracy improved from 0.59810 to 0.62401, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.7451 - accuracy: 0.5523 - val_loss: 1.5457 - val_accuracy: 0.6240 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6699 - accuracy: 0.5641\n",
      "Epoch 00012: val_accuracy improved from 0.62401 to 0.62734, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 1.6684 - accuracy: 0.5638 - val_loss: 1.5177 - val_accuracy: 0.6273 - lr: 0.0010\n",
      "Epoch 13/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.6058 - accuracy: 0.5865\n",
      "Epoch 00013: val_accuracy improved from 0.62734 to 0.64068, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.6058 - accuracy: 0.5865 - val_loss: 1.4636 - val_accuracy: 0.6407 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5562 - accuracy: 0.6049\n",
      "Epoch 00014: val_accuracy improved from 0.64068 to 0.65222, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.5576 - accuracy: 0.6047 - val_loss: 1.4139 - val_accuracy: 0.6522 - lr: 0.0010\n",
      "Epoch 15/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.4943 - accuracy: 0.6091\n",
      "Epoch 00015: val_accuracy improved from 0.65222 to 0.65863, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.4943 - accuracy: 0.6091 - val_loss: 1.4027 - val_accuracy: 0.6586 - lr: 0.0010\n",
      "Epoch 16/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.4446 - accuracy: 0.6341\n",
      "Epoch 00016: val_accuracy improved from 0.65863 to 0.66915, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.4446 - accuracy: 0.6341 - val_loss: 1.3590 - val_accuracy: 0.6691 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4114 - accuracy: 0.6337\n",
      "Epoch 00017: val_accuracy improved from 0.66915 to 0.67351, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.4105 - accuracy: 0.6337 - val_loss: 1.3485 - val_accuracy: 0.6735 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3625 - accuracy: 0.6523\n",
      "Epoch 00018: val_accuracy improved from 0.67351 to 0.67761, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.3674 - accuracy: 0.6515 - val_loss: 1.3173 - val_accuracy: 0.6776 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3412 - accuracy: 0.6610\n",
      "Epoch 00019: val_accuracy did not improve from 0.67761\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.3444 - accuracy: 0.6595 - val_loss: 1.3098 - val_accuracy: 0.6758 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3131 - accuracy: 0.6563\n",
      "Epoch 00020: val_accuracy improved from 0.67761 to 0.68659, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.3117 - accuracy: 0.6568 - val_loss: 1.2971 - val_accuracy: 0.6866 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2936 - accuracy: 0.6676\n",
      "Epoch 00021: val_accuracy improved from 0.68659 to 0.69479, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 106ms/step - loss: 1.2983 - accuracy: 0.6658 - val_loss: 1.2644 - val_accuracy: 0.6948 - lr: 0.0010\n",
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2523 - accuracy: 0.6766\n",
      "Epoch 00022: val_accuracy improved from 0.69479 to 0.70197, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.2520 - accuracy: 0.6767 - val_loss: 1.2484 - val_accuracy: 0.7020 - lr: 0.0010\n",
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2293 - accuracy: 0.6847\n",
      "Epoch 00023: val_accuracy improved from 0.70197 to 0.70582, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 1.2308 - accuracy: 0.6837 - val_loss: 1.2373 - val_accuracy: 0.7058 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2231 - accuracy: 0.6881\n",
      "Epoch 00024: val_accuracy did not improve from 0.70582\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.2242 - accuracy: 0.6877 - val_loss: 1.2280 - val_accuracy: 0.7058 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1933 - accuracy: 0.6946\n",
      "Epoch 00025: val_accuracy did not improve from 0.70582\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.1938 - accuracy: 0.6941 - val_loss: 1.2301 - val_accuracy: 0.7035 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1621 - accuracy: 0.7041\n",
      "Epoch 00026: val_accuracy improved from 0.70582 to 0.71044, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.1624 - accuracy: 0.7043 - val_loss: 1.2177 - val_accuracy: 0.7104 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1517 - accuracy: 0.7050\n",
      "Epoch 00027: val_accuracy improved from 0.71044 to 0.71788, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 134ms/step - loss: 1.1524 - accuracy: 0.7048 - val_loss: 1.1936 - val_accuracy: 0.7179 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1236 - accuracy: 0.7170\n",
      "Epoch 00028: val_accuracy did not improve from 0.71788\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 1.1271 - accuracy: 0.7161 - val_loss: 1.1985 - val_accuracy: 0.7133 - lr: 0.0010\n",
      "Epoch 29/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1031 - accuracy: 0.7188\n",
      "Epoch 00029: val_accuracy did not improve from 0.71788\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.1031 - accuracy: 0.7188 - val_loss: 1.1974 - val_accuracy: 0.7074 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0862 - accuracy: 0.7218\n",
      "Epoch 00030: val_accuracy did not improve from 0.71788\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.0883 - accuracy: 0.7211 - val_loss: 1.1817 - val_accuracy: 0.7145 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0695 - accuracy: 0.7324\n",
      "Epoch 00031: val_accuracy improved from 0.71788 to 0.72301, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.0701 - accuracy: 0.7321 - val_loss: 1.1627 - val_accuracy: 0.7230 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0416 - accuracy: 0.7367\n",
      "Epoch 00032: val_accuracy did not improve from 0.72301\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0416 - accuracy: 0.7363 - val_loss: 1.1851 - val_accuracy: 0.7163 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0383 - accuracy: 0.7369\n",
      "Epoch 00033: val_accuracy did not improve from 0.72301\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0399 - accuracy: 0.7367 - val_loss: 1.1851 - val_accuracy: 0.7194 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0255 - accuracy: 0.7416\n",
      "Epoch 00034: val_accuracy did not improve from 0.72301\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.0288 - accuracy: 0.7412 - val_loss: 1.1892 - val_accuracy: 0.7125 - lr: 0.0010\n",
      "Epoch 35/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0165 - accuracy: 0.7406\n",
      "Epoch 00035: val_accuracy improved from 0.72301 to 0.72890, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.0165 - accuracy: 0.7406 - val_loss: 1.1430 - val_accuracy: 0.7289 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9879 - accuracy: 0.7517\n",
      "Epoch 00036: val_accuracy did not improve from 0.72890\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9873 - accuracy: 0.7514 - val_loss: 1.1809 - val_accuracy: 0.7181 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9775 - accuracy: 0.7566\n",
      "Epoch 00037: val_accuracy improved from 0.72890 to 0.72993, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.9814 - accuracy: 0.7552 - val_loss: 1.1460 - val_accuracy: 0.7299 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9748 - accuracy: 0.7570\n",
      "Epoch 00038: val_accuracy did not improve from 0.72993\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.9754 - accuracy: 0.7566 - val_loss: 1.1706 - val_accuracy: 0.7251 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9523 - accuracy: 0.7603\n",
      "Epoch 00039: val_accuracy did not improve from 0.72993\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9549 - accuracy: 0.7592 - val_loss: 1.1482 - val_accuracy: 0.7284 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9159 - accuracy: 0.7721\n",
      "Epoch 00040: val_accuracy did not improve from 0.72993\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.9169 - accuracy: 0.7714 - val_loss: 1.1396 - val_accuracy: 0.7286 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9075 - accuracy: 0.7753\n",
      "Epoch 00041: val_accuracy improved from 0.72993 to 0.73147, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.9075 - accuracy: 0.7753 - val_loss: 1.1310 - val_accuracy: 0.7315 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8706 - accuracy: 0.7846\n",
      "Epoch 00042: val_accuracy improved from 0.73147 to 0.73557, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.8731 - accuracy: 0.7835 - val_loss: 1.1168 - val_accuracy: 0.7356 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8689 - accuracy: 0.7873\n",
      "Epoch 00043: val_accuracy did not improve from 0.73557\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8689 - accuracy: 0.7874 - val_loss: 1.1110 - val_accuracy: 0.7345 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8623 - accuracy: 0.7895\n",
      "Epoch 00044: val_accuracy improved from 0.73557 to 0.73763, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 0.8623 - accuracy: 0.7895 - val_loss: 1.1085 - val_accuracy: 0.7376 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8452 - accuracy: 0.7931\n",
      "Epoch 00045: val_accuracy improved from 0.73763 to 0.73865, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 105ms/step - loss: 0.8473 - accuracy: 0.7922 - val_loss: 1.1079 - val_accuracy: 0.7387 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8553 - accuracy: 0.7931\n",
      "Epoch 00046: val_accuracy improved from 0.73865 to 0.74019, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.8553 - accuracy: 0.7931 - val_loss: 1.1103 - val_accuracy: 0.7402 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8439 - accuracy: 0.7932\n",
      "Epoch 00047: val_accuracy did not improve from 0.74019\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8442 - accuracy: 0.7928 - val_loss: 1.1043 - val_accuracy: 0.7402 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8452 - accuracy: 0.7880\n",
      "Epoch 00048: val_accuracy did not improve from 0.74019\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.8485 - accuracy: 0.7869 - val_loss: 1.0990 - val_accuracy: 0.7394 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8204 - accuracy: 0.8048\n",
      "Epoch 00049: val_accuracy improved from 0.74019 to 0.74070, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 0.8228 - accuracy: 0.8045 - val_loss: 1.0985 - val_accuracy: 0.7407 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8241 - accuracy: 0.8008\n",
      "Epoch 00050: val_accuracy did not improve from 0.74070\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8261 - accuracy: 0.8006 - val_loss: 1.0965 - val_accuracy: 0.7399 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8341 - accuracy: 0.7949\n",
      "Epoch 00051: val_accuracy improved from 0.74070 to 0.74147, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.8342 - accuracy: 0.7948 - val_loss: 1.0900 - val_accuracy: 0.7415 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8188 - accuracy: 0.8047\n",
      "Epoch 00052: val_accuracy did not improve from 0.74147\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8189 - accuracy: 0.8043 - val_loss: 1.0917 - val_accuracy: 0.7397 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8157 - accuracy: 0.8007\n",
      "Epoch 00053: val_accuracy did not improve from 0.74147\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8157 - accuracy: 0.8007 - val_loss: 1.0931 - val_accuracy: 0.7404 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8172 - accuracy: 0.8036\n",
      "Epoch 00054: val_accuracy did not improve from 0.74147\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.8172 - accuracy: 0.8036 - val_loss: 1.0874 - val_accuracy: 0.7389 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8010 - accuracy: 0.8027\n",
      "Epoch 00055: val_accuracy improved from 0.74147 to 0.74224, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 0.8010 - accuracy: 0.8027 - val_loss: 1.0925 - val_accuracy: 0.7422 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8040 - accuracy: 0.8069\n",
      "Epoch 00056: val_accuracy improved from 0.74224 to 0.74404, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.8038 - accuracy: 0.8073 - val_loss: 1.0913 - val_accuracy: 0.7440 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8046 - accuracy: 0.8021\n",
      "Epoch 00057: val_accuracy improved from 0.74404 to 0.74481, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 0.8034 - accuracy: 0.8025 - val_loss: 1.0851 - val_accuracy: 0.7448 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8000 - accuracy: 0.8079\n",
      "Epoch 00058: val_accuracy improved from 0.74481 to 0.74686, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.8004 - accuracy: 0.8076 - val_loss: 1.0904 - val_accuracy: 0.7469 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7872 - accuracy: 0.8088\n",
      "Epoch 00059: val_accuracy improved from 0.74686 to 0.74763, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.7872 - accuracy: 0.8088 - val_loss: 1.0839 - val_accuracy: 0.7476 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7990 - accuracy: 0.8076\n",
      "Epoch 00060: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7998 - accuracy: 0.8073 - val_loss: 1.0880 - val_accuracy: 0.7433 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7882 - accuracy: 0.8120\n",
      "Epoch 00061: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7903 - accuracy: 0.8107 - val_loss: 1.0876 - val_accuracy: 0.7435 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7879 - accuracy: 0.8098\n",
      "Epoch 00062: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7879 - accuracy: 0.8098 - val_loss: 1.0935 - val_accuracy: 0.7430 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7900 - accuracy: 0.8071\n",
      "Epoch 00063: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7903 - accuracy: 0.8069 - val_loss: 1.0879 - val_accuracy: 0.7435 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7835 - accuracy: 0.8116\n",
      "Epoch 00064: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7837 - accuracy: 0.8114 - val_loss: 1.0860 - val_accuracy: 0.7469 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7683 - accuracy: 0.8134\n",
      "Epoch 00065: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7701 - accuracy: 0.8132 - val_loss: 1.0888 - val_accuracy: 0.7440 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7606 - accuracy: 0.8190\n",
      "Epoch 00066: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7612 - accuracy: 0.8193 - val_loss: 1.0865 - val_accuracy: 0.7415 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7543 - accuracy: 0.8150\n",
      "Epoch 00067: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7566 - accuracy: 0.8143 - val_loss: 1.0860 - val_accuracy: 0.7433 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7726 - accuracy: 0.8117\n",
      "Epoch 00068: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7776 - accuracy: 0.8105 - val_loss: 1.0915 - val_accuracy: 0.7399 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7791 - accuracy: 0.8096\n",
      "Epoch 00069: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7791 - accuracy: 0.8096 - val_loss: 1.0855 - val_accuracy: 0.7410 - lr: 1.0000e-04\n",
      "Epoch 70/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 0.7583 - accuracy: 0.8188\n",
      "Epoch 00070: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.7583 - accuracy: 0.8188 - val_loss: 1.0777 - val_accuracy: 0.7422 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7702 - accuracy: 0.8129\n",
      "Epoch 00071: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7722 - accuracy: 0.8125 - val_loss: 1.0842 - val_accuracy: 0.7422 - lr: 1.0000e-04\n",
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7659 - accuracy: 0.8128\n",
      "Epoch 00072: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.7670 - accuracy: 0.8125 - val_loss: 1.0835 - val_accuracy: 0.7456 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7572 - accuracy: 0.8173\n",
      "Epoch 00073: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.7587 - accuracy: 0.8167 - val_loss: 1.0849 - val_accuracy: 0.7456 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7564 - accuracy: 0.8153\n",
      "Epoch 00074: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7559 - accuracy: 0.8153 - val_loss: 1.0811 - val_accuracy: 0.7433 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7514 - accuracy: 0.8200\n",
      "Epoch 00075: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7540 - accuracy: 0.8193 - val_loss: 1.0870 - val_accuracy: 0.7420 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7498 - accuracy: 0.8191\n",
      "Epoch 00076: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7528 - accuracy: 0.8183 - val_loss: 1.0876 - val_accuracy: 0.7410 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7482 - accuracy: 0.8202\n",
      "Epoch 00077: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7482 - accuracy: 0.8202 - val_loss: 1.0844 - val_accuracy: 0.7430 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7395 - accuracy: 0.8239\n",
      "Epoch 00078: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.7403 - accuracy: 0.8235 - val_loss: 1.0737 - val_accuracy: 0.7420 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7349 - accuracy: 0.8197\n",
      "Epoch 00079: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.7380 - accuracy: 0.8187 - val_loss: 1.0744 - val_accuracy: 0.7456 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7372 - accuracy: 0.8233\n",
      "Epoch 00080: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.7365 - accuracy: 0.8232 - val_loss: 1.0782 - val_accuracy: 0.7451 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7394 - accuracy: 0.8219\n",
      "Epoch 00081: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7403 - accuracy: 0.8214 - val_loss: 1.0768 - val_accuracy: 0.7458 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7486 - accuracy: 0.8168\n",
      "Epoch 00082: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7486 - accuracy: 0.8168 - val_loss: 1.0762 - val_accuracy: 0.7440 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7364 - accuracy: 0.8182\n",
      "Epoch 00083: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7364 - accuracy: 0.8182 - val_loss: 1.0741 - val_accuracy: 0.7425 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7351 - accuracy: 0.8229\n",
      "Epoch 00084: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7356 - accuracy: 0.8231 - val_loss: 1.0763 - val_accuracy: 0.7443 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7514 - accuracy: 0.8163\n",
      "Epoch 00085: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7516 - accuracy: 0.8166 - val_loss: 1.0762 - val_accuracy: 0.7445 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7420 - accuracy: 0.8198\n",
      "Epoch 00086: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7428 - accuracy: 0.8194 - val_loss: 1.0759 - val_accuracy: 0.7466 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7381 - accuracy: 0.8206\n",
      "Epoch 00087: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7381 - accuracy: 0.8206 - val_loss: 1.0748 - val_accuracy: 0.7456 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7344 - accuracy: 0.8201\n",
      "Epoch 00088: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7359 - accuracy: 0.8198 - val_loss: 1.0759 - val_accuracy: 0.7463 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7264 - accuracy: 0.8282\n",
      "Epoch 00089: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7298 - accuracy: 0.8269 - val_loss: 1.0782 - val_accuracy: 0.7458 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7253 - accuracy: 0.8281\n",
      "Epoch 00090: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7253 - accuracy: 0.8281 - val_loss: 1.0773 - val_accuracy: 0.7420 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7152 - accuracy: 0.8292\n",
      "Epoch 00091: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7177 - accuracy: 0.8282 - val_loss: 1.0750 - val_accuracy: 0.7448 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7309 - accuracy: 0.8256\n",
      "Epoch 00092: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7324 - accuracy: 0.8251 - val_loss: 1.0775 - val_accuracy: 0.7443 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7301 - accuracy: 0.8271\n",
      "Epoch 00093: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7303 - accuracy: 0.8267 - val_loss: 1.0767 - val_accuracy: 0.7461 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7282 - accuracy: 0.8248\n",
      "Epoch 00094: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7282 - accuracy: 0.8248 - val_loss: 1.0783 - val_accuracy: 0.7451 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7333 - accuracy: 0.8224\n",
      "Epoch 00095: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.7346 - accuracy: 0.8224 - val_loss: 1.0804 - val_accuracy: 0.7448 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7369 - accuracy: 0.8228\n",
      "Epoch 00096: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7376 - accuracy: 0.8222 - val_loss: 1.0749 - val_accuracy: 0.7433 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7185 - accuracy: 0.8260\n",
      "Epoch 00097: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7230 - accuracy: 0.8250 - val_loss: 1.0767 - val_accuracy: 0.7425 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7192 - accuracy: 0.8296\n",
      "Epoch 00098: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7192 - accuracy: 0.8293 - val_loss: 1.0760 - val_accuracy: 0.7440 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7277 - accuracy: 0.8226\n",
      "Epoch 00099: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.7306 - accuracy: 0.8218 - val_loss: 1.0736 - val_accuracy: 0.7469 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7176 - accuracy: 0.8326\n",
      "Epoch 00100: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.7185 - accuracy: 0.8320 - val_loss: 1.0766 - val_accuracy: 0.7448 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7244 - accuracy: 0.8253\n",
      "Epoch 00101: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7278 - accuracy: 0.8247 - val_loss: 1.0787 - val_accuracy: 0.7440 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7164 - accuracy: 0.8323\n",
      "Epoch 00102: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7148 - accuracy: 0.8328 - val_loss: 1.0779 - val_accuracy: 0.7448 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7288 - accuracy: 0.8258\n",
      "Epoch 00103: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7280 - accuracy: 0.8258 - val_loss: 1.0769 - val_accuracy: 0.7451 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7365 - accuracy: 0.8241\n",
      "Epoch 00104: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7377 - accuracy: 0.8244 - val_loss: 1.0806 - val_accuracy: 0.7438 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7281 - accuracy: 0.8269\n",
      "Epoch 00105: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.7298 - accuracy: 0.8260 - val_loss: 1.0765 - val_accuracy: 0.7456 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7241 - accuracy: 0.8276\n",
      "Epoch 00106: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7247 - accuracy: 0.8277 - val_loss: 1.0778 - val_accuracy: 0.7451 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7415 - accuracy: 0.8198\n",
      "Epoch 00107: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7414 - accuracy: 0.8200 - val_loss: 1.0788 - val_accuracy: 0.7458 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7450 - accuracy: 0.8198\n",
      "Epoch 00108: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7450 - accuracy: 0.8198 - val_loss: 1.0809 - val_accuracy: 0.7430 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7223 - accuracy: 0.8248\n",
      "Epoch 00109: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7240 - accuracy: 0.8244 - val_loss: 1.0770 - val_accuracy: 0.7415 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7257 - accuracy: 0.8272\n",
      "Epoch 00110: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7270 - accuracy: 0.8269 - val_loss: 1.0800 - val_accuracy: 0.7440 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7214 - accuracy: 0.8316\n",
      "Epoch 00111: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7214 - accuracy: 0.8316 - val_loss: 1.0806 - val_accuracy: 0.7430 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7273 - accuracy: 0.8254\n",
      "Epoch 00112: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7301 - accuracy: 0.8244 - val_loss: 1.0817 - val_accuracy: 0.7451 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7162 - accuracy: 0.8269\n",
      "Epoch 00113: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.7178 - accuracy: 0.8257 - val_loss: 1.0730 - val_accuracy: 0.7445 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7211 - accuracy: 0.8287\n",
      "Epoch 00114: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7211 - accuracy: 0.8287 - val_loss: 1.0801 - val_accuracy: 0.7443 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7141 - accuracy: 0.8280\n",
      "Epoch 00115: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7175 - accuracy: 0.8269 - val_loss: 1.0789 - val_accuracy: 0.7440 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7229 - accuracy: 0.8292\n",
      "Epoch 00116: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7257 - accuracy: 0.8280 - val_loss: 1.0759 - val_accuracy: 0.7448 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7142 - accuracy: 0.8289\n",
      "Epoch 00117: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.7165 - accuracy: 0.8278 - val_loss: 1.0728 - val_accuracy: 0.7428 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7176 - accuracy: 0.8321\n",
      "Epoch 00118: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.7204 - accuracy: 0.8311 - val_loss: 1.0722 - val_accuracy: 0.7440 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7271 - accuracy: 0.8282\n",
      "Epoch 00119: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7271 - accuracy: 0.8282 - val_loss: 1.0738 - val_accuracy: 0.7428 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7234 - accuracy: 0.8286\n",
      "Epoch 00120: val_accuracy did not improve from 0.74763\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7256 - accuracy: 0.8278 - val_loss: 1.0735 - val_accuracy: 0.7448 - lr: 1.0000e-06\n",
      "epoch_number 59\n",
      "train accuracy and validation accuracy 0.8087701797485352 0.7476276159286499\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.0839 - accuracy: 0.7476\n",
      "test_accuracy 0.7476276159286499\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087, 0.7748140692710876, 0.7055655121803284, 0.7168504595756531, 0.6958194375038147, 0.7476276159286499]\n",
      "0.7470825798809528\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S17_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S17_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.3329 - accuracy: 0.1458\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.27828, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 359ms/step - loss: 4.3196 - accuracy: 0.1469 - val_loss: 3.1914 - val_accuracy: 0.2783 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.9121 - accuracy: 0.2687\n",
      "Epoch 00002: val_accuracy improved from 0.27828 to 0.37189, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 2.9127 - accuracy: 0.2687 - val_loss: 2.4764 - val_accuracy: 0.3719 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.5558 - accuracy: 0.3440\n",
      "Epoch 00003: val_accuracy improved from 0.37189 to 0.46627, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 2.5566 - accuracy: 0.3447 - val_loss: 2.1889 - val_accuracy: 0.4663 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.3074 - accuracy: 0.4092\n",
      "Epoch 00004: val_accuracy improved from 0.46627 to 0.49500, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 2.3064 - accuracy: 0.4097 - val_loss: 2.0174 - val_accuracy: 0.4950 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1205 - accuracy: 0.4551\n",
      "Epoch 00005: val_accuracy improved from 0.49500 to 0.52680, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 2.1213 - accuracy: 0.4550 - val_loss: 1.8991 - val_accuracy: 0.5268 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0202 - accuracy: 0.4817\n",
      "Epoch 00006: val_accuracy improved from 0.52680 to 0.55963, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 2.0190 - accuracy: 0.4825 - val_loss: 1.7843 - val_accuracy: 0.5596 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8972 - accuracy: 0.5106\n",
      "Epoch 00007: val_accuracy improved from 0.55963 to 0.56604, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 1.8953 - accuracy: 0.5109 - val_loss: 1.7102 - val_accuracy: 0.5660 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8205 - accuracy: 0.5288\n",
      "Epoch 00008: val_accuracy improved from 0.56604 to 0.58477, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.8241 - accuracy: 0.5281 - val_loss: 1.6708 - val_accuracy: 0.5848 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7337 - accuracy: 0.5618\n",
      "Epoch 00009: val_accuracy improved from 0.58477 to 0.59451, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.7343 - accuracy: 0.5612 - val_loss: 1.6142 - val_accuracy: 0.5945 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6764 - accuracy: 0.5732\n",
      "Epoch 00010: val_accuracy improved from 0.59451 to 0.61375, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.6777 - accuracy: 0.5725 - val_loss: 1.5563 - val_accuracy: 0.6137 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6271 - accuracy: 0.5830\n",
      "Epoch 00011: val_accuracy improved from 0.61375 to 0.62401, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.6267 - accuracy: 0.5828 - val_loss: 1.5045 - val_accuracy: 0.6240 - lr: 0.0010\n",
      "Epoch 12/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.5721 - accuracy: 0.5996\n",
      "Epoch 00012: val_accuracy improved from 0.62401 to 0.62888, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.5721 - accuracy: 0.5996 - val_loss: 1.4702 - val_accuracy: 0.6289 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5084 - accuracy: 0.6182\n",
      "Epoch 00013: val_accuracy improved from 0.62888 to 0.63324, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.5079 - accuracy: 0.6189 - val_loss: 1.4542 - val_accuracy: 0.6332 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4887 - accuracy: 0.6198\n",
      "Epoch 00014: val_accuracy improved from 0.63324 to 0.63939, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.4869 - accuracy: 0.6198 - val_loss: 1.4307 - val_accuracy: 0.6394 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4591 - accuracy: 0.6301\n",
      "Epoch 00015: val_accuracy improved from 0.63939 to 0.64760, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 1.4655 - accuracy: 0.6275 - val_loss: 1.4049 - val_accuracy: 0.6476 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4276 - accuracy: 0.6370\n",
      "Epoch 00016: val_accuracy improved from 0.64760 to 0.64965, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.4298 - accuracy: 0.6361 - val_loss: 1.4029 - val_accuracy: 0.6497 - lr: 0.0010\n",
      "Epoch 17/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.4098 - accuracy: 0.6411\n",
      "Epoch 00017: val_accuracy improved from 0.64965 to 0.65555, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.4098 - accuracy: 0.6411 - val_loss: 1.4009 - val_accuracy: 0.6556 - lr: 0.0010\n",
      "Epoch 18/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.3513 - accuracy: 0.6526\n",
      "Epoch 00018: val_accuracy improved from 0.65555 to 0.66761, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.3513 - accuracy: 0.6526 - val_loss: 1.3480 - val_accuracy: 0.6676 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3282 - accuracy: 0.6664\n",
      "Epoch 00019: val_accuracy improved from 0.66761 to 0.67068, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 1.3295 - accuracy: 0.6666 - val_loss: 1.3310 - val_accuracy: 0.6707 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3007 - accuracy: 0.6681\n",
      "Epoch 00020: val_accuracy improved from 0.67068 to 0.67171, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1.3019 - accuracy: 0.6683 - val_loss: 1.3341 - val_accuracy: 0.6717 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2929 - accuracy: 0.6733\n",
      "Epoch 00021: val_accuracy improved from 0.67171 to 0.67607, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 106ms/step - loss: 1.2943 - accuracy: 0.6732 - val_loss: 1.3065 - val_accuracy: 0.6761 - lr: 0.0010\n",
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2703 - accuracy: 0.6809\n",
      "Epoch 00022: val_accuracy improved from 0.67607 to 0.68197, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 1.2701 - accuracy: 0.6811 - val_loss: 1.2953 - val_accuracy: 0.6820 - lr: 0.0010\n",
      "Epoch 23/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2443 - accuracy: 0.6881\n",
      "Epoch 00023: val_accuracy did not improve from 0.68197\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 1.2443 - accuracy: 0.6881 - val_loss: 1.2931 - val_accuracy: 0.6799 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2153 - accuracy: 0.6920\n",
      "Epoch 00024: val_accuracy did not improve from 0.68197\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.2146 - accuracy: 0.6923 - val_loss: 1.2986 - val_accuracy: 0.6779 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1944 - accuracy: 0.6997\n",
      "Epoch 00025: val_accuracy improved from 0.68197 to 0.68402, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.1990 - accuracy: 0.6992 - val_loss: 1.2741 - val_accuracy: 0.6840 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1806 - accuracy: 0.7032\n",
      "Epoch 00026: val_accuracy improved from 0.68402 to 0.69223, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 1.1843 - accuracy: 0.7029 - val_loss: 1.2607 - val_accuracy: 0.6922 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1704 - accuracy: 0.7061\n",
      "Epoch 00027: val_accuracy improved from 0.69223 to 0.69633, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 130ms/step - loss: 1.1747 - accuracy: 0.7056 - val_loss: 1.2516 - val_accuracy: 0.6963 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1498 - accuracy: 0.7112\n",
      "Epoch 00028: val_accuracy did not improve from 0.69633\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 1.1500 - accuracy: 0.7111 - val_loss: 1.2482 - val_accuracy: 0.6863 - lr: 0.0010\n",
      "Epoch 29/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1182 - accuracy: 0.7208\n",
      "Epoch 00029: val_accuracy did not improve from 0.69633\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.1182 - accuracy: 0.7208 - val_loss: 1.2829 - val_accuracy: 0.6848 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1165 - accuracy: 0.7238\n",
      "Epoch 00030: val_accuracy did not improve from 0.69633\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.1184 - accuracy: 0.7232 - val_loss: 1.2622 - val_accuracy: 0.6909 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1059 - accuracy: 0.7270\n",
      "Epoch 00031: val_accuracy improved from 0.69633 to 0.70069, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.1076 - accuracy: 0.7270 - val_loss: 1.2490 - val_accuracy: 0.7007 - lr: 0.0010\n",
      "Epoch 32/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1089 - accuracy: 0.7230\n",
      "Epoch 00032: val_accuracy improved from 0.70069 to 0.70172, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 1.1089 - accuracy: 0.7230 - val_loss: 1.2325 - val_accuracy: 0.7017 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0894 - accuracy: 0.7276\n",
      "Epoch 00033: val_accuracy did not improve from 0.70172\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0938 - accuracy: 0.7266 - val_loss: 1.2884 - val_accuracy: 0.6745 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0738 - accuracy: 0.7362\n",
      "Epoch 00034: val_accuracy did not improve from 0.70172\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0741 - accuracy: 0.7360 - val_loss: 1.2708 - val_accuracy: 0.6833 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0843 - accuracy: 0.7293\n",
      "Epoch 00035: val_accuracy did not improve from 0.70172\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 1.0853 - accuracy: 0.7289 - val_loss: 1.2473 - val_accuracy: 0.6956 - lr: 0.0010\n",
      "Epoch 36/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0632 - accuracy: 0.7357\n",
      "Epoch 00036: val_accuracy did not improve from 0.70172\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.0632 - accuracy: 0.7357 - val_loss: 1.2294 - val_accuracy: 0.6979 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0397 - accuracy: 0.7402\n",
      "Epoch 00037: val_accuracy improved from 0.70172 to 0.70351, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 1.0383 - accuracy: 0.7406 - val_loss: 1.2333 - val_accuracy: 0.7035 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0288 - accuracy: 0.7459\n",
      "Epoch 00038: val_accuracy did not improve from 0.70351\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.0265 - accuracy: 0.7466 - val_loss: 1.2250 - val_accuracy: 0.6984 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9956 - accuracy: 0.7557\n",
      "Epoch 00039: val_accuracy did not improve from 0.70351\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9949 - accuracy: 0.7558 - val_loss: 1.2273 - val_accuracy: 0.7012 - lr: 0.0010\n",
      "Epoch 40/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9683 - accuracy: 0.7615\n",
      "Epoch 00040: val_accuracy improved from 0.70351 to 0.70659, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 0.9683 - accuracy: 0.7615 - val_loss: 1.2015 - val_accuracy: 0.7066 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9385 - accuracy: 0.7710\n",
      "Epoch 00041: val_accuracy improved from 0.70659 to 0.71146, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.9385 - accuracy: 0.7710 - val_loss: 1.1903 - val_accuracy: 0.7115 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9271 - accuracy: 0.7776\n",
      "Epoch 00042: val_accuracy did not improve from 0.71146\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.9267 - accuracy: 0.7778 - val_loss: 1.1830 - val_accuracy: 0.7094 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9324 - accuracy: 0.7758\n",
      "Epoch 00043: val_accuracy did not improve from 0.71146\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.9324 - accuracy: 0.7758 - val_loss: 1.2115 - val_accuracy: 0.7017 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9105 - accuracy: 0.7838\n",
      "Epoch 00044: val_accuracy improved from 0.71146 to 0.71377, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.9125 - accuracy: 0.7833 - val_loss: 1.1884 - val_accuracy: 0.7138 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9170 - accuracy: 0.7787\n",
      "Epoch 00045: val_accuracy did not improve from 0.71377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 83ms/step - loss: 0.9187 - accuracy: 0.7782 - val_loss: 1.1785 - val_accuracy: 0.7133 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9110 - accuracy: 0.7809\n",
      "Epoch 00046: val_accuracy improved from 0.71377 to 0.71429, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 0.9110 - accuracy: 0.7809 - val_loss: 1.1802 - val_accuracy: 0.7143 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8987 - accuracy: 0.7811\n",
      "Epoch 00047: val_accuracy improved from 0.71429 to 0.71582, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.8987 - accuracy: 0.7811 - val_loss: 1.1774 - val_accuracy: 0.7158 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8970 - accuracy: 0.7848\n",
      "Epoch 00048: val_accuracy did not improve from 0.71582\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.8989 - accuracy: 0.7840 - val_loss: 1.1736 - val_accuracy: 0.7158 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8928 - accuracy: 0.7856\n",
      "Epoch 00049: val_accuracy improved from 0.71582 to 0.71711, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.8928 - accuracy: 0.7856 - val_loss: 1.1730 - val_accuracy: 0.7171 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8908 - accuracy: 0.7836\n",
      "Epoch 00050: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8913 - accuracy: 0.7838 - val_loss: 1.1724 - val_accuracy: 0.7161 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8909 - accuracy: 0.7823\n",
      "Epoch 00051: val_accuracy did not improve from 0.71711\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.8905 - accuracy: 0.7823 - val_loss: 1.1711 - val_accuracy: 0.7122 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8782 - accuracy: 0.7859\n",
      "Epoch 00052: val_accuracy improved from 0.71711 to 0.71865, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.8785 - accuracy: 0.7862 - val_loss: 1.1716 - val_accuracy: 0.7186 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8798 - accuracy: 0.7882\n",
      "Epoch 00053: val_accuracy did not improve from 0.71865\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8808 - accuracy: 0.7880 - val_loss: 1.1723 - val_accuracy: 0.7158 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8744 - accuracy: 0.7887\n",
      "Epoch 00054: val_accuracy did not improve from 0.71865\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8748 - accuracy: 0.7887 - val_loss: 1.1722 - val_accuracy: 0.7151 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8835 - accuracy: 0.7860\n",
      "Epoch 00055: val_accuracy did not improve from 0.71865\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8847 - accuracy: 0.7857 - val_loss: 1.1731 - val_accuracy: 0.7186 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8879 - accuracy: 0.7876\n",
      "Epoch 00056: val_accuracy did not improve from 0.71865\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.8899 - accuracy: 0.7870 - val_loss: 1.1707 - val_accuracy: 0.7179 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8712 - accuracy: 0.7849\n",
      "Epoch 00057: val_accuracy improved from 0.71865 to 0.71967, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 0.8737 - accuracy: 0.7842 - val_loss: 1.1665 - val_accuracy: 0.7197 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8635 - accuracy: 0.7918\n",
      "Epoch 00058: val_accuracy did not improve from 0.71967\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8631 - accuracy: 0.7920 - val_loss: 1.1670 - val_accuracy: 0.7192 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8560 - accuracy: 0.7931\n",
      "Epoch 00059: val_accuracy did not improve from 0.71967\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8557 - accuracy: 0.7933 - val_loss: 1.1689 - val_accuracy: 0.7169 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8668 - accuracy: 0.7907\n",
      "Epoch 00060: val_accuracy did not improve from 0.71967\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8670 - accuracy: 0.7905 - val_loss: 1.1676 - val_accuracy: 0.7192 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8619 - accuracy: 0.7911\n",
      "Epoch 00061: val_accuracy improved from 0.71967 to 0.72121, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.8641 - accuracy: 0.7900 - val_loss: 1.1655 - val_accuracy: 0.7212 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8526 - accuracy: 0.7938\n",
      "Epoch 00062: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8535 - accuracy: 0.7932 - val_loss: 1.1652 - val_accuracy: 0.7174 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8445 - accuracy: 0.7978\n",
      "Epoch 00063: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8489 - accuracy: 0.7960 - val_loss: 1.1659 - val_accuracy: 0.7179 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8455 - accuracy: 0.7943\n",
      "Epoch 00064: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8471 - accuracy: 0.7944 - val_loss: 1.1687 - val_accuracy: 0.7166 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8367 - accuracy: 0.8001\n",
      "Epoch 00065: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8357 - accuracy: 0.8003 - val_loss: 1.1669 - val_accuracy: 0.7194 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8494 - accuracy: 0.7974\n",
      "Epoch 00066: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8499 - accuracy: 0.7976 - val_loss: 1.1655 - val_accuracy: 0.7181 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8305 - accuracy: 0.8015\n",
      "Epoch 00067: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 0.8305 - accuracy: 0.8015 - val_loss: 1.1613 - val_accuracy: 0.7207 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8336 - accuracy: 0.7983\n",
      "Epoch 00068: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8335 - accuracy: 0.7984 - val_loss: 1.1628 - val_accuracy: 0.7192 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8346 - accuracy: 0.8005\n",
      "Epoch 00069: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8346 - accuracy: 0.8005 - val_loss: 1.1630 - val_accuracy: 0.7189 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8335 - accuracy: 0.7988\n",
      "Epoch 00070: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8335 - accuracy: 0.7988 - val_loss: 1.1719 - val_accuracy: 0.7138 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8252 - accuracy: 0.8002\n",
      "Epoch 00071: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8257 - accuracy: 0.7998 - val_loss: 1.1645 - val_accuracy: 0.7171 - lr: 1.0000e-04\n",
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8314 - accuracy: 0.7972\n",
      "Epoch 00072: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8303 - accuracy: 0.7978 - val_loss: 1.1637 - val_accuracy: 0.7161 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8283 - accuracy: 0.8016\n",
      "Epoch 00073: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8273 - accuracy: 0.8021 - val_loss: 1.1656 - val_accuracy: 0.7181 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8356 - accuracy: 0.7981\n",
      "Epoch 00074: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8356 - accuracy: 0.7981 - val_loss: 1.1626 - val_accuracy: 0.7189 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8177 - accuracy: 0.8037\n",
      "Epoch 00075: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8192 - accuracy: 0.8035 - val_loss: 1.1650 - val_accuracy: 0.7151 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8273 - accuracy: 0.8018\n",
      "Epoch 00076: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8289 - accuracy: 0.8011 - val_loss: 1.1621 - val_accuracy: 0.7151 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8056 - accuracy: 0.8076\n",
      "Epoch 00077: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8064 - accuracy: 0.8075 - val_loss: 1.1625 - val_accuracy: 0.7174 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8215 - accuracy: 0.7978\n",
      "Epoch 00078: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8239 - accuracy: 0.7969 - val_loss: 1.1666 - val_accuracy: 0.7186 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8128 - accuracy: 0.8094\n",
      "Epoch 00079: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.8139 - accuracy: 0.8092 - val_loss: 1.1583 - val_accuracy: 0.7189 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8125 - accuracy: 0.8061\n",
      "Epoch 00080: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.8130 - accuracy: 0.8059 - val_loss: 1.1576 - val_accuracy: 0.7194 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8041 - accuracy: 0.8093\n",
      "Epoch 00081: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.8069 - accuracy: 0.8086 - val_loss: 1.1556 - val_accuracy: 0.7204 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8119 - accuracy: 0.8069\n",
      "Epoch 00082: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8120 - accuracy: 0.8071 - val_loss: 1.1557 - val_accuracy: 0.7192 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8105 - accuracy: 0.8061\n",
      "Epoch 00083: val_accuracy did not improve from 0.72121\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.8105 - accuracy: 0.8061 - val_loss: 1.1543 - val_accuracy: 0.7199 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8126 - accuracy: 0.8058\n",
      "Epoch 00084: val_accuracy improved from 0.72121 to 0.72172, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.8135 - accuracy: 0.8051 - val_loss: 1.1555 - val_accuracy: 0.7217 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8126 - accuracy: 0.8043\n",
      "Epoch 00085: val_accuracy did not improve from 0.72172\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8142 - accuracy: 0.8038 - val_loss: 1.1564 - val_accuracy: 0.7207 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8075 - accuracy: 0.8031\n",
      "Epoch 00086: val_accuracy did not improve from 0.72172\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8077 - accuracy: 0.8032 - val_loss: 1.1546 - val_accuracy: 0.7197 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8052 - accuracy: 0.8069\n",
      "Epoch 00087: val_accuracy did not improve from 0.72172\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8052 - accuracy: 0.8069 - val_loss: 1.1551 - val_accuracy: 0.7197 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8051 - accuracy: 0.8053\n",
      "Epoch 00088: val_accuracy did not improve from 0.72172\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8076 - accuracy: 0.8048 - val_loss: 1.1564 - val_accuracy: 0.7207 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8010 - accuracy: 0.8077\n",
      "Epoch 00089: val_accuracy did not improve from 0.72172\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.7997 - accuracy: 0.8083 - val_loss: 1.1540 - val_accuracy: 0.7207 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7981 - accuracy: 0.8106\n",
      "Epoch 00090: val_accuracy did not improve from 0.72172\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7981 - accuracy: 0.8106 - val_loss: 1.1543 - val_accuracy: 0.7202 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8154 - accuracy: 0.8013\n",
      "Epoch 00091: val_accuracy did not improve from 0.72172\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8152 - accuracy: 0.8013 - val_loss: 1.1539 - val_accuracy: 0.7179 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8045 - accuracy: 0.8097\n",
      "Epoch 00092: val_accuracy did not improve from 0.72172\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 0.8095 - accuracy: 0.8086 - val_loss: 1.1537 - val_accuracy: 0.7202 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7952 - accuracy: 0.8106\n",
      "Epoch 00093: val_accuracy did not improve from 0.72172\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7971 - accuracy: 0.8100 - val_loss: 1.1546 - val_accuracy: 0.7194 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8038 - accuracy: 0.8061\n",
      "Epoch 00094: val_accuracy did not improve from 0.72172\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.8049 - accuracy: 0.8058 - val_loss: 1.1523 - val_accuracy: 0.7199 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8006 - accuracy: 0.8076\n",
      "Epoch 00095: val_accuracy did not improve from 0.72172\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8031 - accuracy: 0.8073 - val_loss: 1.1547 - val_accuracy: 0.7215 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8053 - accuracy: 0.8064\n",
      "Epoch 00096: val_accuracy improved from 0.72172 to 0.72249, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.8053 - accuracy: 0.8064 - val_loss: 1.1545 - val_accuracy: 0.7225 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8142 - accuracy: 0.8051\n",
      "Epoch 00097: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8142 - accuracy: 0.8049 - val_loss: 1.1539 - val_accuracy: 0.7204 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8074 - accuracy: 0.8057\n",
      "Epoch 00098: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8074 - accuracy: 0.8057 - val_loss: 1.1538 - val_accuracy: 0.7202 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8039 - accuracy: 0.8101\n",
      "Epoch 00099: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8040 - accuracy: 0.8099 - val_loss: 1.1542 - val_accuracy: 0.7194 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8047 - accuracy: 0.8081\n",
      "Epoch 00100: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8102 - accuracy: 0.8060 - val_loss: 1.1551 - val_accuracy: 0.7212 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8016 - accuracy: 0.8113\n",
      "Epoch 00101: val_accuracy did not improve from 0.72249\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.8013 - accuracy: 0.8115 - val_loss: 1.1539 - val_accuracy: 0.7202 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7897 - accuracy: 0.8104\n",
      "Epoch 00102: val_accuracy improved from 0.72249 to 0.72352, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.7897 - accuracy: 0.8104 - val_loss: 1.1540 - val_accuracy: 0.7235 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7898 - accuracy: 0.8102\n",
      "Epoch 00103: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.7905 - accuracy: 0.8100 - val_loss: 1.1520 - val_accuracy: 0.7207 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7971 - accuracy: 0.8118\n",
      "Epoch 00104: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7971 - accuracy: 0.8118 - val_loss: 1.1520 - val_accuracy: 0.7212 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7972 - accuracy: 0.8053\n",
      "Epoch 00105: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7972 - accuracy: 0.8053 - val_loss: 1.1542 - val_accuracy: 0.7207 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8006 - accuracy: 0.8133\n",
      "Epoch 00106: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8006 - accuracy: 0.8133 - val_loss: 1.1528 - val_accuracy: 0.7179 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8030 - accuracy: 0.8112\n",
      "Epoch 00107: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 0.8033 - accuracy: 0.8107 - val_loss: 1.1513 - val_accuracy: 0.7215 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7985 - accuracy: 0.8084\n",
      "Epoch 00108: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.7985 - accuracy: 0.8084 - val_loss: 1.1499 - val_accuracy: 0.7202 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7925 - accuracy: 0.8061\n",
      "Epoch 00109: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.7949 - accuracy: 0.8057 - val_loss: 1.1529 - val_accuracy: 0.7192 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7931 - accuracy: 0.8099\n",
      "Epoch 00110: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7931 - accuracy: 0.8099 - val_loss: 1.1704 - val_accuracy: 0.7127 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7991 - accuracy: 0.8041\n",
      "Epoch 00111: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.7980 - accuracy: 0.8048 - val_loss: 1.1557 - val_accuracy: 0.7194 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7913 - accuracy: 0.8120\n",
      "Epoch 00112: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7913 - accuracy: 0.8120 - val_loss: 1.1528 - val_accuracy: 0.7210 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7971 - accuracy: 0.8111\n",
      "Epoch 00113: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8014 - accuracy: 0.8098 - val_loss: 1.1548 - val_accuracy: 0.7176 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8025 - accuracy: 0.8083\n",
      "Epoch 00114: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8051 - accuracy: 0.8081 - val_loss: 1.1552 - val_accuracy: 0.7181 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8010 - accuracy: 0.8120\n",
      "Epoch 00115: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.8024 - accuracy: 0.8113 - val_loss: 1.1522 - val_accuracy: 0.7202 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7923 - accuracy: 0.8139\n",
      "Epoch 00116: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.7935 - accuracy: 0.8132 - val_loss: 1.1538 - val_accuracy: 0.7204 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7868 - accuracy: 0.8140\n",
      "Epoch 00117: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7902 - accuracy: 0.8133 - val_loss: 1.1535 - val_accuracy: 0.7212 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7909 - accuracy: 0.8151\n",
      "Epoch 00118: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7944 - accuracy: 0.8140 - val_loss: 1.1536 - val_accuracy: 0.7199 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7960 - accuracy: 0.8100\n",
      "Epoch 00119: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7960 - accuracy: 0.8100 - val_loss: 1.1513 - val_accuracy: 0.7207 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8096 - accuracy: 0.8066\n",
      "Epoch 00120: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8096 - accuracy: 0.8066 - val_loss: 1.1548 - val_accuracy: 0.7197 - lr: 1.0000e-06\n",
      "epoch_number 102\n",
      "train accuracy and validation accuracy 0.8104187250137329 0.7235188484191895\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.1540 - accuracy: 0.7235\n",
      "test_accuracy 0.7235188484191895\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087, 0.7748140692710876, 0.7055655121803284, 0.7168504595756531, 0.6958194375038147, 0.7476276159286499, 0.7235188484191895]\n",
      "0.7456964780302608\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S18_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S18_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 5.0190 - accuracy: 0.0921\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.18569, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 362ms/step - loss: 5.0190 - accuracy: 0.0921 - val_loss: 4.2002 - val_accuracy: 0.1857 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.3273 - accuracy: 0.1902\n",
      "Epoch 00002: val_accuracy improved from 0.18569 to 0.26802, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 3.3246 - accuracy: 0.1912 - val_loss: 2.9386 - val_accuracy: 0.2680 - lr: 0.0010\n",
      "Epoch 3/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.7880 - accuracy: 0.3025\n",
      "Epoch 00003: val_accuracy improved from 0.26802 to 0.39805, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 113ms/step - loss: 2.7880 - accuracy: 0.3025 - val_loss: 2.4543 - val_accuracy: 0.3981 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4313 - accuracy: 0.3791\n",
      "Epoch 00004: val_accuracy improved from 0.39805 to 0.44832, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 2.4321 - accuracy: 0.3786 - val_loss: 2.1832 - val_accuracy: 0.4483 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1871 - accuracy: 0.4406\n",
      "Epoch 00005: val_accuracy improved from 0.44832 to 0.51449, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 2.1888 - accuracy: 0.4402 - val_loss: 1.9508 - val_accuracy: 0.5145 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9978 - accuracy: 0.4826\n",
      "Epoch 00006: val_accuracy improved from 0.51449 to 0.55912, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.9992 - accuracy: 0.4820 - val_loss: 1.7525 - val_accuracy: 0.5591 - lr: 0.0010\n",
      "Epoch 7/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.8513 - accuracy: 0.5124\n",
      "Epoch 00007: val_accuracy improved from 0.55912 to 0.60118, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.8513 - accuracy: 0.5124 - val_loss: 1.6268 - val_accuracy: 0.6012 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7224 - accuracy: 0.5492\n",
      "Epoch 00008: val_accuracy improved from 0.60118 to 0.61939, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.7206 - accuracy: 0.5495 - val_loss: 1.5354 - val_accuracy: 0.6194 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6115 - accuracy: 0.5751\n",
      "Epoch 00009: val_accuracy improved from 0.61939 to 0.64452, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 1.6110 - accuracy: 0.5749 - val_loss: 1.4436 - val_accuracy: 0.6445 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5277 - accuracy: 0.5994\n",
      "Epoch 00010: val_accuracy improved from 0.64452 to 0.65196, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.5272 - accuracy: 0.5997 - val_loss: 1.3999 - val_accuracy: 0.6520 - lr: 0.0010\n",
      "Epoch 11/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.4540 - accuracy: 0.6204\n",
      "Epoch 00011: val_accuracy improved from 0.65196 to 0.66581, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.4540 - accuracy: 0.6204 - val_loss: 1.3163 - val_accuracy: 0.6658 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3763 - accuracy: 0.6418\n",
      "Epoch 00012: val_accuracy improved from 0.66581 to 0.68992, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.3758 - accuracy: 0.6418 - val_loss: 1.2584 - val_accuracy: 0.6899 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3126 - accuracy: 0.6592\n",
      "Epoch 00013: val_accuracy improved from 0.68992 to 0.69172, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.3100 - accuracy: 0.6596 - val_loss: 1.2207 - val_accuracy: 0.6917 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2552 - accuracy: 0.6754\n",
      "Epoch 00014: val_accuracy improved from 0.69172 to 0.70018, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.2543 - accuracy: 0.6757 - val_loss: 1.1859 - val_accuracy: 0.7002 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2155 - accuracy: 0.6880\n",
      "Epoch 00015: val_accuracy improved from 0.70018 to 0.70941, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 1.2139 - accuracy: 0.6884 - val_loss: 1.1639 - val_accuracy: 0.7094 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1694 - accuracy: 0.6989\n",
      "Epoch 00016: val_accuracy did not improve from 0.70941\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 1.1702 - accuracy: 0.6989 - val_loss: 1.1499 - val_accuracy: 0.7063 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1470 - accuracy: 0.7078\n",
      "Epoch 00017: val_accuracy improved from 0.70941 to 0.71788, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.1474 - accuracy: 0.7076 - val_loss: 1.1283 - val_accuracy: 0.7179 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1089 - accuracy: 0.7144\n",
      "Epoch 00018: val_accuracy improved from 0.71788 to 0.72326, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.1100 - accuracy: 0.7139 - val_loss: 1.1097 - val_accuracy: 0.7233 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0678 - accuracy: 0.7190\n",
      "Epoch 00019: val_accuracy improved from 0.72326 to 0.72660, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.0710 - accuracy: 0.7180 - val_loss: 1.0785 - val_accuracy: 0.7266 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0583 - accuracy: 0.7287\n",
      "Epoch 00020: val_accuracy did not improve from 0.72660\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0616 - accuracy: 0.7281 - val_loss: 1.1138 - val_accuracy: 0.7256 - lr: 0.0010\n",
      "Epoch 21/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0333 - accuracy: 0.7384\n",
      "Epoch 00021: val_accuracy improved from 0.72660 to 0.72788, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.0333 - accuracy: 0.7384 - val_loss: 1.1038 - val_accuracy: 0.7279 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0198 - accuracy: 0.7361\n",
      "Epoch 00022: val_accuracy improved from 0.72788 to 0.72890, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.0213 - accuracy: 0.7361 - val_loss: 1.0693 - val_accuracy: 0.7289 - lr: 0.0010\n",
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9663 - accuracy: 0.7556\n",
      "Epoch 00023: val_accuracy improved from 0.72890 to 0.74660, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 123ms/step - loss: 0.9667 - accuracy: 0.7555 - val_loss: 1.0273 - val_accuracy: 0.7466 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9607 - accuracy: 0.7559\n",
      "Epoch 00024: val_accuracy did not improve from 0.74660\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.9626 - accuracy: 0.7557 - val_loss: 1.0595 - val_accuracy: 0.7343 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9289 - accuracy: 0.7646\n",
      "Epoch 00025: val_accuracy improved from 0.74660 to 0.74763, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 0.9308 - accuracy: 0.7643 - val_loss: 1.0373 - val_accuracy: 0.7476 - lr: 0.0010\n",
      "Epoch 26/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9383 - accuracy: 0.7655\n",
      "Epoch 00026: val_accuracy improved from 0.74763 to 0.75404, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.9383 - accuracy: 0.7655 - val_loss: 1.0011 - val_accuracy: 0.7540 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9324 - accuracy: 0.7641\n",
      "Epoch 00027: val_accuracy improved from 0.75404 to 0.75583, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.9327 - accuracy: 0.7641 - val_loss: 1.0083 - val_accuracy: 0.7558 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8976 - accuracy: 0.7758\n",
      "Epoch 00028: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.9004 - accuracy: 0.7753 - val_loss: 1.0191 - val_accuracy: 0.7520 - lr: 0.0010\n",
      "Epoch 29/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9059 - accuracy: 0.7694\n",
      "Epoch 00029: val_accuracy did not improve from 0.75583\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9059 - accuracy: 0.7694 - val_loss: 1.0135 - val_accuracy: 0.7528 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8699 - accuracy: 0.7841\n",
      "Epoch 00030: val_accuracy improved from 0.75583 to 0.75891, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.8720 - accuracy: 0.7837 - val_loss: 0.9935 - val_accuracy: 0.7589 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8623 - accuracy: 0.7841\n",
      "Epoch 00031: val_accuracy improved from 0.75891 to 0.76404, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.8644 - accuracy: 0.7836 - val_loss: 0.9850 - val_accuracy: 0.7640 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8467 - accuracy: 0.7881\n",
      "Epoch 00032: val_accuracy did not improve from 0.76404\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.8488 - accuracy: 0.7874 - val_loss: 0.9808 - val_accuracy: 0.7615 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8236 - accuracy: 0.7938\n",
      "Epoch 00033: val_accuracy did not improve from 0.76404\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.8219 - accuracy: 0.7942 - val_loss: 0.9778 - val_accuracy: 0.7579 - lr: 0.0010\n",
      "Epoch 34/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8211 - accuracy: 0.7947\n",
      "Epoch 00034: val_accuracy did not improve from 0.76404\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8211 - accuracy: 0.7947 - val_loss: 1.0087 - val_accuracy: 0.7561 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7977 - accuracy: 0.8031\n",
      "Epoch 00035: val_accuracy improved from 0.76404 to 0.76661, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.7988 - accuracy: 0.8032 - val_loss: 0.9714 - val_accuracy: 0.7666 - lr: 0.0010\n",
      "Epoch 36/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7921 - accuracy: 0.8018\n",
      "Epoch 00036: val_accuracy did not improve from 0.76661\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7921 - accuracy: 0.8018 - val_loss: 0.9756 - val_accuracy: 0.7640 - lr: 0.0010\n",
      "Epoch 37/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7619 - accuracy: 0.8120\n",
      "Epoch 00037: val_accuracy improved from 0.76661 to 0.77789, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 0.7619 - accuracy: 0.8120 - val_loss: 0.9459 - val_accuracy: 0.7779 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7495 - accuracy: 0.8188\n",
      "Epoch 00038: val_accuracy did not improve from 0.77789\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7486 - accuracy: 0.8193 - val_loss: 0.9690 - val_accuracy: 0.7740 - lr: 0.0010\n",
      "Epoch 39/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7542 - accuracy: 0.8151\n",
      "Epoch 00039: val_accuracy did not improve from 0.77789\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7542 - accuracy: 0.8151 - val_loss: 0.9705 - val_accuracy: 0.7710 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7273 - accuracy: 0.8243\n",
      "Epoch 00040: val_accuracy improved from 0.77789 to 0.77969, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 108ms/step - loss: 0.7284 - accuracy: 0.8235 - val_loss: 0.9433 - val_accuracy: 0.7797 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7030 - accuracy: 0.8268\n",
      "Epoch 00041: val_accuracy improved from 0.77969 to 0.78020, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 0.7030 - accuracy: 0.8268 - val_loss: 0.9347 - val_accuracy: 0.7802 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6862 - accuracy: 0.8392\n",
      "Epoch 00042: val_accuracy improved from 0.78020 to 0.78712, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.6870 - accuracy: 0.8389 - val_loss: 0.9246 - val_accuracy: 0.7871 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6737 - accuracy: 0.8448\n",
      "Epoch 00043: val_accuracy improved from 0.78712 to 0.78789, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 0.6736 - accuracy: 0.8448 - val_loss: 0.9166 - val_accuracy: 0.7879 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6692 - accuracy: 0.8447\n",
      "Epoch 00044: val_accuracy did not improve from 0.78789\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.6692 - accuracy: 0.8447 - val_loss: 0.9175 - val_accuracy: 0.7864 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6608 - accuracy: 0.8431\n",
      "Epoch 00045: val_accuracy did not improve from 0.78789\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.6608 - accuracy: 0.8431 - val_loss: 0.9160 - val_accuracy: 0.7879 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6724 - accuracy: 0.8388\n",
      "Epoch 00046: val_accuracy did not improve from 0.78789\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.6716 - accuracy: 0.8387 - val_loss: 0.9132 - val_accuracy: 0.7871 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6666 - accuracy: 0.8397\n",
      "Epoch 00047: val_accuracy did not improve from 0.78789\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.6664 - accuracy: 0.8395 - val_loss: 0.9142 - val_accuracy: 0.7871 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6439 - accuracy: 0.8469\n",
      "Epoch 00048: val_accuracy did not improve from 0.78789\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.6451 - accuracy: 0.8466 - val_loss: 0.9102 - val_accuracy: 0.7871 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6474 - accuracy: 0.8457\n",
      "Epoch 00049: val_accuracy did not improve from 0.78789\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 0.6474 - accuracy: 0.8457 - val_loss: 0.9097 - val_accuracy: 0.7871 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6360 - accuracy: 0.8556\n",
      "Epoch 00050: val_accuracy did not improve from 0.78789\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.6384 - accuracy: 0.8548 - val_loss: 0.9137 - val_accuracy: 0.7869 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6461 - accuracy: 0.8449\n",
      "Epoch 00051: val_accuracy improved from 0.78789 to 0.78815, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 0.6461 - accuracy: 0.8449 - val_loss: 0.9078 - val_accuracy: 0.7882 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6392 - accuracy: 0.8538\n",
      "Epoch 00052: val_accuracy did not improve from 0.78815\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.6395 - accuracy: 0.8534 - val_loss: 0.9084 - val_accuracy: 0.7866 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6304 - accuracy: 0.8523\n",
      "Epoch 00053: val_accuracy improved from 0.78815 to 0.78918, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 0.6326 - accuracy: 0.8517 - val_loss: 0.9079 - val_accuracy: 0.7892 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6391 - accuracy: 0.8506\n",
      "Epoch 00054: val_accuracy did not improve from 0.78918\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.6394 - accuracy: 0.8504 - val_loss: 0.9059 - val_accuracy: 0.7876 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6234 - accuracy: 0.8531\n",
      "Epoch 00055: val_accuracy did not improve from 0.78918\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.6244 - accuracy: 0.8526 - val_loss: 0.9075 - val_accuracy: 0.7876 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6165 - accuracy: 0.8577\n",
      "Epoch 00056: val_accuracy did not improve from 0.78918\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.6190 - accuracy: 0.8565 - val_loss: 0.9130 - val_accuracy: 0.7882 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6273 - accuracy: 0.8559\n",
      "Epoch 00057: val_accuracy did not improve from 0.78918\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.6275 - accuracy: 0.8560 - val_loss: 0.9097 - val_accuracy: 0.7858 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6213 - accuracy: 0.8583\n",
      "Epoch 00058: val_accuracy improved from 0.78918 to 0.78969, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 112ms/step - loss: 0.6221 - accuracy: 0.8581 - val_loss: 0.9034 - val_accuracy: 0.7897 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6250 - accuracy: 0.8541\n",
      "Epoch 00059: val_accuracy did not improve from 0.78969\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.6272 - accuracy: 0.8536 - val_loss: 0.8987 - val_accuracy: 0.7897 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6226 - accuracy: 0.8548\n",
      "Epoch 00060: val_accuracy did not improve from 0.78969\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.6240 - accuracy: 0.8546 - val_loss: 0.9008 - val_accuracy: 0.7894 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6162 - accuracy: 0.8562\n",
      "Epoch 00061: val_accuracy did not improve from 0.78969\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.6184 - accuracy: 0.8555 - val_loss: 0.9020 - val_accuracy: 0.7884 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6228 - accuracy: 0.8561\n",
      "Epoch 00062: val_accuracy did not improve from 0.78969\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.6224 - accuracy: 0.8564 - val_loss: 0.9033 - val_accuracy: 0.7892 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6107 - accuracy: 0.8601\n",
      "Epoch 00063: val_accuracy did not improve from 0.78969\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.6121 - accuracy: 0.8597 - val_loss: 0.9059 - val_accuracy: 0.7869 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6138 - accuracy: 0.8572\n",
      "Epoch 00064: val_accuracy did not improve from 0.78969\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.6138 - accuracy: 0.8572 - val_loss: 0.9070 - val_accuracy: 0.7889 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6138 - accuracy: 0.8618\n",
      "Epoch 00065: val_accuracy did not improve from 0.78969\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.6141 - accuracy: 0.8614 - val_loss: 0.9034 - val_accuracy: 0.7861 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6118 - accuracy: 0.8557\n",
      "Epoch 00066: val_accuracy did not improve from 0.78969\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.6118 - accuracy: 0.8557 - val_loss: 0.9074 - val_accuracy: 0.7892 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6037 - accuracy: 0.8594\n",
      "Epoch 00067: val_accuracy improved from 0.78969 to 0.78995, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 0.6037 - accuracy: 0.8594 - val_loss: 0.9033 - val_accuracy: 0.7899 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6090 - accuracy: 0.8579\n",
      "Epoch 00068: val_accuracy did not improve from 0.78995\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6108 - accuracy: 0.8572 - val_loss: 0.9080 - val_accuracy: 0.7887 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5950 - accuracy: 0.8639\n",
      "Epoch 00069: val_accuracy did not improve from 0.78995\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5950 - accuracy: 0.8639 - val_loss: 0.9091 - val_accuracy: 0.7871 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5956 - accuracy: 0.8612\n",
      "Epoch 00070: val_accuracy did not improve from 0.78995\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5956 - accuracy: 0.8612 - val_loss: 0.9027 - val_accuracy: 0.7897 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5885 - accuracy: 0.8633\n",
      "Epoch 00071: val_accuracy improved from 0.78995 to 0.79020, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 0.5892 - accuracy: 0.8634 - val_loss: 0.8961 - val_accuracy: 0.7902 - lr: 1.0000e-04\n",
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6039 - accuracy: 0.8589\n",
      "Epoch 00072: val_accuracy improved from 0.79020 to 0.79251, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.6048 - accuracy: 0.8582 - val_loss: 0.8969 - val_accuracy: 0.7925 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5933 - accuracy: 0.8599\n",
      "Epoch 00073: val_accuracy did not improve from 0.79251\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5957 - accuracy: 0.8593 - val_loss: 0.8968 - val_accuracy: 0.7910 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5886 - accuracy: 0.8651\n",
      "Epoch 00074: val_accuracy did not improve from 0.79251\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5888 - accuracy: 0.8648 - val_loss: 0.8986 - val_accuracy: 0.7907 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5896 - accuracy: 0.8628\n",
      "Epoch 00075: val_accuracy improved from 0.79251 to 0.79636, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 0.5896 - accuracy: 0.8628 - val_loss: 0.8954 - val_accuracy: 0.7964 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5861 - accuracy: 0.8642\n",
      "Epoch 00076: val_accuracy improved from 0.79636 to 0.79815, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.5881 - accuracy: 0.8636 - val_loss: 0.8964 - val_accuracy: 0.7982 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5764 - accuracy: 0.8696\n",
      "Epoch 00077: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5768 - accuracy: 0.8692 - val_loss: 0.8994 - val_accuracy: 0.7946 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5937 - accuracy: 0.8619\n",
      "Epoch 00078: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5937 - accuracy: 0.8619 - val_loss: 0.8988 - val_accuracy: 0.7889 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5813 - accuracy: 0.8687\n",
      "Epoch 00079: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5826 - accuracy: 0.8680 - val_loss: 0.8992 - val_accuracy: 0.7902 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5800 - accuracy: 0.8680\n",
      "Epoch 00080: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5806 - accuracy: 0.8677 - val_loss: 0.8990 - val_accuracy: 0.7887 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5698 - accuracy: 0.8707\n",
      "Epoch 00081: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5701 - accuracy: 0.8701 - val_loss: 0.8961 - val_accuracy: 0.7930 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5874 - accuracy: 0.8653\n",
      "Epoch 00082: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5874 - accuracy: 0.8653 - val_loss: 0.9020 - val_accuracy: 0.7912 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5792 - accuracy: 0.8676\n",
      "Epoch 00083: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.5792 - accuracy: 0.8676 - val_loss: 0.8988 - val_accuracy: 0.7943 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5772 - accuracy: 0.8693\n",
      "Epoch 00084: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5772 - accuracy: 0.8693 - val_loss: 0.8988 - val_accuracy: 0.7933 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5770 - accuracy: 0.8676\n",
      "Epoch 00085: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5781 - accuracy: 0.8673 - val_loss: 0.9013 - val_accuracy: 0.7902 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5797 - accuracy: 0.8668\n",
      "Epoch 00086: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.5806 - accuracy: 0.8662 - val_loss: 0.8995 - val_accuracy: 0.7905 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5734 - accuracy: 0.8714\n",
      "Epoch 00087: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5752 - accuracy: 0.8712 - val_loss: 0.8982 - val_accuracy: 0.7907 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5658 - accuracy: 0.8713\n",
      "Epoch 00088: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.5680 - accuracy: 0.8704 - val_loss: 0.8964 - val_accuracy: 0.7920 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5678 - accuracy: 0.8698\n",
      "Epoch 00089: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5688 - accuracy: 0.8700 - val_loss: 0.9019 - val_accuracy: 0.7930 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5716 - accuracy: 0.8684\n",
      "Epoch 00090: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5743 - accuracy: 0.8679 - val_loss: 0.8986 - val_accuracy: 0.7920 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5711 - accuracy: 0.8680\n",
      "Epoch 00091: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 0.5728 - accuracy: 0.8675 - val_loss: 0.8952 - val_accuracy: 0.7948 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5718 - accuracy: 0.8686\n",
      "Epoch 00092: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5732 - accuracy: 0.8683 - val_loss: 0.8973 - val_accuracy: 0.7907 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5639 - accuracy: 0.8743\n",
      "Epoch 00093: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5650 - accuracy: 0.8741 - val_loss: 0.8959 - val_accuracy: 0.7928 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5742 - accuracy: 0.8709\n",
      "Epoch 00094: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.5742 - accuracy: 0.8709 - val_loss: 0.8939 - val_accuracy: 0.7920 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5672 - accuracy: 0.8733\n",
      "Epoch 00095: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.5676 - accuracy: 0.8734 - val_loss: 0.9003 - val_accuracy: 0.7912 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5706 - accuracy: 0.8721\n",
      "Epoch 00096: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5714 - accuracy: 0.8717 - val_loss: 0.8993 - val_accuracy: 0.7951 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5722 - accuracy: 0.8706\n",
      "Epoch 00097: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5730 - accuracy: 0.8701 - val_loss: 0.8960 - val_accuracy: 0.7920 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5659 - accuracy: 0.8732\n",
      "Epoch 00098: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5660 - accuracy: 0.8731 - val_loss: 0.8964 - val_accuracy: 0.7925 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5669 - accuracy: 0.8730\n",
      "Epoch 00099: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5669 - accuracy: 0.8730 - val_loss: 0.8947 - val_accuracy: 0.7964 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5644 - accuracy: 0.8708\n",
      "Epoch 00100: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5660 - accuracy: 0.8702 - val_loss: 0.8955 - val_accuracy: 0.7938 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5703 - accuracy: 0.8707\n",
      "Epoch 00101: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.5713 - accuracy: 0.8702 - val_loss: 0.8971 - val_accuracy: 0.7907 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5774 - accuracy: 0.8670\n",
      "Epoch 00102: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5773 - accuracy: 0.8671 - val_loss: 0.8952 - val_accuracy: 0.7930 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5756 - accuracy: 0.8669\n",
      "Epoch 00103: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5756 - accuracy: 0.8669 - val_loss: 0.8981 - val_accuracy: 0.7933 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5619 - accuracy: 0.8752\n",
      "Epoch 00104: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5629 - accuracy: 0.8744 - val_loss: 0.8988 - val_accuracy: 0.7902 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5643 - accuracy: 0.8737\n",
      "Epoch 00105: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.5651 - accuracy: 0.8733 - val_loss: 0.8939 - val_accuracy: 0.7935 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5623 - accuracy: 0.8707\n",
      "Epoch 00106: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5647 - accuracy: 0.8702 - val_loss: 0.8966 - val_accuracy: 0.7938 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5537 - accuracy: 0.8753\n",
      "Epoch 00107: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 0.5584 - accuracy: 0.8742 - val_loss: 0.8926 - val_accuracy: 0.7933 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5637 - accuracy: 0.8730\n",
      "Epoch 00108: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.5646 - accuracy: 0.8726 - val_loss: 0.8944 - val_accuracy: 0.7948 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5770 - accuracy: 0.86 - ETA: 0s - loss: 0.5774 - accuracy: 0.8666\n",
      "Epoch 00109: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.5790 - accuracy: 0.8658 - val_loss: 0.8934 - val_accuracy: 0.7940 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5572 - accuracy: 0.8700\n",
      "Epoch 00110: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5584 - accuracy: 0.8699 - val_loss: 0.8965 - val_accuracy: 0.7938 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5649 - accuracy: 0.8714\n",
      "Epoch 00111: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5673 - accuracy: 0.8705 - val_loss: 0.8958 - val_accuracy: 0.7953 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5704 - accuracy: 0.8657\n",
      "Epoch 00112: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5731 - accuracy: 0.8652 - val_loss: 0.8979 - val_accuracy: 0.7928 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5668 - accuracy: 0.8728\n",
      "Epoch 00113: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5686 - accuracy: 0.8722 - val_loss: 0.8965 - val_accuracy: 0.7958 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5647 - accuracy: 0.8760\n",
      "Epoch 00114: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5664 - accuracy: 0.8753 - val_loss: 0.8985 - val_accuracy: 0.7923 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5727 - accuracy: 0.8709\n",
      "Epoch 00115: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 69ms/step - loss: 0.5741 - accuracy: 0.8703 - val_loss: 0.8967 - val_accuracy: 0.7938 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5622 - accuracy: 0.8716\n",
      "Epoch 00116: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.5645 - accuracy: 0.8702 - val_loss: 0.8978 - val_accuracy: 0.7938 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5608 - accuracy: 0.8700\n",
      "Epoch 00117: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5608 - accuracy: 0.8700 - val_loss: 0.9001 - val_accuracy: 0.7943 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5621 - accuracy: 0.8732\n",
      "Epoch 00118: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5633 - accuracy: 0.8730 - val_loss: 0.8992 - val_accuracy: 0.7940 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5686 - accuracy: 0.8737\n",
      "Epoch 00119: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.5686 - accuracy: 0.8737 - val_loss: 0.8925 - val_accuracy: 0.7938 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.5567 - accuracy: 0.8768\n",
      "Epoch 00120: val_accuracy did not improve from 0.79815\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 0.5583 - accuracy: 0.8760 - val_loss: 0.9000 - val_accuracy: 0.7923 - lr: 1.0000e-06\n",
      "epoch_number 76\n",
      "train accuracy and validation accuracy 0.8636114001274109 0.7981534004211426\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.8964 - accuracy: 0.7982\n",
      "test_accuracy 0.7981534004211426\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087, 0.7748140692710876, 0.7055655121803284, 0.7168504595756531, 0.6958194375038147, 0.7476276159286499, 0.7235188484191895, 0.7981534004211426]\n",
      "0.748610751496421\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S19_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S19_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 4.8954 - accuracy: 0.0965\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.19723, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 4s 370ms/step - loss: 4.8954 - accuracy: 0.0965 - val_loss: 3.5544 - val_accuracy: 0.1972 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.0316 - accuracy: 0.2338\n",
      "Epoch 00002: val_accuracy improved from 0.19723 to 0.33650, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 3.0317 - accuracy: 0.2343 - val_loss: 2.5804 - val_accuracy: 0.3365 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.5701 - accuracy: 0.3383\n",
      "Epoch 00003: val_accuracy improved from 0.33650 to 0.43242, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 2.5707 - accuracy: 0.3383 - val_loss: 2.2276 - val_accuracy: 0.4324 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2713 - accuracy: 0.4062\n",
      "Epoch 00004: val_accuracy improved from 0.43242 to 0.49808, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 2.2680 - accuracy: 0.4076 - val_loss: 1.9834 - val_accuracy: 0.4981 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0445 - accuracy: 0.4610\n",
      "Epoch 00005: val_accuracy improved from 0.49808 to 0.55065, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 2.0460 - accuracy: 0.4611 - val_loss: 1.8074 - val_accuracy: 0.5507 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8649 - accuracy: 0.5031\n",
      "Epoch 00006: val_accuracy improved from 0.55065 to 0.59066, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 1.8647 - accuracy: 0.5031 - val_loss: 1.6207 - val_accuracy: 0.5907 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7613 - accuracy: 0.5258\n",
      "Epoch 00007: val_accuracy improved from 0.59066 to 0.62272, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 105ms/step - loss: 1.7602 - accuracy: 0.5259 - val_loss: 1.4907 - val_accuracy: 0.6227 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6314 - accuracy: 0.5666\n",
      "Epoch 00008: val_accuracy improved from 0.62272 to 0.64709, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.6304 - accuracy: 0.5670 - val_loss: 1.4050 - val_accuracy: 0.6471 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5558 - accuracy: 0.5818\n",
      "Epoch 00009: val_accuracy improved from 0.64709 to 0.66017, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.5561 - accuracy: 0.5817 - val_loss: 1.3341 - val_accuracy: 0.6602 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4935 - accuracy: 0.6016\n",
      "Epoch 00010: val_accuracy improved from 0.66017 to 0.67402, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 1.4957 - accuracy: 0.6004 - val_loss: 1.2740 - val_accuracy: 0.6740 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4314 - accuracy: 0.6261\n",
      "Epoch 00011: val_accuracy improved from 0.67402 to 0.69531, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 1.4336 - accuracy: 0.6250 - val_loss: 1.2371 - val_accuracy: 0.6953 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3888 - accuracy: 0.6290\n",
      "Epoch 00012: val_accuracy did not improve from 0.69531\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.3897 - accuracy: 0.6288 - val_loss: 1.2007 - val_accuracy: 0.6925 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3312 - accuracy: 0.6468\n",
      "Epoch 00013: val_accuracy improved from 0.69531 to 0.70685, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.3315 - accuracy: 0.6466 - val_loss: 1.1531 - val_accuracy: 0.7068 - lr: 0.0010\n",
      "Epoch 14/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.2972 - accuracy: 0.6571\n",
      "Epoch 00014: val_accuracy improved from 0.70685 to 0.71634, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 107ms/step - loss: 1.2972 - accuracy: 0.6571 - val_loss: 1.1287 - val_accuracy: 0.7163 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2686 - accuracy: 0.6619\n",
      "Epoch 00015: val_accuracy improved from 0.71634 to 0.71865, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 1.2680 - accuracy: 0.6619 - val_loss: 1.1241 - val_accuracy: 0.7186 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2192 - accuracy: 0.6796\n",
      "Epoch 00016: val_accuracy improved from 0.71865 to 0.72147, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 1.2223 - accuracy: 0.6788 - val_loss: 1.0956 - val_accuracy: 0.7215 - lr: 0.0010\n",
      "Epoch 17/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1808 - accuracy: 0.6858\n",
      "Epoch 00017: val_accuracy improved from 0.72147 to 0.74865, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 117ms/step - loss: 1.1808 - accuracy: 0.6858 - val_loss: 1.0471 - val_accuracy: 0.7487 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1617 - accuracy: 0.6950\n",
      "Epoch 00018: val_accuracy did not improve from 0.74865\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.1633 - accuracy: 0.6940 - val_loss: 1.0408 - val_accuracy: 0.7469 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1208 - accuracy: 0.7089\n",
      "Epoch 00019: val_accuracy did not improve from 0.74865\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.1241 - accuracy: 0.7083 - val_loss: 1.0314 - val_accuracy: 0.7466 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1341 - accuracy: 0.7038\n",
      "Epoch 00020: val_accuracy did not improve from 0.74865\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1.1356 - accuracy: 0.7032 - val_loss: 1.0308 - val_accuracy: 0.7428 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0973 - accuracy: 0.7104\n",
      "Epoch 00021: val_accuracy improved from 0.74865 to 0.75071, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 106ms/step - loss: 1.0952 - accuracy: 0.7113 - val_loss: 1.0123 - val_accuracy: 0.7507 - lr: 0.0010\n",
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0825 - accuracy: 0.7154\n",
      "Epoch 00022: val_accuracy improved from 0.75071 to 0.75224, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 117ms/step - loss: 1.0831 - accuracy: 0.7151 - val_loss: 1.0117 - val_accuracy: 0.7522 - lr: 0.0010\n",
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0642 - accuracy: 0.7277\n",
      "Epoch 00023: val_accuracy did not improve from 0.75224\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 1.0653 - accuracy: 0.7276 - val_loss: 1.0048 - val_accuracy: 0.7517 - lr: 0.0010\n",
      "Epoch 24/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0394 - accuracy: 0.7313\n",
      "Epoch 00024: val_accuracy did not improve from 0.75224\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0394 - accuracy: 0.7313 - val_loss: 1.0207 - val_accuracy: 0.7438 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0266 - accuracy: 0.7293\n",
      "Epoch 00025: val_accuracy did not improve from 0.75224\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1.0302 - accuracy: 0.7285 - val_loss: 0.9966 - val_accuracy: 0.7512 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0496 - accuracy: 0.7291\n",
      "Epoch 00026: val_accuracy did not improve from 0.75224\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0501 - accuracy: 0.7287 - val_loss: 1.0178 - val_accuracy: 0.7494 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0291 - accuracy: 0.7289\n",
      "Epoch 00027: val_accuracy improved from 0.75224 to 0.76096, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 110ms/step - loss: 1.0327 - accuracy: 0.7277 - val_loss: 0.9685 - val_accuracy: 0.7610 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9933 - accuracy: 0.7430\n",
      "Epoch 00028: val_accuracy did not improve from 0.76096\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 0.9930 - accuracy: 0.7432 - val_loss: 0.9667 - val_accuracy: 0.7592 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9724 - accuracy: 0.7530\n",
      "Epoch 00029: val_accuracy improved from 0.76096 to 0.76225, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 0.9734 - accuracy: 0.7526 - val_loss: 0.9599 - val_accuracy: 0.7622 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9754 - accuracy: 0.7440\n",
      "Epoch 00030: val_accuracy improved from 0.76225 to 0.76507, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 109ms/step - loss: 0.9750 - accuracy: 0.7446 - val_loss: 0.9501 - val_accuracy: 0.7651 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9272 - accuracy: 0.7600\n",
      "Epoch 00031: val_accuracy improved from 0.76507 to 0.76815, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 0.9282 - accuracy: 0.7596 - val_loss: 0.9395 - val_accuracy: 0.7681 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9146 - accuracy: 0.7682\n",
      "Epoch 00032: val_accuracy improved from 0.76815 to 0.76840, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.9151 - accuracy: 0.7676 - val_loss: 0.9350 - val_accuracy: 0.7684 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9087 - accuracy: 0.7643\n",
      "Epoch 00033: val_accuracy did not improve from 0.76840\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9081 - accuracy: 0.7645 - val_loss: 0.9324 - val_accuracy: 0.7646 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8899 - accuracy: 0.7663\n",
      "Epoch 00034: val_accuracy improved from 0.76840 to 0.77020, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.8909 - accuracy: 0.7661 - val_loss: 0.9191 - val_accuracy: 0.7702 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9037 - accuracy: 0.7681\n",
      "Epoch 00035: val_accuracy improved from 0.77020 to 0.77097, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 0.9058 - accuracy: 0.7677 - val_loss: 0.9291 - val_accuracy: 0.7710 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8942 - accuracy: 0.7708\n",
      "Epoch 00036: val_accuracy improved from 0.77097 to 0.77584, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.8939 - accuracy: 0.7709 - val_loss: 0.9002 - val_accuracy: 0.7758 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8818 - accuracy: 0.7716\n",
      "Epoch 00037: val_accuracy did not improve from 0.77584\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8828 - accuracy: 0.7713 - val_loss: 0.9163 - val_accuracy: 0.7689 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8431 - accuracy: 0.7860\n",
      "Epoch 00038: val_accuracy did not improve from 0.77584\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.8453 - accuracy: 0.7851 - val_loss: 0.9220 - val_accuracy: 0.7628 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8663 - accuracy: 0.7776\n",
      "Epoch 00039: val_accuracy improved from 0.77584 to 0.77687, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.8670 - accuracy: 0.7774 - val_loss: 0.9075 - val_accuracy: 0.7769 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8201 - accuracy: 0.7890\n",
      "Epoch 00040: val_accuracy improved from 0.77687 to 0.78533, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.8235 - accuracy: 0.7882 - val_loss: 0.8879 - val_accuracy: 0.7853 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8159 - accuracy: 0.7958\n",
      "Epoch 00041: val_accuracy improved from 0.78533 to 0.78712, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.8158 - accuracy: 0.7957 - val_loss: 0.8783 - val_accuracy: 0.7871 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8009 - accuracy: 0.7977\n",
      "Epoch 00042: val_accuracy improved from 0.78712 to 0.78764, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.8033 - accuracy: 0.7973 - val_loss: 0.8738 - val_accuracy: 0.7876 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7702 - accuracy: 0.8060\n",
      "Epoch 00043: val_accuracy improved from 0.78764 to 0.78866, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 0.7709 - accuracy: 0.8058 - val_loss: 0.8652 - val_accuracy: 0.7887 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7680 - accuracy: 0.8082\n",
      "Epoch 00044: val_accuracy improved from 0.78866 to 0.78943, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 0.7704 - accuracy: 0.8072 - val_loss: 0.8590 - val_accuracy: 0.7894 - lr: 1.0000e-04\n",
      "Epoch 45/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7681 - accuracy: 0.8053\n",
      "Epoch 00045: val_accuracy improved from 0.78943 to 0.79072, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.7692 - accuracy: 0.8055 - val_loss: 0.8549 - val_accuracy: 0.7907 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7601 - accuracy: 0.8082\n",
      "Epoch 00046: val_accuracy did not improve from 0.79072\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7625 - accuracy: 0.8075 - val_loss: 0.8543 - val_accuracy: 0.7907 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7652 - accuracy: 0.8063\n",
      "Epoch 00047: val_accuracy improved from 0.79072 to 0.79225, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.7657 - accuracy: 0.8065 - val_loss: 0.8537 - val_accuracy: 0.7923 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7594 - accuracy: 0.8086\n",
      "Epoch 00048: val_accuracy did not improve from 0.79225\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7587 - accuracy: 0.8088 - val_loss: 0.8504 - val_accuracy: 0.7905 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7424 - accuracy: 0.8159\n",
      "Epoch 00049: val_accuracy did not improve from 0.79225\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7416 - accuracy: 0.8159 - val_loss: 0.8473 - val_accuracy: 0.7905 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7493 - accuracy: 0.8087\n",
      "Epoch 00050: val_accuracy did not improve from 0.79225\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7496 - accuracy: 0.8086 - val_loss: 0.8487 - val_accuracy: 0.7923 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7550 - accuracy: 0.8096\n",
      "Epoch 00051: val_accuracy improved from 0.79225 to 0.79405, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.7560 - accuracy: 0.8091 - val_loss: 0.8564 - val_accuracy: 0.7940 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7491 - accuracy: 0.8164\n",
      "Epoch 00052: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7517 - accuracy: 0.8158 - val_loss: 0.8541 - val_accuracy: 0.7905 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7332 - accuracy: 0.8179\n",
      "Epoch 00053: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7333 - accuracy: 0.8182 - val_loss: 0.8493 - val_accuracy: 0.7925 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7402 - accuracy: 0.8146\n",
      "Epoch 00054: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7414 - accuracy: 0.8143 - val_loss: 0.8467 - val_accuracy: 0.7923 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7304 - accuracy: 0.8121\n",
      "Epoch 00055: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.7298 - accuracy: 0.8128 - val_loss: 0.8480 - val_accuracy: 0.7925 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7439 - accuracy: 0.8128\n",
      "Epoch 00056: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7443 - accuracy: 0.8127 - val_loss: 0.8432 - val_accuracy: 0.7912 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7145 - accuracy: 0.8198\n",
      "Epoch 00057: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.7139 - accuracy: 0.8199 - val_loss: 0.8442 - val_accuracy: 0.7897 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7377 - accuracy: 0.8117\n",
      "Epoch 00058: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7362 - accuracy: 0.8123 - val_loss: 0.8446 - val_accuracy: 0.7912 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7144 - accuracy: 0.8232\n",
      "Epoch 00059: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.7136 - accuracy: 0.8236 - val_loss: 0.8389 - val_accuracy: 0.7935 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7200 - accuracy: 0.8193\n",
      "Epoch 00060: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7222 - accuracy: 0.8187 - val_loss: 0.8399 - val_accuracy: 0.7930 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7117 - accuracy: 0.8223\n",
      "Epoch 00061: val_accuracy improved from 0.79405 to 0.79431, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 0.7120 - accuracy: 0.8222 - val_loss: 0.8398 - val_accuracy: 0.7943 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7297 - accuracy: 0.8121\n",
      "Epoch 00062: val_accuracy did not improve from 0.79431\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7308 - accuracy: 0.8120 - val_loss: 0.8412 - val_accuracy: 0.7940 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7104 - accuracy: 0.8291\n",
      "Epoch 00063: val_accuracy did not improve from 0.79431\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7121 - accuracy: 0.8283 - val_loss: 0.8358 - val_accuracy: 0.7935 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7133 - accuracy: 0.8239\n",
      "Epoch 00064: val_accuracy improved from 0.79431 to 0.80046, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.7131 - accuracy: 0.8238 - val_loss: 0.8340 - val_accuracy: 0.8005 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7069 - accuracy: 0.8224\n",
      "Epoch 00065: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7100 - accuracy: 0.8220 - val_loss: 0.8317 - val_accuracy: 0.7940 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7138 - accuracy: 0.8214\n",
      "Epoch 00066: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7143 - accuracy: 0.8213 - val_loss: 0.8304 - val_accuracy: 0.7943 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6982 - accuracy: 0.8260\n",
      "Epoch 00067: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6993 - accuracy: 0.8256 - val_loss: 0.8305 - val_accuracy: 0.7961 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7013 - accuracy: 0.8301\n",
      "Epoch 00068: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7018 - accuracy: 0.8295 - val_loss: 0.8306 - val_accuracy: 0.7933 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7041 - accuracy: 0.8258\n",
      "Epoch 00069: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7052 - accuracy: 0.8256 - val_loss: 0.8356 - val_accuracy: 0.7894 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6906 - accuracy: 0.8314\n",
      "Epoch 00070: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6925 - accuracy: 0.8308 - val_loss: 0.8377 - val_accuracy: 0.7907 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6850 - accuracy: 0.8289\n",
      "Epoch 00071: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.6873 - accuracy: 0.8284 - val_loss: 0.8336 - val_accuracy: 0.7951 - lr: 1.0000e-04\n",
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6983 - accuracy: 0.8293\n",
      "Epoch 00072: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6984 - accuracy: 0.8290 - val_loss: 0.8334 - val_accuracy: 0.7987 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6858 - accuracy: 0.8269\n",
      "Epoch 00073: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6891 - accuracy: 0.8260 - val_loss: 0.8388 - val_accuracy: 0.7953 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6838 - accuracy: 0.8303\n",
      "Epoch 00074: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6884 - accuracy: 0.8294 - val_loss: 0.8384 - val_accuracy: 0.7997 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6908 - accuracy: 0.8298\n",
      "Epoch 00075: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6914 - accuracy: 0.8294 - val_loss: 0.8342 - val_accuracy: 0.7933 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6898 - accuracy: 0.8296\n",
      "Epoch 00076: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6916 - accuracy: 0.8291 - val_loss: 0.8345 - val_accuracy: 0.7953 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6878 - accuracy: 0.8304\n",
      "Epoch 00077: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6906 - accuracy: 0.8297 - val_loss: 0.8261 - val_accuracy: 0.7964 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6790 - accuracy: 0.8331\n",
      "Epoch 00078: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6802 - accuracy: 0.8332 - val_loss: 0.8385 - val_accuracy: 0.7969 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6917 - accuracy: 0.8256\n",
      "Epoch 00079: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6910 - accuracy: 0.8257 - val_loss: 0.8330 - val_accuracy: 0.7961 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6779 - accuracy: 0.8316\n",
      "Epoch 00080: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6780 - accuracy: 0.8316 - val_loss: 0.8298 - val_accuracy: 0.7989 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6802 - accuracy: 0.8334\n",
      "Epoch 00081: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6828 - accuracy: 0.8328 - val_loss: 0.8338 - val_accuracy: 0.7984 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6802 - accuracy: 0.8301\n",
      "Epoch 00082: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6827 - accuracy: 0.8300 - val_loss: 0.8336 - val_accuracy: 0.8005 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6745 - accuracy: 0.8340\n",
      "Epoch 00083: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6748 - accuracy: 0.8340 - val_loss: 0.8320 - val_accuracy: 0.7984 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6682 - accuracy: 0.8369\n",
      "Epoch 00084: val_accuracy did not improve from 0.80046\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6681 - accuracy: 0.8364 - val_loss: 0.8296 - val_accuracy: 0.7999 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6861 - accuracy: 0.8293\n",
      "Epoch 00085: val_accuracy improved from 0.80046 to 0.80123, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 0.6864 - accuracy: 0.8293 - val_loss: 0.8286 - val_accuracy: 0.8012 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6729 - accuracy: 0.8337\n",
      "Epoch 00086: val_accuracy did not improve from 0.80123\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6747 - accuracy: 0.8331 - val_loss: 0.8280 - val_accuracy: 0.8010 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6802 - accuracy: 0.8338\n",
      "Epoch 00087: val_accuracy did not improve from 0.80123\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6814 - accuracy: 0.8336 - val_loss: 0.8266 - val_accuracy: 0.7994 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6604 - accuracy: 0.8387\n",
      "Epoch 00088: val_accuracy improved from 0.80123 to 0.80149, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.6632 - accuracy: 0.8380 - val_loss: 0.8263 - val_accuracy: 0.8015 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6694 - accuracy: 0.8363\n",
      "Epoch 00089: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6700 - accuracy: 0.8360 - val_loss: 0.8236 - val_accuracy: 0.8012 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6764 - accuracy: 0.8363\n",
      "Epoch 00090: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6770 - accuracy: 0.8360 - val_loss: 0.8308 - val_accuracy: 0.7997 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6838 - accuracy: 0.8320\n",
      "Epoch 00091: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6857 - accuracy: 0.8316 - val_loss: 0.8281 - val_accuracy: 0.8005 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6819 - accuracy: 0.8320\n",
      "Epoch 00092: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6835 - accuracy: 0.8309 - val_loss: 0.8257 - val_accuracy: 0.7989 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6748 - accuracy: 0.8277\n",
      "Epoch 00093: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6758 - accuracy: 0.8277 - val_loss: 0.8208 - val_accuracy: 0.8002 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6839 - accuracy: 0.8276\n",
      "Epoch 00094: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6837 - accuracy: 0.8281 - val_loss: 0.8207 - val_accuracy: 0.8002 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6688 - accuracy: 0.8339\n",
      "Epoch 00095: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6688 - accuracy: 0.8343 - val_loss: 0.8217 - val_accuracy: 0.8012 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6804 - accuracy: 0.8289\n",
      "Epoch 00096: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6818 - accuracy: 0.8283 - val_loss: 0.8224 - val_accuracy: 0.8007 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6627 - accuracy: 0.8351\n",
      "Epoch 00097: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6659 - accuracy: 0.8343 - val_loss: 0.8283 - val_accuracy: 0.8007 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6641 - accuracy: 0.8373\n",
      "Epoch 00098: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6646 - accuracy: 0.8371 - val_loss: 0.8257 - val_accuracy: 0.8005 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6711 - accuracy: 0.8357\n",
      "Epoch 00099: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6718 - accuracy: 0.8354 - val_loss: 0.8254 - val_accuracy: 0.7982 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6703 - accuracy: 0.8322\n",
      "Epoch 00100: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6706 - accuracy: 0.8317 - val_loss: 0.8257 - val_accuracy: 0.8002 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6765 - accuracy: 0.8299\n",
      "Epoch 00101: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6765 - accuracy: 0.8299 - val_loss: 0.8212 - val_accuracy: 0.7989 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6653 - accuracy: 0.8341\n",
      "Epoch 00102: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6681 - accuracy: 0.8336 - val_loss: 0.8266 - val_accuracy: 0.7989 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6741 - accuracy: 0.8306\n",
      "Epoch 00103: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6740 - accuracy: 0.8299 - val_loss: 0.8218 - val_accuracy: 0.7992 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6723 - accuracy: 0.8350\n",
      "Epoch 00104: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6715 - accuracy: 0.8355 - val_loss: 0.8246 - val_accuracy: 0.7984 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6675 - accuracy: 0.8378\n",
      "Epoch 00105: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6677 - accuracy: 0.8375 - val_loss: 0.8236 - val_accuracy: 0.7997 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6679 - accuracy: 0.8347\n",
      "Epoch 00106: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6712 - accuracy: 0.8332 - val_loss: 0.8560 - val_accuracy: 0.7946 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6788 - accuracy: 0.8290\n",
      "Epoch 00107: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6809 - accuracy: 0.8282 - val_loss: 0.8297 - val_accuracy: 0.7992 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6726 - accuracy: 0.8326\n",
      "Epoch 00108: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6738 - accuracy: 0.8322 - val_loss: 0.8271 - val_accuracy: 0.8002 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6784 - accuracy: 0.8317\n",
      "Epoch 00109: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6784 - accuracy: 0.8313 - val_loss: 0.8235 - val_accuracy: 0.7987 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6770 - accuracy: 0.8329\n",
      "Epoch 00110: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6803 - accuracy: 0.8322 - val_loss: 0.8242 - val_accuracy: 0.7999 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6633 - accuracy: 0.8338\n",
      "Epoch 00111: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6644 - accuracy: 0.8335 - val_loss: 0.8236 - val_accuracy: 0.7987 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6547 - accuracy: 0.8407\n",
      "Epoch 00112: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.6557 - accuracy: 0.8404 - val_loss: 0.8233 - val_accuracy: 0.7999 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6735 - accuracy: 0.8326\n",
      "Epoch 00113: val_accuracy did not improve from 0.80149\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6724 - accuracy: 0.8327 - val_loss: 0.8232 - val_accuracy: 0.8010 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6668 - accuracy: 0.8376\n",
      "Epoch 00114: val_accuracy improved from 0.80149 to 0.80174, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 0.6699 - accuracy: 0.8365 - val_loss: 0.8223 - val_accuracy: 0.8017 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6674 - accuracy: 0.8311\n",
      "Epoch 00115: val_accuracy did not improve from 0.80174\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6678 - accuracy: 0.8312 - val_loss: 0.8205 - val_accuracy: 0.8012 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6704 - accuracy: 0.8366\n",
      "Epoch 00116: val_accuracy did not improve from 0.80174\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6706 - accuracy: 0.8365 - val_loss: 0.8228 - val_accuracy: 0.8005 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6637 - accuracy: 0.8383\n",
      "Epoch 00117: val_accuracy did not improve from 0.80174\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6639 - accuracy: 0.8379 - val_loss: 0.8218 - val_accuracy: 0.7984 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6546 - accuracy: 0.8372\n",
      "Epoch 00118: val_accuracy did not improve from 0.80174\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6587 - accuracy: 0.8364 - val_loss: 0.8158 - val_accuracy: 0.8017 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6672 - accuracy: 0.8329\n",
      "Epoch 00119: val_accuracy did not improve from 0.80174\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6680 - accuracy: 0.8324 - val_loss: 0.8167 - val_accuracy: 0.7999 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6579 - accuracy: 0.8388\n",
      "Epoch 00120: val_accuracy did not improve from 0.80174\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6585 - accuracy: 0.8383 - val_loss: 0.8199 - val_accuracy: 0.7987 - lr: 1.0000e-06\n",
      "epoch_number 114\n",
      "train accuracy and validation accuracy 0.8364655375480652 0.8017440438270569\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.8223 - accuracy: 0.8017\n",
      "test_accuracy 0.8017440438270569\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087, 0.7748140692710876, 0.7055655121803284, 0.7168504595756531, 0.6958194375038147, 0.7476276159286499, 0.7235188484191895, 0.7981534004211426, 0.8017440438270569]\n",
      "0.7514072405664545\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S20_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S20_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.8188 - accuracy: 0.1051\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.19338, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 3s 347ms/step - loss: 4.8069 - accuracy: 0.1054 - val_loss: 3.7183 - val_accuracy: 0.1934 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.0088 - accuracy: 0.2313\n",
      "Epoch 00002: val_accuracy improved from 0.19338 to 0.32855, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 3.0055 - accuracy: 0.2319 - val_loss: 2.4863 - val_accuracy: 0.3285 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4424 - accuracy: 0.3498\n",
      "Epoch 00003: val_accuracy improved from 0.32855 to 0.46243, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 2.4422 - accuracy: 0.3501 - val_loss: 2.0889 - val_accuracy: 0.4624 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1276 - accuracy: 0.4340\n",
      "Epoch 00004: val_accuracy improved from 0.46243 to 0.52167, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 2.1258 - accuracy: 0.4350 - val_loss: 1.8213 - val_accuracy: 0.5217 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9020 - accuracy: 0.4879\n",
      "Epoch 00005: val_accuracy improved from 0.52167 to 0.57605, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 1.9026 - accuracy: 0.4877 - val_loss: 1.6575 - val_accuracy: 0.5760 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7342 - accuracy: 0.5312\n",
      "Epoch 00006: val_accuracy improved from 0.57605 to 0.59862, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.7360 - accuracy: 0.5309 - val_loss: 1.5313 - val_accuracy: 0.5986 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6357 - accuracy: 0.5592\n",
      "Epoch 00007: val_accuracy improved from 0.59862 to 0.62221, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.6381 - accuracy: 0.5586 - val_loss: 1.4484 - val_accuracy: 0.6222 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5332 - accuracy: 0.5832\n",
      "Epoch 00008: val_accuracy improved from 0.62221 to 0.64093, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.5337 - accuracy: 0.5835 - val_loss: 1.3879 - val_accuracy: 0.6409 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4381 - accuracy: 0.6154\n",
      "Epoch 00009: val_accuracy improved from 0.64093 to 0.66325, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.4380 - accuracy: 0.6155 - val_loss: 1.3279 - val_accuracy: 0.6632 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4002 - accuracy: 0.6202\n",
      "Epoch 00010: val_accuracy improved from 0.66325 to 0.66376, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.4027 - accuracy: 0.6197 - val_loss: 1.2700 - val_accuracy: 0.6638 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3424 - accuracy: 0.6414\n",
      "Epoch 00011: val_accuracy improved from 0.66376 to 0.68274, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.3409 - accuracy: 0.6415 - val_loss: 1.2344 - val_accuracy: 0.6827 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2964 - accuracy: 0.6524\n",
      "Epoch 00012: val_accuracy did not improve from 0.68274\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.2965 - accuracy: 0.6524 - val_loss: 1.2656 - val_accuracy: 0.6738 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2482 - accuracy: 0.6663\n",
      "Epoch 00013: val_accuracy improved from 0.68274 to 0.68659, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.2519 - accuracy: 0.6657 - val_loss: 1.2168 - val_accuracy: 0.6866 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2141 - accuracy: 0.6754\n",
      "Epoch 00014: val_accuracy improved from 0.68659 to 0.68684, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.2197 - accuracy: 0.6739 - val_loss: 1.2017 - val_accuracy: 0.6868 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2039 - accuracy: 0.6784\n",
      "Epoch 00015: val_accuracy improved from 0.68684 to 0.69787, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.2052 - accuracy: 0.6777 - val_loss: 1.1751 - val_accuracy: 0.6979 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1503 - accuracy: 0.6948\n",
      "Epoch 00016: val_accuracy improved from 0.69787 to 0.70223, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.1519 - accuracy: 0.6943 - val_loss: 1.1593 - val_accuracy: 0.7022 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1356 - accuracy: 0.6962\n",
      "Epoch 00017: val_accuracy improved from 0.70223 to 0.71531, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.1371 - accuracy: 0.6963 - val_loss: 1.1201 - val_accuracy: 0.7153 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1136 - accuracy: 0.7020\n",
      "Epoch 00018: val_accuracy did not improve from 0.71531\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1144 - accuracy: 0.7019 - val_loss: 1.1385 - val_accuracy: 0.7094 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0806 - accuracy: 0.7174\n",
      "Epoch 00019: val_accuracy improved from 0.71531 to 0.71659, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.0830 - accuracy: 0.7169 - val_loss: 1.1060 - val_accuracy: 0.7166 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0769 - accuracy: 0.7146\n",
      "Epoch 00020: val_accuracy did not improve from 0.71659\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.0786 - accuracy: 0.7145 - val_loss: 1.1073 - val_accuracy: 0.7135 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0507 - accuracy: 0.7256\n",
      "Epoch 00021: val_accuracy did not improve from 0.71659\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0532 - accuracy: 0.7248 - val_loss: 1.0771 - val_accuracy: 0.7161 - lr: 0.0010\n",
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0069 - accuracy: 0.7380\n",
      "Epoch 00022: val_accuracy improved from 0.71659 to 0.72506, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 91ms/step - loss: 1.0073 - accuracy: 0.7377 - val_loss: 1.0835 - val_accuracy: 0.7251 - lr: 0.0010\n",
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9956 - accuracy: 0.7392\n",
      "Epoch 00023: val_accuracy did not improve from 0.72506\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9980 - accuracy: 0.7383 - val_loss: 1.0925 - val_accuracy: 0.7163 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9838 - accuracy: 0.7420\n",
      "Epoch 00024: val_accuracy did not improve from 0.72506\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9841 - accuracy: 0.7419 - val_loss: 1.0807 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9709 - accuracy: 0.7447\n",
      "Epoch 00025: val_accuracy did not improve from 0.72506\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9723 - accuracy: 0.7448 - val_loss: 1.0703 - val_accuracy: 0.7210 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9571 - accuracy: 0.7521\n",
      "Epoch 00026: val_accuracy did not improve from 0.72506\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.9571 - accuracy: 0.7520 - val_loss: 1.0864 - val_accuracy: 0.7212 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9276 - accuracy: 0.7587\n",
      "Epoch 00027: val_accuracy improved from 0.72506 to 0.73686, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.9299 - accuracy: 0.7582 - val_loss: 1.0388 - val_accuracy: 0.7369 - lr: 0.0010\n",
      "Epoch 28/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9025 - accuracy: 0.7633\n",
      "Epoch 00028: val_accuracy did not improve from 0.73686\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9025 - accuracy: 0.7633 - val_loss: 1.0448 - val_accuracy: 0.7335 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9039 - accuracy: 0.7616\n",
      "Epoch 00029: val_accuracy did not improve from 0.73686\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.9046 - accuracy: 0.7618 - val_loss: 1.0414 - val_accuracy: 0.7325 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8965 - accuracy: 0.7666\n",
      "Epoch 00030: val_accuracy improved from 0.73686 to 0.74891, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.8959 - accuracy: 0.7669 - val_loss: 1.0393 - val_accuracy: 0.7489 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9032 - accuracy: 0.7668\n",
      "Epoch 00031: val_accuracy did not improve from 0.74891\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.9036 - accuracy: 0.7667 - val_loss: 1.0251 - val_accuracy: 0.7389 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8787 - accuracy: 0.7704\n",
      "Epoch 00032: val_accuracy did not improve from 0.74891\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8798 - accuracy: 0.7700 - val_loss: 1.0147 - val_accuracy: 0.7420 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8450 - accuracy: 0.7847\n",
      "Epoch 00033: val_accuracy did not improve from 0.74891\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8447 - accuracy: 0.7845 - val_loss: 1.0170 - val_accuracy: 0.7445 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8449 - accuracy: 0.7841\n",
      "Epoch 00034: val_accuracy did not improve from 0.74891\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8442 - accuracy: 0.7846 - val_loss: 1.0260 - val_accuracy: 0.7425 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8340 - accuracy: 0.7834\n",
      "Epoch 00035: val_accuracy improved from 0.74891 to 0.74942, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.8348 - accuracy: 0.7824 - val_loss: 0.9967 - val_accuracy: 0.7494 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8209 - accuracy: 0.7864\n",
      "Epoch 00036: val_accuracy improved from 0.74942 to 0.74994, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 0.8208 - accuracy: 0.7866 - val_loss: 1.0079 - val_accuracy: 0.7499 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8113 - accuracy: 0.7939\n",
      "Epoch 00037: val_accuracy did not improve from 0.74994\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8127 - accuracy: 0.7933 - val_loss: 1.0243 - val_accuracy: 0.7481 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8254 - accuracy: 0.7869\n",
      "Epoch 00038: val_accuracy did not improve from 0.74994\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8303 - accuracy: 0.7858 - val_loss: 1.0335 - val_accuracy: 0.7356 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8365 - accuracy: 0.7837\n",
      "Epoch 00039: val_accuracy did not improve from 0.74994\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8365 - accuracy: 0.7836 - val_loss: 1.0086 - val_accuracy: 0.7461 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7966 - accuracy: 0.8001\n",
      "Epoch 00040: val_accuracy improved from 0.74994 to 0.75173, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.7976 - accuracy: 0.8000 - val_loss: 0.9856 - val_accuracy: 0.7517 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7809 - accuracy: 0.8052\n",
      "Epoch 00041: val_accuracy improved from 0.75173 to 0.75686, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.7810 - accuracy: 0.8053 - val_loss: 0.9734 - val_accuracy: 0.7569 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7683 - accuracy: 0.8019\n",
      "Epoch 00042: val_accuracy improved from 0.75686 to 0.75917, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 0.7713 - accuracy: 0.8007 - val_loss: 0.9757 - val_accuracy: 0.7592 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7481 - accuracy: 0.8133\n",
      "Epoch 00043: val_accuracy improved from 0.75917 to 0.76122, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.7502 - accuracy: 0.8121 - val_loss: 0.9689 - val_accuracy: 0.7612 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7452 - accuracy: 0.8103\n",
      "Epoch 00044: val_accuracy did not improve from 0.76122\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7445 - accuracy: 0.8106 - val_loss: 0.9719 - val_accuracy: 0.7589 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7400 - accuracy: 0.8111\n",
      "Epoch 00045: val_accuracy improved from 0.76122 to 0.76199, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.7432 - accuracy: 0.8099 - val_loss: 0.9714 - val_accuracy: 0.7620 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7241 - accuracy: 0.8186\n",
      "Epoch 00046: val_accuracy improved from 0.76199 to 0.76250, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.7250 - accuracy: 0.8179 - val_loss: 0.9672 - val_accuracy: 0.7625 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7153 - accuracy: 0.8186\n",
      "Epoch 00047: val_accuracy improved from 0.76250 to 0.76430, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.7182 - accuracy: 0.8179 - val_loss: 0.9616 - val_accuracy: 0.7643 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7152 - accuracy: 0.8194\n",
      "Epoch 00048: val_accuracy did not improve from 0.76430\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7151 - accuracy: 0.8197 - val_loss: 0.9616 - val_accuracy: 0.7638 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7155 - accuracy: 0.8213\n",
      "Epoch 00049: val_accuracy improved from 0.76430 to 0.76635, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.7153 - accuracy: 0.8212 - val_loss: 0.9650 - val_accuracy: 0.7664 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7099 - accuracy: 0.8263\n",
      "Epoch 00050: val_accuracy did not improve from 0.76635\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7111 - accuracy: 0.8256 - val_loss: 0.9621 - val_accuracy: 0.7648 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7184 - accuracy: 0.8220\n",
      "Epoch 00051: val_accuracy did not improve from 0.76635\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7174 - accuracy: 0.8224 - val_loss: 0.9587 - val_accuracy: 0.7633 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7005 - accuracy: 0.8263\n",
      "Epoch 00052: val_accuracy did not improve from 0.76635\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7005 - accuracy: 0.8266 - val_loss: 0.9569 - val_accuracy: 0.7664 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7070 - accuracy: 0.8228\n",
      "Epoch 00053: val_accuracy did not improve from 0.76635\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.7095 - accuracy: 0.8221 - val_loss: 0.9587 - val_accuracy: 0.7656 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7018 - accuracy: 0.8276\n",
      "Epoch 00054: val_accuracy did not improve from 0.76635\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7008 - accuracy: 0.8276 - val_loss: 0.9509 - val_accuracy: 0.7656 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6917 - accuracy: 0.8310\n",
      "Epoch 00055: val_accuracy did not improve from 0.76635\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6938 - accuracy: 0.8304 - val_loss: 0.9543 - val_accuracy: 0.7607 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6770 - accuracy: 0.8356\n",
      "Epoch 00056: val_accuracy did not improve from 0.76635\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6789 - accuracy: 0.8348 - val_loss: 0.9533 - val_accuracy: 0.7664 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7004 - accuracy: 0.8224\n",
      "Epoch 00057: val_accuracy did not improve from 0.76635\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7008 - accuracy: 0.8226 - val_loss: 0.9512 - val_accuracy: 0.7664 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6805 - accuracy: 0.8281\n",
      "Epoch 00058: val_accuracy improved from 0.76635 to 0.76815, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.6787 - accuracy: 0.8292 - val_loss: 0.9564 - val_accuracy: 0.7681 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6772 - accuracy: 0.8328\n",
      "Epoch 00059: val_accuracy improved from 0.76815 to 0.76917, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 0.6781 - accuracy: 0.8327 - val_loss: 0.9518 - val_accuracy: 0.7692 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6656 - accuracy: 0.8367\n",
      "Epoch 00060: val_accuracy improved from 0.76917 to 0.77174, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.6655 - accuracy: 0.8366 - val_loss: 0.9496 - val_accuracy: 0.7717 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6738 - accuracy: 0.8346\n",
      "Epoch 00061: val_accuracy did not improve from 0.77174\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6745 - accuracy: 0.8343 - val_loss: 0.9534 - val_accuracy: 0.7715 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6852 - accuracy: 0.8306\n",
      "Epoch 00062: val_accuracy did not improve from 0.77174\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6841 - accuracy: 0.8311 - val_loss: 0.9582 - val_accuracy: 0.7705 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6785 - accuracy: 0.8322\n",
      "Epoch 00063: val_accuracy did not improve from 0.77174\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6812 - accuracy: 0.8314 - val_loss: 0.9603 - val_accuracy: 0.7671 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6752 - accuracy: 0.8328\n",
      "Epoch 00064: val_accuracy did not improve from 0.77174\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6764 - accuracy: 0.8327 - val_loss: 0.9489 - val_accuracy: 0.7676 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6702 - accuracy: 0.8347\n",
      "Epoch 00065: val_accuracy did not improve from 0.77174\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6710 - accuracy: 0.8346 - val_loss: 0.9476 - val_accuracy: 0.7684 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6551 - accuracy: 0.8416\n",
      "Epoch 00066: val_accuracy did not improve from 0.77174\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6552 - accuracy: 0.8412 - val_loss: 0.9507 - val_accuracy: 0.7692 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6599 - accuracy: 0.8384\n",
      "Epoch 00067: val_accuracy did not improve from 0.77174\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6617 - accuracy: 0.8381 - val_loss: 0.9482 - val_accuracy: 0.7697 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6606 - accuracy: 0.8376\n",
      "Epoch 00068: val_accuracy improved from 0.77174 to 0.77199, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 0.6618 - accuracy: 0.8369 - val_loss: 0.9526 - val_accuracy: 0.7720 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6546 - accuracy: 0.8361\n",
      "Epoch 00069: val_accuracy did not improve from 0.77199\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6577 - accuracy: 0.8350 - val_loss: 0.9613 - val_accuracy: 0.7658 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6539 - accuracy: 0.8372\n",
      "Epoch 00070: val_accuracy did not improve from 0.77199\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6541 - accuracy: 0.8372 - val_loss: 0.9543 - val_accuracy: 0.7656 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6554 - accuracy: 0.8376\n",
      "Epoch 00071: val_accuracy did not improve from 0.77199\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6550 - accuracy: 0.8376 - val_loss: 0.9494 - val_accuracy: 0.7687 - lr: 1.0000e-04\n",
      "Epoch 72/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6505 - accuracy: 0.8400\n",
      "Epoch 00072: val_accuracy improved from 0.77199 to 0.77302, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.6504 - accuracy: 0.8401 - val_loss: 0.9464 - val_accuracy: 0.7730 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6638 - accuracy: 0.8332\n",
      "Epoch 00073: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.6658 - accuracy: 0.8332 - val_loss: 0.9445 - val_accuracy: 0.7720 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6410 - accuracy: 0.8416\n",
      "Epoch 00074: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6420 - accuracy: 0.8411 - val_loss: 0.9467 - val_accuracy: 0.7720 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6578 - accuracy: 0.8412\n",
      "Epoch 00075: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.6582 - accuracy: 0.8408 - val_loss: 0.9522 - val_accuracy: 0.7710 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6439 - accuracy: 0.8403\n",
      "Epoch 00076: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6456 - accuracy: 0.8397 - val_loss: 0.9530 - val_accuracy: 0.7676 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6566 - accuracy: 0.8398\n",
      "Epoch 00077: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6575 - accuracy: 0.8397 - val_loss: 0.9455 - val_accuracy: 0.7707 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6335 - accuracy: 0.8469\n",
      "Epoch 00078: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6346 - accuracy: 0.8468 - val_loss: 0.9484 - val_accuracy: 0.7715 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6371 - accuracy: 0.8477\n",
      "Epoch 00079: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.6392 - accuracy: 0.8473 - val_loss: 0.9541 - val_accuracy: 0.7664 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6445 - accuracy: 0.8427\n",
      "Epoch 00080: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6437 - accuracy: 0.8429 - val_loss: 0.9479 - val_accuracy: 0.7697 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6411 - accuracy: 0.8437\n",
      "Epoch 00081: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6416 - accuracy: 0.8435 - val_loss: 0.9498 - val_accuracy: 0.7699 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6393 - accuracy: 0.8443\n",
      "Epoch 00082: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6407 - accuracy: 0.8442 - val_loss: 0.9483 - val_accuracy: 0.7687 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6336 - accuracy: 0.8470\n",
      "Epoch 00083: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6358 - accuracy: 0.8465 - val_loss: 0.9502 - val_accuracy: 0.7648 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6430 - accuracy: 0.8371\n",
      "Epoch 00084: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6441 - accuracy: 0.8368 - val_loss: 0.9448 - val_accuracy: 0.7694 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6384 - accuracy: 0.8466\n",
      "Epoch 00085: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6420 - accuracy: 0.8458 - val_loss: 0.9532 - val_accuracy: 0.7694 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6391 - accuracy: 0.8463\n",
      "Epoch 00086: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6395 - accuracy: 0.8459 - val_loss: 0.9460 - val_accuracy: 0.7705 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6339 - accuracy: 0.8483\n",
      "Epoch 00087: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6340 - accuracy: 0.8484 - val_loss: 0.9421 - val_accuracy: 0.7710 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6251 - accuracy: 0.8486\n",
      "Epoch 00088: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6279 - accuracy: 0.8475 - val_loss: 0.9504 - val_accuracy: 0.7651 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6314 - accuracy: 0.8487\n",
      "Epoch 00089: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6357 - accuracy: 0.8472 - val_loss: 0.9608 - val_accuracy: 0.7633 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6390 - accuracy: 0.8419\n",
      "Epoch 00090: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6401 - accuracy: 0.8413 - val_loss: 0.9573 - val_accuracy: 0.7635 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6281 - accuracy: 0.8453\n",
      "Epoch 00091: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6302 - accuracy: 0.8446 - val_loss: 0.9420 - val_accuracy: 0.7702 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6361 - accuracy: 0.8428\n",
      "Epoch 00092: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6365 - accuracy: 0.8423 - val_loss: 0.9446 - val_accuracy: 0.7692 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6300 - accuracy: 0.8450\n",
      "Epoch 00093: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6320 - accuracy: 0.8447 - val_loss: 0.9462 - val_accuracy: 0.7705 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6356 - accuracy: 0.8441\n",
      "Epoch 00094: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6378 - accuracy: 0.8442 - val_loss: 0.9477 - val_accuracy: 0.7702 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6379 - accuracy: 0.8437\n",
      "Epoch 00095: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6377 - accuracy: 0.8438 - val_loss: 0.9484 - val_accuracy: 0.7707 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6421 - accuracy: 0.8387\n",
      "Epoch 00096: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6433 - accuracy: 0.8384 - val_loss: 0.9475 - val_accuracy: 0.7720 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6366 - accuracy: 0.8417\n",
      "Epoch 00097: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6368 - accuracy: 0.8420 - val_loss: 0.9461 - val_accuracy: 0.7710 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6345 - accuracy: 0.8444\n",
      "Epoch 00098: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.6358 - accuracy: 0.8438 - val_loss: 0.9449 - val_accuracy: 0.7694 - lr: 1.0000e-05\n",
      "Epoch 99/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6378 - accuracy: 0.8482\n",
      "Epoch 00099: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.6397 - accuracy: 0.8476 - val_loss: 0.9419 - val_accuracy: 0.7707 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6375 - accuracy: 0.8446\n",
      "Epoch 00100: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6364 - accuracy: 0.8448 - val_loss: 0.9444 - val_accuracy: 0.7712 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6278 - accuracy: 0.8471\n",
      "Epoch 00101: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6273 - accuracy: 0.8471 - val_loss: 0.9453 - val_accuracy: 0.7684 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6314 - accuracy: 0.8444\n",
      "Epoch 00102: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6319 - accuracy: 0.8447 - val_loss: 0.9423 - val_accuracy: 0.7712 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6318 - accuracy: 0.8446\n",
      "Epoch 00103: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6330 - accuracy: 0.8440 - val_loss: 0.9437 - val_accuracy: 0.7712 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6217 - accuracy: 0.8478\n",
      "Epoch 00104: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6244 - accuracy: 0.8470 - val_loss: 0.9427 - val_accuracy: 0.7699 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6388 - accuracy: 0.8456\n",
      "Epoch 00105: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6435 - accuracy: 0.8445 - val_loss: 0.9450 - val_accuracy: 0.7699 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6332 - accuracy: 0.8474\n",
      "Epoch 00106: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6349 - accuracy: 0.8468 - val_loss: 0.9475 - val_accuracy: 0.7717 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6283 - accuracy: 0.8488\n",
      "Epoch 00107: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6291 - accuracy: 0.8490 - val_loss: 0.9483 - val_accuracy: 0.7720 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6371 - accuracy: 0.8490\n",
      "Epoch 00108: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6382 - accuracy: 0.8482 - val_loss: 0.9453 - val_accuracy: 0.7710 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6263 - accuracy: 0.8502\n",
      "Epoch 00109: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.6272 - accuracy: 0.8500 - val_loss: 0.9431 - val_accuracy: 0.7712 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6260 - accuracy: 0.8446\n",
      "Epoch 00110: val_accuracy did not improve from 0.77302\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6291 - accuracy: 0.8437 - val_loss: 0.9430 - val_accuracy: 0.7702 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6154 - accuracy: 0.8516\n",
      "Epoch 00111: val_accuracy improved from 0.77302 to 0.77328, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.6153 - accuracy: 0.8515 - val_loss: 0.9411 - val_accuracy: 0.7733 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6282 - accuracy: 0.8472\n",
      "Epoch 00112: val_accuracy did not improve from 0.77328\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6282 - accuracy: 0.8468 - val_loss: 0.9403 - val_accuracy: 0.7712 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6349 - accuracy: 0.8454\n",
      "Epoch 00113: val_accuracy did not improve from 0.77328\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6350 - accuracy: 0.8454 - val_loss: 0.9441 - val_accuracy: 0.7697 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6376 - accuracy: 0.8453\n",
      "Epoch 00114: val_accuracy did not improve from 0.77328\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6383 - accuracy: 0.8448 - val_loss: 0.9433 - val_accuracy: 0.7722 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6209 - accuracy: 0.8530\n",
      "Epoch 00115: val_accuracy did not improve from 0.77328\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6208 - accuracy: 0.8533 - val_loss: 0.9417 - val_accuracy: 0.7710 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6221 - accuracy: 0.8508\n",
      "Epoch 00116: val_accuracy did not improve from 0.77328\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 0.6234 - accuracy: 0.8505 - val_loss: 0.9438 - val_accuracy: 0.7694 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6306 - accuracy: 0.8481\n",
      "Epoch 00117: val_accuracy did not improve from 0.77328\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6330 - accuracy: 0.8471 - val_loss: 0.9459 - val_accuracy: 0.7689 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6161 - accuracy: 0.8518\n",
      "Epoch 00118: val_accuracy did not improve from 0.77328\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.6188 - accuracy: 0.8508 - val_loss: 0.9469 - val_accuracy: 0.7697 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6306 - accuracy: 0.8470\n",
      "Epoch 00119: val_accuracy did not improve from 0.77328\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.6304 - accuracy: 0.8473 - val_loss: 0.9574 - val_accuracy: 0.7653 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6273 - accuracy: 0.8474\n",
      "Epoch 00120: val_accuracy did not improve from 0.77328\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6280 - accuracy: 0.8471 - val_loss: 0.9459 - val_accuracy: 0.7715 - lr: 1.0000e-06\n",
      "epoch_number 111\n",
      "train accuracy and validation accuracy 0.8515221476554871 0.7732751965522766\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.9411 - accuracy: 0.7733\n",
      "test_accuracy 0.7732751965522766\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087, 0.7748140692710876, 0.7055655121803284, 0.7168504595756531, 0.6958194375038147, 0.7476276159286499, 0.7235188484191895, 0.7981534004211426, 0.8017440438270569, 0.7732751965522766]\n",
      "0.7525006383657455\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S21_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S21_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.6695 - accuracy: 0.1641\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.28879, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 3s 346ms/step - loss: 4.6525 - accuracy: 0.1642 - val_loss: 3.5830 - val_accuracy: 0.2888 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.8102 - accuracy: 0.2837\n",
      "Epoch 00002: val_accuracy improved from 0.28879 to 0.41087, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 2.8098 - accuracy: 0.2843 - val_loss: 2.3430 - val_accuracy: 0.4109 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.3379 - accuracy: 0.3880\n",
      "Epoch 00003: val_accuracy improved from 0.41087 to 0.50757, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 2.3389 - accuracy: 0.3884 - val_loss: 1.9696 - val_accuracy: 0.5076 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0817 - accuracy: 0.4516\n",
      "Epoch 00004: val_accuracy improved from 0.50757 to 0.55655, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 2.0808 - accuracy: 0.4516 - val_loss: 1.7706 - val_accuracy: 0.5566 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8858 - accuracy: 0.4977\n",
      "Epoch 00005: val_accuracy improved from 0.55655 to 0.58707, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.8873 - accuracy: 0.4973 - val_loss: 1.6315 - val_accuracy: 0.5871 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7515 - accuracy: 0.5344\n",
      "Epoch 00006: val_accuracy improved from 0.58707 to 0.61580, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.7540 - accuracy: 0.5339 - val_loss: 1.5113 - val_accuracy: 0.6158 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6432 - accuracy: 0.5624\n",
      "Epoch 00007: val_accuracy improved from 0.61580 to 0.63452, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 1.6433 - accuracy: 0.5615 - val_loss: 1.4306 - val_accuracy: 0.6345 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5599 - accuracy: 0.5797\n",
      "Epoch 00008: val_accuracy improved from 0.63452 to 0.66120, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.5609 - accuracy: 0.5797 - val_loss: 1.3594 - val_accuracy: 0.6612 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4844 - accuracy: 0.6032\n",
      "Epoch 00009: val_accuracy improved from 0.66120 to 0.67658, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 1.4831 - accuracy: 0.6037 - val_loss: 1.3050 - val_accuracy: 0.6766 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3989 - accuracy: 0.6341\n",
      "Epoch 00010: val_accuracy improved from 0.67658 to 0.68915, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.3979 - accuracy: 0.6346 - val_loss: 1.2700 - val_accuracy: 0.6892 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3385 - accuracy: 0.6482\n",
      "Epoch 00011: val_accuracy improved from 0.68915 to 0.69992, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1.3390 - accuracy: 0.6482 - val_loss: 1.2204 - val_accuracy: 0.6999 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2838 - accuracy: 0.6554\n",
      "Epoch 00012: val_accuracy improved from 0.69992 to 0.70095, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.2831 - accuracy: 0.6555 - val_loss: 1.2160 - val_accuracy: 0.7009 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2698 - accuracy: 0.6606\n",
      "Epoch 00013: val_accuracy improved from 0.70095 to 0.70172, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.2721 - accuracy: 0.6602 - val_loss: 1.1844 - val_accuracy: 0.7017 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2282 - accuracy: 0.6772\n",
      "Epoch 00014: val_accuracy improved from 0.70172 to 0.71634, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.2287 - accuracy: 0.6774 - val_loss: 1.1699 - val_accuracy: 0.7163 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1723 - accuracy: 0.6928\n",
      "Epoch 00015: val_accuracy improved from 0.71634 to 0.72352, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.1720 - accuracy: 0.6927 - val_loss: 1.1474 - val_accuracy: 0.7235 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1391 - accuracy: 0.6991\n",
      "Epoch 00016: val_accuracy did not improve from 0.72352\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.1434 - accuracy: 0.6982 - val_loss: 1.1401 - val_accuracy: 0.7220 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1243 - accuracy: 0.7020\n",
      "Epoch 00017: val_accuracy improved from 0.72352 to 0.72839, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.1261 - accuracy: 0.7015 - val_loss: 1.1288 - val_accuracy: 0.7284 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1198 - accuracy: 0.6978\n",
      "Epoch 00018: val_accuracy improved from 0.72839 to 0.73532, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 1.1215 - accuracy: 0.6972 - val_loss: 1.1066 - val_accuracy: 0.7353 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0816 - accuracy: 0.7193\n",
      "Epoch 00019: val_accuracy did not improve from 0.73532\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0818 - accuracy: 0.7188 - val_loss: 1.1009 - val_accuracy: 0.7333 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0557 - accuracy: 0.7252\n",
      "Epoch 00020: val_accuracy did not improve from 0.73532\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.0598 - accuracy: 0.7235 - val_loss: 1.1190 - val_accuracy: 0.7328 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0462 - accuracy: 0.7319\n",
      "Epoch 00021: val_accuracy improved from 0.73532 to 0.74224, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.0493 - accuracy: 0.7308 - val_loss: 1.0843 - val_accuracy: 0.7422 - lr: 0.0010\n",
      "Epoch 22/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0366 - accuracy: 0.7347\n",
      "Epoch 00022: val_accuracy improved from 0.74224 to 0.74532, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 129ms/step - loss: 1.0373 - accuracy: 0.7345 - val_loss: 1.0636 - val_accuracy: 0.7453 - lr: 0.0010\n",
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0385 - accuracy: 0.7291\n",
      "Epoch 00023: val_accuracy did not improve from 0.74532\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.0389 - accuracy: 0.7290 - val_loss: 1.0664 - val_accuracy: 0.7445 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9845 - accuracy: 0.7509\n",
      "Epoch 00024: val_accuracy improved from 0.74532 to 0.74660, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 0.9872 - accuracy: 0.7501 - val_loss: 1.0563 - val_accuracy: 0.7466 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9928 - accuracy: 0.7420\n",
      "Epoch 00025: val_accuracy improved from 0.74660 to 0.75455, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.9905 - accuracy: 0.7423 - val_loss: 1.0484 - val_accuracy: 0.7546 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9563 - accuracy: 0.7557\n",
      "Epoch 00026: val_accuracy improved from 0.75455 to 0.75507, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 0.9589 - accuracy: 0.7550 - val_loss: 1.0483 - val_accuracy: 0.7551 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9584 - accuracy: 0.7572\n",
      "Epoch 00027: val_accuracy did not improve from 0.75507\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9600 - accuracy: 0.7566 - val_loss: 1.0862 - val_accuracy: 0.7425 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9574 - accuracy: 0.7542\n",
      "Epoch 00028: val_accuracy did not improve from 0.75507\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9622 - accuracy: 0.7527 - val_loss: 1.0601 - val_accuracy: 0.7530 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9558 - accuracy: 0.7538\n",
      "Epoch 00029: val_accuracy improved from 0.75507 to 0.75635, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.9561 - accuracy: 0.7534 - val_loss: 1.0282 - val_accuracy: 0.7563 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9294 - accuracy: 0.7603\n",
      "Epoch 00030: val_accuracy did not improve from 0.75635\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.9317 - accuracy: 0.7599 - val_loss: 1.0424 - val_accuracy: 0.7553 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9050 - accuracy: 0.7712\n",
      "Epoch 00031: val_accuracy did not improve from 0.75635\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.9060 - accuracy: 0.7711 - val_loss: 1.0409 - val_accuracy: 0.7556 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8890 - accuracy: 0.7728\n",
      "Epoch 00032: val_accuracy improved from 0.75635 to 0.76712, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.8888 - accuracy: 0.7726 - val_loss: 1.0108 - val_accuracy: 0.7671 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8686 - accuracy: 0.7738\n",
      "Epoch 00033: val_accuracy improved from 0.76712 to 0.77071, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.8682 - accuracy: 0.7740 - val_loss: 1.0069 - val_accuracy: 0.7707 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8534 - accuracy: 0.7859\n",
      "Epoch 00034: val_accuracy did not improve from 0.77071\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8543 - accuracy: 0.7857 - val_loss: 1.0132 - val_accuracy: 0.7669 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8418 - accuracy: 0.7900\n",
      "Epoch 00035: val_accuracy did not improve from 0.77071\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.8427 - accuracy: 0.7894 - val_loss: 1.0107 - val_accuracy: 0.7643 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8215 - accuracy: 0.7927\n",
      "Epoch 00036: val_accuracy improved from 0.77071 to 0.77225, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.8210 - accuracy: 0.7924 - val_loss: 0.9921 - val_accuracy: 0.7722 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8360 - accuracy: 0.7938\n",
      "Epoch 00037: val_accuracy improved from 0.77225 to 0.77404, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.8363 - accuracy: 0.7935 - val_loss: 0.9898 - val_accuracy: 0.7740 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7979 - accuracy: 0.8038\n",
      "Epoch 00038: val_accuracy did not improve from 0.77404\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7992 - accuracy: 0.8032 - val_loss: 0.9986 - val_accuracy: 0.7722 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7991 - accuracy: 0.8004\n",
      "Epoch 00039: val_accuracy did not improve from 0.77404\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7988 - accuracy: 0.8006 - val_loss: 1.0036 - val_accuracy: 0.7733 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7723 - accuracy: 0.8089\n",
      "Epoch 00040: val_accuracy improved from 0.77404 to 0.77917, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.7726 - accuracy: 0.8087 - val_loss: 0.9828 - val_accuracy: 0.7792 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7450 - accuracy: 0.8190\n",
      "Epoch 00041: val_accuracy improved from 0.77917 to 0.78123, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.7451 - accuracy: 0.8189 - val_loss: 0.9731 - val_accuracy: 0.7812 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7492 - accuracy: 0.8172\n",
      "Epoch 00042: val_accuracy improved from 0.78123 to 0.78302, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.7503 - accuracy: 0.8165 - val_loss: 0.9721 - val_accuracy: 0.7830 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7289 - accuracy: 0.8237\n",
      "Epoch 00043: val_accuracy improved from 0.78302 to 0.78456, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.7272 - accuracy: 0.8242 - val_loss: 0.9658 - val_accuracy: 0.7846 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7277 - accuracy: 0.8232\n",
      "Epoch 00044: val_accuracy improved from 0.78456 to 0.78764, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 0.7286 - accuracy: 0.8227 - val_loss: 0.9641 - val_accuracy: 0.7876 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7211 - accuracy: 0.8216\n",
      "Epoch 00045: val_accuracy did not improve from 0.78764\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.7228 - accuracy: 0.8205 - val_loss: 0.9647 - val_accuracy: 0.7864 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7060 - accuracy: 0.8310\n",
      "Epoch 00046: val_accuracy did not improve from 0.78764\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.7073 - accuracy: 0.8309 - val_loss: 0.9628 - val_accuracy: 0.7848 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7108 - accuracy: 0.8284\n",
      "Epoch 00047: val_accuracy did not improve from 0.78764\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.7138 - accuracy: 0.8280 - val_loss: 0.9717 - val_accuracy: 0.7848 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7104 - accuracy: 0.8259\n",
      "Epoch 00048: val_accuracy did not improve from 0.78764\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7110 - accuracy: 0.8257 - val_loss: 0.9628 - val_accuracy: 0.7840 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7017 - accuracy: 0.8323\n",
      "Epoch 00049: val_accuracy did not improve from 0.78764\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7013 - accuracy: 0.8328 - val_loss: 0.9533 - val_accuracy: 0.7848 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6894 - accuracy: 0.8353\n",
      "Epoch 00050: val_accuracy did not improve from 0.78764\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6894 - accuracy: 0.8351 - val_loss: 0.9531 - val_accuracy: 0.7835 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6930 - accuracy: 0.8393\n",
      "Epoch 00051: val_accuracy did not improve from 0.78764\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6916 - accuracy: 0.8397 - val_loss: 0.9583 - val_accuracy: 0.7861 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6899 - accuracy: 0.8341\n",
      "Epoch 00052: val_accuracy did not improve from 0.78764\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6906 - accuracy: 0.8340 - val_loss: 0.9542 - val_accuracy: 0.7861 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6934 - accuracy: 0.8346\n",
      "Epoch 00053: val_accuracy did not improve from 0.78764\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.6930 - accuracy: 0.8346 - val_loss: 0.9517 - val_accuracy: 0.7869 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6831 - accuracy: 0.8353\n",
      "Epoch 00054: val_accuracy did not improve from 0.78764\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.6835 - accuracy: 0.8353 - val_loss: 0.9507 - val_accuracy: 0.7871 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6867 - accuracy: 0.8377\n",
      "Epoch 00055: val_accuracy did not improve from 0.78764\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6873 - accuracy: 0.8376 - val_loss: 0.9597 - val_accuracy: 0.7866 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6765 - accuracy: 0.8373\n",
      "Epoch 00056: val_accuracy did not improve from 0.78764\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6779 - accuracy: 0.8366 - val_loss: 0.9545 - val_accuracy: 0.7874 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6728 - accuracy: 0.8414\n",
      "Epoch 00057: val_accuracy improved from 0.78764 to 0.78969, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 0.6753 - accuracy: 0.8406 - val_loss: 0.9490 - val_accuracy: 0.7897 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6704 - accuracy: 0.8407\n",
      "Epoch 00058: val_accuracy improved from 0.78969 to 0.79174, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.6715 - accuracy: 0.8404 - val_loss: 0.9470 - val_accuracy: 0.7917 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6714 - accuracy: 0.8399\n",
      "Epoch 00059: val_accuracy did not improve from 0.79174\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6720 - accuracy: 0.8403 - val_loss: 0.9423 - val_accuracy: 0.7894 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6680 - accuracy: 0.8411\n",
      "Epoch 00060: val_accuracy did not improve from 0.79174\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.6732 - accuracy: 0.8395 - val_loss: 0.9486 - val_accuracy: 0.7894 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6699 - accuracy: 0.8429\n",
      "Epoch 00061: val_accuracy did not improve from 0.79174\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6723 - accuracy: 0.8426 - val_loss: 0.9494 - val_accuracy: 0.7869 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6646 - accuracy: 0.8372\n",
      "Epoch 00062: val_accuracy did not improve from 0.79174\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6671 - accuracy: 0.8367 - val_loss: 0.9450 - val_accuracy: 0.7882 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6684 - accuracy: 0.8377\n",
      "Epoch 00063: val_accuracy did not improve from 0.79174\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6700 - accuracy: 0.8378 - val_loss: 0.9494 - val_accuracy: 0.7892 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6623 - accuracy: 0.8419\n",
      "Epoch 00064: val_accuracy did not improve from 0.79174\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6634 - accuracy: 0.8420 - val_loss: 0.9423 - val_accuracy: 0.7897 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6581 - accuracy: 0.8397\n",
      "Epoch 00065: val_accuracy did not improve from 0.79174\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6582 - accuracy: 0.8397 - val_loss: 0.9476 - val_accuracy: 0.7871 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6540 - accuracy: 0.8490\n",
      "Epoch 00066: val_accuracy did not improve from 0.79174\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6535 - accuracy: 0.8493 - val_loss: 0.9516 - val_accuracy: 0.7887 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6522 - accuracy: 0.8466\n",
      "Epoch 00067: val_accuracy did not improve from 0.79174\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6530 - accuracy: 0.8464 - val_loss: 0.9510 - val_accuracy: 0.7882 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6588 - accuracy: 0.8476\n",
      "Epoch 00068: val_accuracy did not improve from 0.79174\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6611 - accuracy: 0.8462 - val_loss: 0.9488 - val_accuracy: 0.7889 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6541 - accuracy: 0.8459\n",
      "Epoch 00069: val_accuracy did not improve from 0.79174\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6563 - accuracy: 0.8454 - val_loss: 0.9467 - val_accuracy: 0.7910 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6439 - accuracy: 0.8491\n",
      "Epoch 00070: val_accuracy did not improve from 0.79174\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6447 - accuracy: 0.8490 - val_loss: 0.9478 - val_accuracy: 0.7866 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6408 - accuracy: 0.8480\n",
      "Epoch 00071: val_accuracy did not improve from 0.79174\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6416 - accuracy: 0.8478 - val_loss: 0.9422 - val_accuracy: 0.7879 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6493 - accuracy: 0.8468\n",
      "Epoch 00072: val_accuracy improved from 0.79174 to 0.79405, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 0.6526 - accuracy: 0.8461 - val_loss: 0.9414 - val_accuracy: 0.7940 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6413 - accuracy: 0.8499\n",
      "Epoch 00073: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.6434 - accuracy: 0.8495 - val_loss: 0.9480 - val_accuracy: 0.7892 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6549 - accuracy: 0.8420\n",
      "Epoch 00074: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.6547 - accuracy: 0.8424 - val_loss: 0.9491 - val_accuracy: 0.7897 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6509 - accuracy: 0.8430\n",
      "Epoch 00075: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6528 - accuracy: 0.8423 - val_loss: 0.9416 - val_accuracy: 0.7889 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6430 - accuracy: 0.8457\n",
      "Epoch 00076: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6454 - accuracy: 0.8446 - val_loss: 0.9436 - val_accuracy: 0.7910 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6304 - accuracy: 0.8526\n",
      "Epoch 00077: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6305 - accuracy: 0.8523 - val_loss: 0.9494 - val_accuracy: 0.7892 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6293 - accuracy: 0.8516\n",
      "Epoch 00078: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6301 - accuracy: 0.8512 - val_loss: 0.9499 - val_accuracy: 0.7861 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6390 - accuracy: 0.8488\n",
      "Epoch 00079: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6391 - accuracy: 0.8488 - val_loss: 0.9466 - val_accuracy: 0.7892 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6271 - accuracy: 0.8517\n",
      "Epoch 00080: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6279 - accuracy: 0.8519 - val_loss: 0.9402 - val_accuracy: 0.7925 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6174 - accuracy: 0.8570\n",
      "Epoch 00081: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6189 - accuracy: 0.8567 - val_loss: 0.9418 - val_accuracy: 0.7902 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6241 - accuracy: 0.8538\n",
      "Epoch 00082: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6250 - accuracy: 0.8535 - val_loss: 0.9431 - val_accuracy: 0.7894 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6251 - accuracy: 0.8521\n",
      "Epoch 00083: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6250 - accuracy: 0.8523 - val_loss: 0.9425 - val_accuracy: 0.7894 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6301 - accuracy: 0.8496\n",
      "Epoch 00084: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.6305 - accuracy: 0.8497 - val_loss: 0.9389 - val_accuracy: 0.7899 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6273 - accuracy: 0.8519\n",
      "Epoch 00085: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6283 - accuracy: 0.8516 - val_loss: 0.9391 - val_accuracy: 0.7882 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6249 - accuracy: 0.8531\n",
      "Epoch 00086: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6260 - accuracy: 0.8533 - val_loss: 0.9398 - val_accuracy: 0.7917 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6318 - accuracy: 0.8511\n",
      "Epoch 00087: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6312 - accuracy: 0.8510 - val_loss: 0.9389 - val_accuracy: 0.7892 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6218 - accuracy: 0.8500\n",
      "Epoch 00088: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6247 - accuracy: 0.8493 - val_loss: 0.9399 - val_accuracy: 0.7905 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6302 - accuracy: 0.8516\n",
      "Epoch 00089: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6314 - accuracy: 0.8513 - val_loss: 0.9437 - val_accuracy: 0.7897 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6345 - accuracy: 0.8472\n",
      "Epoch 00090: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6348 - accuracy: 0.8471 - val_loss: 0.9421 - val_accuracy: 0.7897 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6229 - accuracy: 0.8537\n",
      "Epoch 00091: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6229 - accuracy: 0.8537 - val_loss: 0.9424 - val_accuracy: 0.7907 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6189 - accuracy: 0.8570\n",
      "Epoch 00092: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6211 - accuracy: 0.8560 - val_loss: 0.9433 - val_accuracy: 0.7897 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6127 - accuracy: 0.8578\n",
      "Epoch 00093: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.6123 - accuracy: 0.8581 - val_loss: 0.9427 - val_accuracy: 0.7879 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6127 - accuracy: 0.8546\n",
      "Epoch 00094: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.6161 - accuracy: 0.8535 - val_loss: 0.9455 - val_accuracy: 0.7876 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6178 - accuracy: 0.8597\n",
      "Epoch 00095: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.6218 - accuracy: 0.8582 - val_loss: 0.9426 - val_accuracy: 0.7884 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6215 - accuracy: 0.8559\n",
      "Epoch 00096: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6217 - accuracy: 0.8558 - val_loss: 0.9428 - val_accuracy: 0.7889 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6209 - accuracy: 0.8521\n",
      "Epoch 00097: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6227 - accuracy: 0.8520 - val_loss: 0.9409 - val_accuracy: 0.7897 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6195 - accuracy: 0.8567\n",
      "Epoch 00098: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6200 - accuracy: 0.8565 - val_loss: 0.9410 - val_accuracy: 0.7899 - lr: 1.0000e-05\n",
      "Epoch 99/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6152 - accuracy: 0.8570\n",
      "Epoch 00099: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6174 - accuracy: 0.8561 - val_loss: 0.9401 - val_accuracy: 0.7897 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6100 - accuracy: 0.8606\n",
      "Epoch 00100: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6087 - accuracy: 0.8611 - val_loss: 0.9399 - val_accuracy: 0.7884 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6124 - accuracy: 0.8596\n",
      "Epoch 00101: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6127 - accuracy: 0.8592 - val_loss: 0.9395 - val_accuracy: 0.7882 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6152 - accuracy: 0.8547\n",
      "Epoch 00102: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6184 - accuracy: 0.8539 - val_loss: 0.9435 - val_accuracy: 0.7902 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6220 - accuracy: 0.8523\n",
      "Epoch 00103: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6243 - accuracy: 0.8520 - val_loss: 0.9415 - val_accuracy: 0.7928 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6088 - accuracy: 0.8586\n",
      "Epoch 00104: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6086 - accuracy: 0.8588 - val_loss: 0.9417 - val_accuracy: 0.7892 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6123 - accuracy: 0.8584\n",
      "Epoch 00105: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6132 - accuracy: 0.8584 - val_loss: 0.9424 - val_accuracy: 0.7884 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6115 - accuracy: 0.8582\n",
      "Epoch 00106: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6154 - accuracy: 0.8568 - val_loss: 0.9376 - val_accuracy: 0.7894 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6173 - accuracy: 0.8541\n",
      "Epoch 00107: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6195 - accuracy: 0.8536 - val_loss: 0.9383 - val_accuracy: 0.7917 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6241 - accuracy: 0.8559\n",
      "Epoch 00108: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.6242 - accuracy: 0.8555 - val_loss: 0.9393 - val_accuracy: 0.7887 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6107 - accuracy: 0.8578\n",
      "Epoch 00109: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6131 - accuracy: 0.8572 - val_loss: 0.9383 - val_accuracy: 0.7892 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6004 - accuracy: 0.8618\n",
      "Epoch 00110: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6029 - accuracy: 0.8603 - val_loss: 0.9409 - val_accuracy: 0.7897 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6079 - accuracy: 0.8606\n",
      "Epoch 00111: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6090 - accuracy: 0.8605 - val_loss: 0.9401 - val_accuracy: 0.7882 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6239 - accuracy: 0.8551\n",
      "Epoch 00112: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6238 - accuracy: 0.8551 - val_loss: 0.9416 - val_accuracy: 0.7894 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6082 - accuracy: 0.8574\n",
      "Epoch 00113: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6109 - accuracy: 0.8565 - val_loss: 0.9386 - val_accuracy: 0.7907 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6086 - accuracy: 0.8576\n",
      "Epoch 00114: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6107 - accuracy: 0.8567 - val_loss: 0.9381 - val_accuracy: 0.7899 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6126 - accuracy: 0.8580\n",
      "Epoch 00115: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6132 - accuracy: 0.8578 - val_loss: 0.9411 - val_accuracy: 0.7892 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6138 - accuracy: 0.8567\n",
      "Epoch 00116: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.6134 - accuracy: 0.8565 - val_loss: 0.9421 - val_accuracy: 0.7892 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6043 - accuracy: 0.8621\n",
      "Epoch 00117: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6079 - accuracy: 0.8611 - val_loss: 0.9397 - val_accuracy: 0.7889 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6167 - accuracy: 0.8543\n",
      "Epoch 00118: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.6163 - accuracy: 0.8547 - val_loss: 0.9394 - val_accuracy: 0.7907 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6076 - accuracy: 0.8598\n",
      "Epoch 00119: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.6077 - accuracy: 0.8598 - val_loss: 0.9375 - val_accuracy: 0.7897 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.6064 - accuracy: 0.8604\n",
      "Epoch 00120: val_accuracy did not improve from 0.79405\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.6074 - accuracy: 0.8602 - val_loss: 0.9390 - val_accuracy: 0.7884 - lr: 1.0000e-06\n",
      "epoch_number 72\n",
      "train accuracy and validation accuracy 0.8461369276046753 0.7940497398376465\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.9414 - accuracy: 0.7940\n",
      "test_accuracy 0.7940497398376465\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087, 0.7748140692710876, 0.7055655121803284, 0.7168504595756531, 0.6958194375038147, 0.7476276159286499, 0.7235188484191895, 0.7981534004211426, 0.8017440438270569, 0.7732751965522766, 0.7940497398376465]\n",
      "0.7544791670072646\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S22_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S22_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.5719 - accuracy: 0.0894\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.17030, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 3s 346ms/step - loss: 4.5623 - accuracy: 0.0899 - val_loss: 3.6611 - val_accuracy: 0.1703 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.2799 - accuracy: 0.1826\n",
      "Epoch 00002: val_accuracy improved from 0.17030 to 0.26366, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 3.2752 - accuracy: 0.1832 - val_loss: 2.8229 - val_accuracy: 0.2637 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.8658 - accuracy: 0.2598\n",
      "Epoch 00003: val_accuracy improved from 0.26366 to 0.36856, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 2.8664 - accuracy: 0.2601 - val_loss: 2.4784 - val_accuracy: 0.3686 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.5364 - accuracy: 0.3288\n",
      "Epoch 00004: val_accuracy improved from 0.36856 to 0.42421, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 2.5387 - accuracy: 0.3283 - val_loss: 2.2741 - val_accuracy: 0.4242 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.3016 - accuracy: 0.3891\n",
      "Epoch 00005: val_accuracy improved from 0.42421 to 0.47987, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 2.3021 - accuracy: 0.3885 - val_loss: 2.0441 - val_accuracy: 0.4799 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1262 - accuracy: 0.4307\n",
      "Epoch 00006: val_accuracy improved from 0.47987 to 0.51757, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 2.1260 - accuracy: 0.4310 - val_loss: 1.8638 - val_accuracy: 0.5176 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9930 - accuracy: 0.4683\n",
      "Epoch 00007: val_accuracy improved from 0.51757 to 0.54347, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.9934 - accuracy: 0.4673 - val_loss: 1.7293 - val_accuracy: 0.5435 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8541 - accuracy: 0.5007\n",
      "Epoch 00008: val_accuracy improved from 0.54347 to 0.55860, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.8557 - accuracy: 0.5003 - val_loss: 1.6707 - val_accuracy: 0.5586 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7910 - accuracy: 0.5154\n",
      "Epoch 00009: val_accuracy improved from 0.55860 to 0.57605, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.7919 - accuracy: 0.5143 - val_loss: 1.6071 - val_accuracy: 0.5760 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6915 - accuracy: 0.5353\n",
      "Epoch 00010: val_accuracy improved from 0.57605 to 0.58810, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 1.6934 - accuracy: 0.5349 - val_loss: 1.5245 - val_accuracy: 0.5881 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6331 - accuracy: 0.5516\n",
      "Epoch 00011: val_accuracy improved from 0.58810 to 0.62247, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.6312 - accuracy: 0.5520 - val_loss: 1.4505 - val_accuracy: 0.6225 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5856 - accuracy: 0.5681\n",
      "Epoch 00012: val_accuracy did not improve from 0.62247\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.5883 - accuracy: 0.5680 - val_loss: 1.4431 - val_accuracy: 0.6199 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5348 - accuracy: 0.5860\n",
      "Epoch 00013: val_accuracy improved from 0.62247 to 0.63478, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.5351 - accuracy: 0.5861 - val_loss: 1.3773 - val_accuracy: 0.6348 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4722 - accuracy: 0.5949\n",
      "Epoch 00014: val_accuracy improved from 0.63478 to 0.64401, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.4725 - accuracy: 0.5953 - val_loss: 1.3422 - val_accuracy: 0.6440 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4149 - accuracy: 0.6121\n",
      "Epoch 00015: val_accuracy did not improve from 0.64401\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.4149 - accuracy: 0.6125 - val_loss: 1.3523 - val_accuracy: 0.6414 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3876 - accuracy: 0.6224\n",
      "Epoch 00016: val_accuracy improved from 0.64401 to 0.64786, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.3883 - accuracy: 0.6219 - val_loss: 1.3312 - val_accuracy: 0.6479 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3676 - accuracy: 0.6268\n",
      "Epoch 00017: val_accuracy improved from 0.64786 to 0.67043, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.3659 - accuracy: 0.6267 - val_loss: 1.2875 - val_accuracy: 0.6704 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3323 - accuracy: 0.6413\n",
      "Epoch 00018: val_accuracy did not improve from 0.67043\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.3342 - accuracy: 0.6414 - val_loss: 1.2771 - val_accuracy: 0.6666 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3157 - accuracy: 0.6409\n",
      "Epoch 00019: val_accuracy did not improve from 0.67043\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 1.3205 - accuracy: 0.6396 - val_loss: 1.2853 - val_accuracy: 0.6689 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3049 - accuracy: 0.6497\n",
      "Epoch 00020: val_accuracy improved from 0.67043 to 0.67658, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.3078 - accuracy: 0.6482 - val_loss: 1.2650 - val_accuracy: 0.6766 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2817 - accuracy: 0.6542\n",
      "Epoch 00021: val_accuracy improved from 0.67658 to 0.68351, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 1.2820 - accuracy: 0.6542 - val_loss: 1.2344 - val_accuracy: 0.6835 - lr: 0.0010\n",
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2547 - accuracy: 0.6597\n",
      "Epoch 00022: val_accuracy improved from 0.68351 to 0.68684, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 94ms/step - loss: 1.2546 - accuracy: 0.6593 - val_loss: 1.2167 - val_accuracy: 0.6868 - lr: 0.0010\n",
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2015 - accuracy: 0.6778\n",
      "Epoch 00023: val_accuracy did not improve from 0.68684\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.2022 - accuracy: 0.6778 - val_loss: 1.2002 - val_accuracy: 0.6866 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1844 - accuracy: 0.6817\n",
      "Epoch 00024: val_accuracy improved from 0.68684 to 0.69249, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.1843 - accuracy: 0.6815 - val_loss: 1.1908 - val_accuracy: 0.6925 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1531 - accuracy: 0.6867\n",
      "Epoch 00025: val_accuracy improved from 0.69249 to 0.69274, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 1.1520 - accuracy: 0.6870 - val_loss: 1.1961 - val_accuracy: 0.6927 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1320 - accuracy: 0.6940\n",
      "Epoch 00026: val_accuracy improved from 0.69274 to 0.70300, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.1342 - accuracy: 0.6936 - val_loss: 1.1720 - val_accuracy: 0.7030 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1150 - accuracy: 0.6991\n",
      "Epoch 00027: val_accuracy did not improve from 0.70300\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1163 - accuracy: 0.6985 - val_loss: 1.1978 - val_accuracy: 0.6927 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1195 - accuracy: 0.7006\n",
      "Epoch 00028: val_accuracy did not improve from 0.70300\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1198 - accuracy: 0.7010 - val_loss: 1.1892 - val_accuracy: 0.7017 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1168 - accuracy: 0.7006\n",
      "Epoch 00029: val_accuracy did not improve from 0.70300\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1149 - accuracy: 0.7012 - val_loss: 1.2008 - val_accuracy: 0.7007 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0843 - accuracy: 0.7089\n",
      "Epoch 00030: val_accuracy improved from 0.70300 to 0.70839, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1.0842 - accuracy: 0.7092 - val_loss: 1.1702 - val_accuracy: 0.7084 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0823 - accuracy: 0.7083\n",
      "Epoch 00031: val_accuracy improved from 0.70839 to 0.70890, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.0825 - accuracy: 0.7083 - val_loss: 1.1471 - val_accuracy: 0.7089 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0607 - accuracy: 0.7129\n",
      "Epoch 00032: val_accuracy did not improve from 0.70890\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.0604 - accuracy: 0.7124 - val_loss: 1.1647 - val_accuracy: 0.6976 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0426 - accuracy: 0.7156\n",
      "Epoch 00033: val_accuracy did not improve from 0.70890\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.0425 - accuracy: 0.7152 - val_loss: 1.1363 - val_accuracy: 0.7061 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0352 - accuracy: 0.7206\n",
      "Epoch 00034: val_accuracy did not improve from 0.70890\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.0351 - accuracy: 0.7205 - val_loss: 1.1380 - val_accuracy: 0.7081 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9974 - accuracy: 0.7370\n",
      "Epoch 00035: val_accuracy improved from 0.70890 to 0.71275, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.9997 - accuracy: 0.7363 - val_loss: 1.1406 - val_accuracy: 0.7127 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9982 - accuracy: 0.7327\n",
      "Epoch 00036: val_accuracy improved from 0.71275 to 0.71865, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.9993 - accuracy: 0.7326 - val_loss: 1.1228 - val_accuracy: 0.7186 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9831 - accuracy: 0.7374\n",
      "Epoch 00037: val_accuracy did not improve from 0.71865\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9859 - accuracy: 0.7369 - val_loss: 1.1273 - val_accuracy: 0.7068 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9666 - accuracy: 0.7431\n",
      "Epoch 00038: val_accuracy did not improve from 0.71865\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9645 - accuracy: 0.7437 - val_loss: 1.1205 - val_accuracy: 0.7156 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9633 - accuracy: 0.7404\n",
      "Epoch 00039: val_accuracy did not improve from 0.71865\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9649 - accuracy: 0.7401 - val_loss: 1.1150 - val_accuracy: 0.7094 - lr: 0.0010\n",
      "Epoch 40/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9414 - accuracy: 0.7502\n",
      "Epoch 00040: val_accuracy improved from 0.71865 to 0.72198, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 0.9414 - accuracy: 0.7502 - val_loss: 1.0980 - val_accuracy: 0.7220 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8988 - accuracy: 0.7590\n",
      "Epoch 00041: val_accuracy improved from 0.72198 to 0.72711, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.9023 - accuracy: 0.7583 - val_loss: 1.0838 - val_accuracy: 0.7271 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9095 - accuracy: 0.7586\n",
      "Epoch 00042: val_accuracy improved from 0.72711 to 0.72788, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 0.9111 - accuracy: 0.7578 - val_loss: 1.0833 - val_accuracy: 0.7279 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9046 - accuracy: 0.7636\n",
      "Epoch 00043: val_accuracy improved from 0.72788 to 0.73224, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 0.9047 - accuracy: 0.7635 - val_loss: 1.0757 - val_accuracy: 0.7322 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8945 - accuracy: 0.7652\n",
      "Epoch 00044: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8962 - accuracy: 0.7648 - val_loss: 1.0785 - val_accuracy: 0.7317 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8872 - accuracy: 0.7631\n",
      "Epoch 00045: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8893 - accuracy: 0.7627 - val_loss: 1.0738 - val_accuracy: 0.7297 - lr: 1.0000e-04\n",
      "Epoch 46/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8824 - accuracy: 0.7728\n",
      "Epoch 00046: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.8836 - accuracy: 0.7723 - val_loss: 1.0709 - val_accuracy: 0.7317 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8703 - accuracy: 0.7747\n",
      "Epoch 00047: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8729 - accuracy: 0.7739 - val_loss: 1.0703 - val_accuracy: 0.7302 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8718 - accuracy: 0.7759\n",
      "Epoch 00048: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.8747 - accuracy: 0.7749 - val_loss: 1.0673 - val_accuracy: 0.7315 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8536 - accuracy: 0.7789\n",
      "Epoch 00049: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8555 - accuracy: 0.7782 - val_loss: 1.0695 - val_accuracy: 0.7299 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8632 - accuracy: 0.7727\n",
      "Epoch 00050: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8618 - accuracy: 0.7736 - val_loss: 1.0694 - val_accuracy: 0.7299 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8667 - accuracy: 0.7698\n",
      "Epoch 00051: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8689 - accuracy: 0.7688 - val_loss: 1.0695 - val_accuracy: 0.7320 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8556 - accuracy: 0.7757\n",
      "Epoch 00052: val_accuracy did not improve from 0.73224\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.8557 - accuracy: 0.7751 - val_loss: 1.0716 - val_accuracy: 0.7320 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8482 - accuracy: 0.7811\n",
      "Epoch 00053: val_accuracy improved from 0.73224 to 0.73506, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 0.8482 - accuracy: 0.7812 - val_loss: 1.0711 - val_accuracy: 0.7351 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8500 - accuracy: 0.7813\n",
      "Epoch 00054: val_accuracy improved from 0.73506 to 0.73660, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.8547 - accuracy: 0.7796 - val_loss: 1.0667 - val_accuracy: 0.7366 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8476 - accuracy: 0.7812\n",
      "Epoch 00055: val_accuracy did not improve from 0.73660\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8469 - accuracy: 0.7812 - val_loss: 1.0612 - val_accuracy: 0.7358 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8503 - accuracy: 0.7773\n",
      "Epoch 00056: val_accuracy did not improve from 0.73660\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8539 - accuracy: 0.7765 - val_loss: 1.0620 - val_accuracy: 0.7297 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8508 - accuracy: 0.7736\n",
      "Epoch 00057: val_accuracy did not improve from 0.73660\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8521 - accuracy: 0.7734 - val_loss: 1.0645 - val_accuracy: 0.7312 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8310 - accuracy: 0.7842\n",
      "Epoch 00058: val_accuracy did not improve from 0.73660\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8327 - accuracy: 0.7834 - val_loss: 1.0599 - val_accuracy: 0.7333 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8359 - accuracy: 0.7840\n",
      "Epoch 00059: val_accuracy did not improve from 0.73660\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8358 - accuracy: 0.7838 - val_loss: 1.0546 - val_accuracy: 0.7356 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8278 - accuracy: 0.7844\n",
      "Epoch 00060: val_accuracy did not improve from 0.73660\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8297 - accuracy: 0.7839 - val_loss: 1.0604 - val_accuracy: 0.7366 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8427 - accuracy: 0.7783\n",
      "Epoch 00061: val_accuracy did not improve from 0.73660\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8428 - accuracy: 0.7778 - val_loss: 1.0690 - val_accuracy: 0.7348 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8423 - accuracy: 0.7781\n",
      "Epoch 00062: val_accuracy did not improve from 0.73660\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8457 - accuracy: 0.7771 - val_loss: 1.0705 - val_accuracy: 0.7343 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8272 - accuracy: 0.7873\n",
      "Epoch 00063: val_accuracy improved from 0.73660 to 0.73814, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 0.8261 - accuracy: 0.7874 - val_loss: 1.0623 - val_accuracy: 0.7381 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8215 - accuracy: 0.7844\n",
      "Epoch 00064: val_accuracy improved from 0.73814 to 0.73942, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 0.8255 - accuracy: 0.7834 - val_loss: 1.0578 - val_accuracy: 0.7394 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8186 - accuracy: 0.7869\n",
      "Epoch 00065: val_accuracy did not improve from 0.73942\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.8176 - accuracy: 0.7873 - val_loss: 1.0563 - val_accuracy: 0.7366 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8220 - accuracy: 0.7879\n",
      "Epoch 00066: val_accuracy did not improve from 0.73942\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.8242 - accuracy: 0.7874 - val_loss: 1.0584 - val_accuracy: 0.7358 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8185 - accuracy: 0.7826\n",
      "Epoch 00067: val_accuracy did not improve from 0.73942\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8200 - accuracy: 0.7825 - val_loss: 1.0576 - val_accuracy: 0.7394 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8115 - accuracy: 0.7886\n",
      "Epoch 00068: val_accuracy did not improve from 0.73942\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8132 - accuracy: 0.7884 - val_loss: 1.0607 - val_accuracy: 0.7371 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8315 - accuracy: 0.7821\n",
      "Epoch 00069: val_accuracy did not improve from 0.73942\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8334 - accuracy: 0.7817 - val_loss: 1.0591 - val_accuracy: 0.7394 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8203 - accuracy: 0.7869\n",
      "Epoch 00070: val_accuracy did not improve from 0.73942\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8211 - accuracy: 0.7865 - val_loss: 1.0564 - val_accuracy: 0.7389 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8085 - accuracy: 0.7920\n",
      "Epoch 00071: val_accuracy improved from 0.73942 to 0.74019, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.8121 - accuracy: 0.7913 - val_loss: 1.0559 - val_accuracy: 0.7402 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8118 - accuracy: 0.7868\n",
      "Epoch 00072: val_accuracy did not improve from 0.74019\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8141 - accuracy: 0.7858 - val_loss: 1.0580 - val_accuracy: 0.7366 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8123 - accuracy: 0.7900\n",
      "Epoch 00073: val_accuracy did not improve from 0.74019\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8137 - accuracy: 0.7894 - val_loss: 1.0540 - val_accuracy: 0.7366 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8048 - accuracy: 0.7933\n",
      "Epoch 00074: val_accuracy did not improve from 0.74019\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8044 - accuracy: 0.7932 - val_loss: 1.0540 - val_accuracy: 0.7366 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8142 - accuracy: 0.7884\n",
      "Epoch 00075: val_accuracy did not improve from 0.74019\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8160 - accuracy: 0.7874 - val_loss: 1.0590 - val_accuracy: 0.7333 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8037 - accuracy: 0.7917\n",
      "Epoch 00076: val_accuracy did not improve from 0.74019\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8072 - accuracy: 0.7909 - val_loss: 1.0587 - val_accuracy: 0.7376 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8058 - accuracy: 0.7921\n",
      "Epoch 00077: val_accuracy did not improve from 0.74019\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8079 - accuracy: 0.7911 - val_loss: 1.0527 - val_accuracy: 0.7366 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7941 - accuracy: 0.7940\n",
      "Epoch 00078: val_accuracy improved from 0.74019 to 0.74045, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.7958 - accuracy: 0.7933 - val_loss: 1.0461 - val_accuracy: 0.7404 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8007 - accuracy: 0.7920\n",
      "Epoch 00079: val_accuracy improved from 0.74045 to 0.74070, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.8041 - accuracy: 0.7909 - val_loss: 1.0508 - val_accuracy: 0.7407 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7945 - accuracy: 0.7957\n",
      "Epoch 00080: val_accuracy improved from 0.74070 to 0.74327, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 0.7945 - accuracy: 0.7958 - val_loss: 1.0498 - val_accuracy: 0.7433 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7917 - accuracy: 0.7989\n",
      "Epoch 00081: val_accuracy did not improve from 0.74327\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7972 - accuracy: 0.7969 - val_loss: 1.0549 - val_accuracy: 0.7392 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7774 - accuracy: 0.7991\n",
      "Epoch 00082: val_accuracy did not improve from 0.74327\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7789 - accuracy: 0.7983 - val_loss: 1.0456 - val_accuracy: 0.7422 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7868 - accuracy: 0.7897\n",
      "Epoch 00083: val_accuracy improved from 0.74327 to 0.74378, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 0.7889 - accuracy: 0.7895 - val_loss: 1.0479 - val_accuracy: 0.7438 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7952 - accuracy: 0.7934\n",
      "Epoch 00084: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.7969 - accuracy: 0.7923 - val_loss: 1.0453 - val_accuracy: 0.7430 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7789 - accuracy: 0.8036\n",
      "Epoch 00085: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7814 - accuracy: 0.8026 - val_loss: 1.0456 - val_accuracy: 0.7410 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7874 - accuracy: 0.7964\n",
      "Epoch 00086: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7875 - accuracy: 0.7966 - val_loss: 1.0440 - val_accuracy: 0.7420 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7894 - accuracy: 0.7981\n",
      "Epoch 00087: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.7900 - accuracy: 0.7978 - val_loss: 1.0447 - val_accuracy: 0.7417 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7875 - accuracy: 0.7922\n",
      "Epoch 00088: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.7911 - accuracy: 0.7912 - val_loss: 1.0443 - val_accuracy: 0.7420 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7971 - accuracy: 0.7919\n",
      "Epoch 00089: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7991 - accuracy: 0.7909 - val_loss: 1.0477 - val_accuracy: 0.7389 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7884 - accuracy: 0.7982\n",
      "Epoch 00090: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7897 - accuracy: 0.7980 - val_loss: 1.0468 - val_accuracy: 0.7397 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7969 - accuracy: 0.7912\n",
      "Epoch 00091: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7991 - accuracy: 0.7903 - val_loss: 1.0498 - val_accuracy: 0.7389 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7943 - accuracy: 0.7934\n",
      "Epoch 00092: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7970 - accuracy: 0.7924 - val_loss: 1.0486 - val_accuracy: 0.7389 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7907 - accuracy: 0.7913\n",
      "Epoch 00093: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7935 - accuracy: 0.7902 - val_loss: 1.0482 - val_accuracy: 0.7410 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7897 - accuracy: 0.7988\n",
      "Epoch 00094: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7917 - accuracy: 0.7979 - val_loss: 1.0488 - val_accuracy: 0.7381 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7848 - accuracy: 0.7928\n",
      "Epoch 00095: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7841 - accuracy: 0.7934 - val_loss: 1.0463 - val_accuracy: 0.7387 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7745 - accuracy: 0.8010\n",
      "Epoch 00096: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7770 - accuracy: 0.8002 - val_loss: 1.0455 - val_accuracy: 0.7394 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7803 - accuracy: 0.7976\n",
      "Epoch 00097: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.7838 - accuracy: 0.7968 - val_loss: 1.0463 - val_accuracy: 0.7399 - lr: 1.0000e-05\n",
      "Epoch 98/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7847 - accuracy: 0.7959\n",
      "Epoch 00098: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7851 - accuracy: 0.7954 - val_loss: 1.0466 - val_accuracy: 0.7402 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7750 - accuracy: 0.7999\n",
      "Epoch 00099: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7768 - accuracy: 0.7993 - val_loss: 1.0462 - val_accuracy: 0.7397 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7867 - accuracy: 0.7992\n",
      "Epoch 00100: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7862 - accuracy: 0.7992 - val_loss: 1.0476 - val_accuracy: 0.7397 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7803 - accuracy: 0.7984\n",
      "Epoch 00101: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.7821 - accuracy: 0.7979 - val_loss: 1.0477 - val_accuracy: 0.7389 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7777 - accuracy: 0.7984\n",
      "Epoch 00102: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.7797 - accuracy: 0.7977 - val_loss: 1.0410 - val_accuracy: 0.7397 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7829 - accuracy: 0.7980\n",
      "Epoch 00103: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7847 - accuracy: 0.7978 - val_loss: 1.0466 - val_accuracy: 0.7397 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7806 - accuracy: 0.7959\n",
      "Epoch 00104: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.7817 - accuracy: 0.7955 - val_loss: 1.0444 - val_accuracy: 0.7397 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7798 - accuracy: 0.7960\n",
      "Epoch 00105: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7800 - accuracy: 0.7956 - val_loss: 1.0445 - val_accuracy: 0.7397 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7800 - accuracy: 0.7972\n",
      "Epoch 00106: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7810 - accuracy: 0.7967 - val_loss: 1.0468 - val_accuracy: 0.7404 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7745 - accuracy: 0.8000\n",
      "Epoch 00107: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7745 - accuracy: 0.7999 - val_loss: 1.0456 - val_accuracy: 0.7425 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7771 - accuracy: 0.8001\n",
      "Epoch 00108: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.7775 - accuracy: 0.7994 - val_loss: 1.0453 - val_accuracy: 0.7399 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7798 - accuracy: 0.7976\n",
      "Epoch 00109: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.7817 - accuracy: 0.7968 - val_loss: 1.0463 - val_accuracy: 0.7397 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7830 - accuracy: 0.7960\n",
      "Epoch 00110: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.7866 - accuracy: 0.7949 - val_loss: 1.0482 - val_accuracy: 0.7402 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7712 - accuracy: 0.8014\n",
      "Epoch 00111: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7729 - accuracy: 0.8010 - val_loss: 1.0464 - val_accuracy: 0.7415 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7816 - accuracy: 0.8007\n",
      "Epoch 00112: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7816 - accuracy: 0.8004 - val_loss: 1.0473 - val_accuracy: 0.7417 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7905 - accuracy: 0.7977\n",
      "Epoch 00113: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7935 - accuracy: 0.7965 - val_loss: 1.0447 - val_accuracy: 0.7384 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7760 - accuracy: 0.7983\n",
      "Epoch 00114: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7773 - accuracy: 0.7980 - val_loss: 1.0449 - val_accuracy: 0.7376 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7709 - accuracy: 0.8033\n",
      "Epoch 00115: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7723 - accuracy: 0.8031 - val_loss: 1.0457 - val_accuracy: 0.7381 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7802 - accuracy: 0.7984\n",
      "Epoch 00116: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7807 - accuracy: 0.7984 - val_loss: 1.0447 - val_accuracy: 0.7394 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7829 - accuracy: 0.7948\n",
      "Epoch 00117: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7836 - accuracy: 0.7947 - val_loss: 1.0429 - val_accuracy: 0.7402 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7787 - accuracy: 0.7981\n",
      "Epoch 00118: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7809 - accuracy: 0.7972 - val_loss: 1.0495 - val_accuracy: 0.7389 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7752 - accuracy: 0.8062\n",
      "Epoch 00119: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.7746 - accuracy: 0.8065 - val_loss: 1.0475 - val_accuracy: 0.7371 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7860 - accuracy: 0.7938\n",
      "Epoch 00120: val_accuracy did not improve from 0.74378\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7872 - accuracy: 0.7934 - val_loss: 1.0438 - val_accuracy: 0.7376 - lr: 1.0000e-06\n",
      "epoch_number 83\n",
      "train accuracy and validation accuracy 0.7895373106002808 0.7437804341316223\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.0479 - accuracy: 0.7438\n",
      "test_accuracy 0.7437804341316223\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087, 0.7748140692710876, 0.7055655121803284, 0.7168504595756531, 0.6958194375038147, 0.7476276159286499, 0.7235188484191895, 0.7981534004211426, 0.8017440438270569, 0.7732751965522766, 0.7940497398376465, 0.7437804341316223]\n",
      "0.7539928609674628\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S23_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S23_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 5.0093 - accuracy: 0.0861\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.16799, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 3s 341ms/step - loss: 4.9943 - accuracy: 0.0864 - val_loss: 3.9151 - val_accuracy: 0.1680 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.3676 - accuracy: 0.1773\n",
      "Epoch 00002: val_accuracy improved from 0.16799 to 0.26314, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 3.3659 - accuracy: 0.1778 - val_loss: 2.9498 - val_accuracy: 0.2631 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.9537 - accuracy: 0.2612\n",
      "Epoch 00003: val_accuracy improved from 0.26314 to 0.36009, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 2.9538 - accuracy: 0.2611 - val_loss: 2.6199 - val_accuracy: 0.3601 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.6785 - accuracy: 0.3203\n",
      "Epoch 00004: val_accuracy improved from 0.36009 to 0.39779, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 2.6779 - accuracy: 0.3207 - val_loss: 2.3919 - val_accuracy: 0.3978 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4564 - accuracy: 0.3649\n",
      "Epoch 00005: val_accuracy improved from 0.39779 to 0.44960, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 2.4543 - accuracy: 0.3656 - val_loss: 2.2038 - val_accuracy: 0.4496 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2997 - accuracy: 0.4001\n",
      "Epoch 00006: val_accuracy improved from 0.44960 to 0.48961, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 2.2996 - accuracy: 0.4003 - val_loss: 2.0737 - val_accuracy: 0.4896 - lr: 0.0010\n",
      "Epoch 7/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 2.1640 - accuracy: 0.4406\n",
      "Epoch 00007: val_accuracy improved from 0.48961 to 0.52834, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 2.1640 - accuracy: 0.4406 - val_loss: 1.9524 - val_accuracy: 0.5283 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0535 - accuracy: 0.4614\n",
      "Epoch 00008: val_accuracy improved from 0.52834 to 0.53527, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 2.0538 - accuracy: 0.4613 - val_loss: 1.8881 - val_accuracy: 0.5353 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9702 - accuracy: 0.4829\n",
      "Epoch 00009: val_accuracy improved from 0.53527 to 0.56296, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.9728 - accuracy: 0.4830 - val_loss: 1.7859 - val_accuracy: 0.5630 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9034 - accuracy: 0.5083\n",
      "Epoch 00010: val_accuracy improved from 0.56296 to 0.57989, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1.9064 - accuracy: 0.5073 - val_loss: 1.7339 - val_accuracy: 0.5799 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8320 - accuracy: 0.5197\n",
      "Epoch 00011: val_accuracy improved from 0.57989 to 0.59374, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.8308 - accuracy: 0.5202 - val_loss: 1.6770 - val_accuracy: 0.5937 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7811 - accuracy: 0.5337\n",
      "Epoch 00012: val_accuracy did not improve from 0.59374\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.7823 - accuracy: 0.5330 - val_loss: 1.7017 - val_accuracy: 0.5871 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7186 - accuracy: 0.5531\n",
      "Epoch 00013: val_accuracy improved from 0.59374 to 0.60785, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.7193 - accuracy: 0.5530 - val_loss: 1.6196 - val_accuracy: 0.6078 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6509 - accuracy: 0.5651\n",
      "Epoch 00014: val_accuracy improved from 0.60785 to 0.61272, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.6510 - accuracy: 0.5652 - val_loss: 1.5793 - val_accuracy: 0.6127 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6151 - accuracy: 0.5801\n",
      "Epoch 00015: val_accuracy improved from 0.61272 to 0.63093, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1.6186 - accuracy: 0.5801 - val_loss: 1.5307 - val_accuracy: 0.6309 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5902 - accuracy: 0.5861\n",
      "Epoch 00016: val_accuracy did not improve from 0.63093\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.5956 - accuracy: 0.5850 - val_loss: 1.5358 - val_accuracy: 0.6222 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5686 - accuracy: 0.5960\n",
      "Epoch 00017: val_accuracy did not improve from 0.63093\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.5678 - accuracy: 0.5963 - val_loss: 1.5093 - val_accuracy: 0.6307 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5335 - accuracy: 0.6066\n",
      "Epoch 00018: val_accuracy improved from 0.63093 to 0.64016, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.5326 - accuracy: 0.6068 - val_loss: 1.4855 - val_accuracy: 0.6402 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5095 - accuracy: 0.6098\n",
      "Epoch 00019: val_accuracy did not improve from 0.64016\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.5124 - accuracy: 0.6090 - val_loss: 1.4860 - val_accuracy: 0.6368 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4651 - accuracy: 0.6219\n",
      "Epoch 00020: val_accuracy improved from 0.64016 to 0.64709, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.4649 - accuracy: 0.6220 - val_loss: 1.4800 - val_accuracy: 0.6471 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4359 - accuracy: 0.6287\n",
      "Epoch 00021: val_accuracy did not improve from 0.64709\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.4416 - accuracy: 0.6267 - val_loss: 1.4528 - val_accuracy: 0.6450 - lr: 0.0010\n",
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4205 - accuracy: 0.6373\n",
      "Epoch 00022: val_accuracy improved from 0.64709 to 0.66094, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.4249 - accuracy: 0.6362 - val_loss: 1.4308 - val_accuracy: 0.6609 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4031 - accuracy: 0.6306\n",
      "Epoch 00023: val_accuracy did not improve from 0.66094\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.4037 - accuracy: 0.6309 - val_loss: 1.4193 - val_accuracy: 0.6484 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3847 - accuracy: 0.6452\n",
      "Epoch 00024: val_accuracy did not improve from 0.66094\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.3855 - accuracy: 0.6445 - val_loss: 1.4052 - val_accuracy: 0.6563 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3327 - accuracy: 0.6601\n",
      "Epoch 00025: val_accuracy did not improve from 0.66094\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.3357 - accuracy: 0.6586 - val_loss: 1.4175 - val_accuracy: 0.6568 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3358 - accuracy: 0.6627\n",
      "Epoch 00026: val_accuracy did not improve from 0.66094\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.3364 - accuracy: 0.6628 - val_loss: 1.4212 - val_accuracy: 0.6591 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3081 - accuracy: 0.6627\n",
      "Epoch 00027: val_accuracy improved from 0.66094 to 0.66684, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 1.3082 - accuracy: 0.6623 - val_loss: 1.3883 - val_accuracy: 0.6668 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2843 - accuracy: 0.6682\n",
      "Epoch 00028: val_accuracy did not improve from 0.66684\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.2845 - accuracy: 0.6678 - val_loss: 1.3836 - val_accuracy: 0.6627 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2535 - accuracy: 0.6763\n",
      "Epoch 00029: val_accuracy improved from 0.66684 to 0.66786, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1.2571 - accuracy: 0.6757 - val_loss: 1.3845 - val_accuracy: 0.6679 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2484 - accuracy: 0.6773\n",
      "Epoch 00030: val_accuracy did not improve from 0.66786\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.2469 - accuracy: 0.6774 - val_loss: 1.4093 - val_accuracy: 0.6632 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2312 - accuracy: 0.6868\n",
      "Epoch 00031: val_accuracy improved from 0.66786 to 0.67838, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.2303 - accuracy: 0.6869 - val_loss: 1.3548 - val_accuracy: 0.6784 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2203 - accuracy: 0.6872\n",
      "Epoch 00032: val_accuracy did not improve from 0.67838\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.2213 - accuracy: 0.6867 - val_loss: 1.3794 - val_accuracy: 0.6666 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1904 - accuracy: 0.6951\n",
      "Epoch 00033: val_accuracy improved from 0.67838 to 0.68479, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.1919 - accuracy: 0.6947 - val_loss: 1.3328 - val_accuracy: 0.6848 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2104 - accuracy: 0.6922\n",
      "Epoch 00034: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.2084 - accuracy: 0.6926 - val_loss: 1.3800 - val_accuracy: 0.6681 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1766 - accuracy: 0.7002\n",
      "Epoch 00035: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1789 - accuracy: 0.6994 - val_loss: 1.3584 - val_accuracy: 0.6735 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1601 - accuracy: 0.7008\n",
      "Epoch 00036: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.1613 - accuracy: 0.7000 - val_loss: 1.3485 - val_accuracy: 0.6766 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1562 - accuracy: 0.7006\n",
      "Epoch 00037: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 1.1547 - accuracy: 0.7015 - val_loss: 1.3524 - val_accuracy: 0.6761 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1331 - accuracy: 0.7107\n",
      "Epoch 00038: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1320 - accuracy: 0.7115 - val_loss: 1.3357 - val_accuracy: 0.6830 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1231 - accuracy: 0.7157\n",
      "Epoch 00039: val_accuracy did not improve from 0.68479\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.1254 - accuracy: 0.7149 - val_loss: 1.3272 - val_accuracy: 0.6776 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0850 - accuracy: 0.7273\n",
      "Epoch 00040: val_accuracy improved from 0.68479 to 0.68992, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.0884 - accuracy: 0.7268 - val_loss: 1.3024 - val_accuracy: 0.6899 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0676 - accuracy: 0.7288\n",
      "Epoch 00041: val_accuracy improved from 0.68992 to 0.69223, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 1.0698 - accuracy: 0.7283 - val_loss: 1.3076 - val_accuracy: 0.6922 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0514 - accuracy: 0.7333\n",
      "Epoch 00042: val_accuracy did not improve from 0.69223\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0515 - accuracy: 0.7334 - val_loss: 1.3007 - val_accuracy: 0.6897 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0437 - accuracy: 0.7327\n",
      "Epoch 00043: val_accuracy improved from 0.69223 to 0.69454, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.0455 - accuracy: 0.7321 - val_loss: 1.2958 - val_accuracy: 0.6945 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0292 - accuracy: 0.7378\n",
      "Epoch 00044: val_accuracy did not improve from 0.69454\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.0314 - accuracy: 0.7376 - val_loss: 1.2986 - val_accuracy: 0.6943 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0219 - accuracy: 0.7434\n",
      "Epoch 00045: val_accuracy did not improve from 0.69454\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0237 - accuracy: 0.7429 - val_loss: 1.2936 - val_accuracy: 0.6930 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0183 - accuracy: 0.7417\n",
      "Epoch 00046: val_accuracy did not improve from 0.69454\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.0183 - accuracy: 0.7417 - val_loss: 1.3056 - val_accuracy: 0.6920 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0195 - accuracy: 0.7423\n",
      "Epoch 00047: val_accuracy did not improve from 0.69454\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0189 - accuracy: 0.7422 - val_loss: 1.2929 - val_accuracy: 0.6915 - lr: 1.0000e-04\n",
      "Epoch 48/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9994 - accuracy: 0.7471\n",
      "Epoch 00048: val_accuracy improved from 0.69454 to 0.69659, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1.0033 - accuracy: 0.7455 - val_loss: 1.2930 - val_accuracy: 0.6966 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0057 - accuracy: 0.7463\n",
      "Epoch 00049: val_accuracy improved from 0.69659 to 0.70069, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1.0061 - accuracy: 0.7462 - val_loss: 1.2936 - val_accuracy: 0.7007 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0008 - accuracy: 0.7457\n",
      "Epoch 00050: val_accuracy did not improve from 0.70069\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0011 - accuracy: 0.7455 - val_loss: 1.2832 - val_accuracy: 0.7007 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9868 - accuracy: 0.7499\n",
      "Epoch 00051: val_accuracy did not improve from 0.70069\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9897 - accuracy: 0.7494 - val_loss: 1.2840 - val_accuracy: 0.6981 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9727 - accuracy: 0.7533\n",
      "Epoch 00052: val_accuracy improved from 0.70069 to 0.70146, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.9750 - accuracy: 0.7530 - val_loss: 1.2820 - val_accuracy: 0.7015 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0064 - accuracy: 0.7460\n",
      "Epoch 00053: val_accuracy did not improve from 0.70146\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0071 - accuracy: 0.7454 - val_loss: 1.2804 - val_accuracy: 0.6986 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9911 - accuracy: 0.7469\n",
      "Epoch 00054: val_accuracy did not improve from 0.70146\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9897 - accuracy: 0.7473 - val_loss: 1.2787 - val_accuracy: 0.6984 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9698 - accuracy: 0.7557\n",
      "Epoch 00055: val_accuracy did not improve from 0.70146\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9710 - accuracy: 0.7557 - val_loss: 1.2827 - val_accuracy: 0.6994 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9884 - accuracy: 0.7490\n",
      "Epoch 00056: val_accuracy did not improve from 0.70146\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.9894 - accuracy: 0.7485 - val_loss: 1.2755 - val_accuracy: 0.6984 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9708 - accuracy: 0.7548\n",
      "Epoch 00057: val_accuracy did not improve from 0.70146\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9713 - accuracy: 0.7545 - val_loss: 1.2757 - val_accuracy: 0.6997 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9595 - accuracy: 0.7571\n",
      "Epoch 00058: val_accuracy did not improve from 0.70146\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9632 - accuracy: 0.7563 - val_loss: 1.2807 - val_accuracy: 0.6992 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9691 - accuracy: 0.7558\n",
      "Epoch 00059: val_accuracy did not improve from 0.70146\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9697 - accuracy: 0.7555 - val_loss: 1.2783 - val_accuracy: 0.6984 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9702 - accuracy: 0.7576\n",
      "Epoch 00060: val_accuracy improved from 0.70146 to 0.70351, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 0.9699 - accuracy: 0.7573 - val_loss: 1.2758 - val_accuracy: 0.7035 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9671 - accuracy: 0.7541\n",
      "Epoch 00061: val_accuracy improved from 0.70351 to 0.70377, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 0.9697 - accuracy: 0.7532 - val_loss: 1.2750 - val_accuracy: 0.7038 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9658 - accuracy: 0.7590\n",
      "Epoch 00062: val_accuracy did not improve from 0.70377\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.9660 - accuracy: 0.7593 - val_loss: 1.2750 - val_accuracy: 0.7004 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9405 - accuracy: 0.7629\n",
      "Epoch 00063: val_accuracy did not improve from 0.70377\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9424 - accuracy: 0.7626 - val_loss: 1.2727 - val_accuracy: 0.7017 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9494 - accuracy: 0.7588\n",
      "Epoch 00064: val_accuracy improved from 0.70377 to 0.70454, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 0.9529 - accuracy: 0.7581 - val_loss: 1.2660 - val_accuracy: 0.7045 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9565 - accuracy: 0.7561\n",
      "Epoch 00065: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9587 - accuracy: 0.7550 - val_loss: 1.2700 - val_accuracy: 0.7030 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9461 - accuracy: 0.7640\n",
      "Epoch 00066: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9470 - accuracy: 0.7635 - val_loss: 1.2759 - val_accuracy: 0.7030 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9533 - accuracy: 0.7617\n",
      "Epoch 00067: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9566 - accuracy: 0.7609 - val_loss: 1.2755 - val_accuracy: 0.7027 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9461 - accuracy: 0.7579\n",
      "Epoch 00068: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9508 - accuracy: 0.7569 - val_loss: 1.2779 - val_accuracy: 0.7040 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9392 - accuracy: 0.7627\n",
      "Epoch 00069: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9383 - accuracy: 0.7633 - val_loss: 1.2788 - val_accuracy: 0.7012 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9269 - accuracy: 0.7632\n",
      "Epoch 00070: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.9277 - accuracy: 0.7626 - val_loss: 1.2688 - val_accuracy: 0.6968 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9399 - accuracy: 0.7600\n",
      "Epoch 00071: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9423 - accuracy: 0.7592 - val_loss: 1.2728 - val_accuracy: 0.6999 - lr: 1.0000e-04\n",
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9395 - accuracy: 0.7609\n",
      "Epoch 00072: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9408 - accuracy: 0.7603 - val_loss: 1.2743 - val_accuracy: 0.7030 - lr: 1.0000e-04\n",
      "Epoch 73/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9305 - accuracy: 0.7653\n",
      "Epoch 00073: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.9308 - accuracy: 0.7652 - val_loss: 1.2659 - val_accuracy: 0.7007 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9228 - accuracy: 0.7687\n",
      "Epoch 00074: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9235 - accuracy: 0.7683 - val_loss: 1.2660 - val_accuracy: 0.7004 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9433 - accuracy: 0.7633\n",
      "Epoch 00075: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9452 - accuracy: 0.7631 - val_loss: 1.2752 - val_accuracy: 0.6986 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9316 - accuracy: 0.7588\n",
      "Epoch 00076: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.9336 - accuracy: 0.7589 - val_loss: 1.2783 - val_accuracy: 0.7020 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9158 - accuracy: 0.7680\n",
      "Epoch 00077: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9163 - accuracy: 0.7680 - val_loss: 1.2871 - val_accuracy: 0.7017 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9116 - accuracy: 0.7722\n",
      "Epoch 00078: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9135 - accuracy: 0.7717 - val_loss: 1.2775 - val_accuracy: 0.7009 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9129 - accuracy: 0.7700\n",
      "Epoch 00079: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9140 - accuracy: 0.7695 - val_loss: 1.2786 - val_accuracy: 0.7007 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9163 - accuracy: 0.7690\n",
      "Epoch 00080: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9178 - accuracy: 0.7687 - val_loss: 1.2772 - val_accuracy: 0.7020 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9035 - accuracy: 0.7714\n",
      "Epoch 00081: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9032 - accuracy: 0.7715 - val_loss: 1.2800 - val_accuracy: 0.6974 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9146 - accuracy: 0.7732\n",
      "Epoch 00082: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9149 - accuracy: 0.7731 - val_loss: 1.2745 - val_accuracy: 0.7025 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9187 - accuracy: 0.7690\n",
      "Epoch 00083: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9173 - accuracy: 0.7695 - val_loss: 1.2746 - val_accuracy: 0.7022 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9248 - accuracy: 0.7579\n",
      "Epoch 00084: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9267 - accuracy: 0.7572 - val_loss: 1.2780 - val_accuracy: 0.7020 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9065 - accuracy: 0.7749\n",
      "Epoch 00085: val_accuracy did not improve from 0.70454\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9080 - accuracy: 0.7749 - val_loss: 1.2745 - val_accuracy: 0.7007 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9084 - accuracy: 0.7657\n",
      "Epoch 00086: val_accuracy improved from 0.70454 to 0.70557, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 0.9113 - accuracy: 0.7647 - val_loss: 1.2759 - val_accuracy: 0.7056 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9036 - accuracy: 0.7697\n",
      "Epoch 00087: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9062 - accuracy: 0.7689 - val_loss: 1.2804 - val_accuracy: 0.7012 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9044 - accuracy: 0.7676\n",
      "Epoch 00088: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.9040 - accuracy: 0.7680 - val_loss: 1.2703 - val_accuracy: 0.7020 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9138 - accuracy: 0.7711\n",
      "Epoch 00089: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9136 - accuracy: 0.7710 - val_loss: 1.2756 - val_accuracy: 0.7009 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9069 - accuracy: 0.7746\n",
      "Epoch 00090: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9069 - accuracy: 0.7743 - val_loss: 1.2714 - val_accuracy: 0.7048 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9141 - accuracy: 0.7724\n",
      "Epoch 00091: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9123 - accuracy: 0.7731 - val_loss: 1.2709 - val_accuracy: 0.7030 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9010 - accuracy: 0.7744\n",
      "Epoch 00092: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9027 - accuracy: 0.7736 - val_loss: 1.2703 - val_accuracy: 0.7017 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9018 - accuracy: 0.7723\n",
      "Epoch 00093: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9006 - accuracy: 0.7725 - val_loss: 1.2680 - val_accuracy: 0.7025 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9018 - accuracy: 0.7746\n",
      "Epoch 00094: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9024 - accuracy: 0.7746 - val_loss: 1.2718 - val_accuracy: 0.7035 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.9124 - accuracy: 0.7701\n",
      "Epoch 00095: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 68ms/step - loss: 0.9124 - accuracy: 0.7701 - val_loss: 1.2675 - val_accuracy: 0.7053 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8990 - accuracy: 0.7767\n",
      "Epoch 00096: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9008 - accuracy: 0.7761 - val_loss: 1.2673 - val_accuracy: 0.7027 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9048 - accuracy: 0.7722\n",
      "Epoch 00097: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9062 - accuracy: 0.7715 - val_loss: 1.2677 - val_accuracy: 0.7053 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9040 - accuracy: 0.7719\n",
      "Epoch 00098: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9077 - accuracy: 0.7713 - val_loss: 1.2709 - val_accuracy: 0.7015 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9081 - accuracy: 0.7677\n",
      "Epoch 00099: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9089 - accuracy: 0.7672 - val_loss: 1.2679 - val_accuracy: 0.7022 - lr: 1.0000e-05\n",
      "Epoch 100/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9129 - accuracy: 0.7670\n",
      "Epoch 00100: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.9132 - accuracy: 0.7669 - val_loss: 1.2643 - val_accuracy: 0.7027 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9092 - accuracy: 0.7687\n",
      "Epoch 00101: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9142 - accuracy: 0.7670 - val_loss: 1.2660 - val_accuracy: 0.7048 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9118 - accuracy: 0.7689\n",
      "Epoch 00102: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9133 - accuracy: 0.7684 - val_loss: 1.2676 - val_accuracy: 0.7033 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8972 - accuracy: 0.7760\n",
      "Epoch 00103: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8999 - accuracy: 0.7753 - val_loss: 1.2711 - val_accuracy: 0.7030 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9106 - accuracy: 0.7674\n",
      "Epoch 00104: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9114 - accuracy: 0.7670 - val_loss: 1.2679 - val_accuracy: 0.7030 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9017 - accuracy: 0.7707\n",
      "Epoch 00105: val_accuracy did not improve from 0.70557\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.9019 - accuracy: 0.7707 - val_loss: 1.2653 - val_accuracy: 0.7048 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9152 - accuracy: 0.7682\n",
      "Epoch 00106: val_accuracy improved from 0.70557 to 0.70633, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 0.9157 - accuracy: 0.7677 - val_loss: 1.2629 - val_accuracy: 0.7063 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9132 - accuracy: 0.7734\n",
      "Epoch 00107: val_accuracy did not improve from 0.70633\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.9116 - accuracy: 0.7736 - val_loss: 1.2629 - val_accuracy: 0.7053 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8969 - accuracy: 0.7751\n",
      "Epoch 00108: val_accuracy did not improve from 0.70633\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8959 - accuracy: 0.7757 - val_loss: 1.2661 - val_accuracy: 0.7053 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8969 - accuracy: 0.7738\n",
      "Epoch 00109: val_accuracy did not improve from 0.70633\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8988 - accuracy: 0.7731 - val_loss: 1.2653 - val_accuracy: 0.7058 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8959 - accuracy: 0.7760\n",
      "Epoch 00110: val_accuracy did not improve from 0.70633\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8980 - accuracy: 0.7757 - val_loss: 1.2641 - val_accuracy: 0.7038 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9069 - accuracy: 0.7684\n",
      "Epoch 00111: val_accuracy did not improve from 0.70633\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9070 - accuracy: 0.7677 - val_loss: 1.2659 - val_accuracy: 0.7022 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9055 - accuracy: 0.7736\n",
      "Epoch 00112: val_accuracy did not improve from 0.70633\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9062 - accuracy: 0.7736 - val_loss: 1.2607 - val_accuracy: 0.7048 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8982 - accuracy: 0.7694\n",
      "Epoch 00113: val_accuracy did not improve from 0.70633\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.8985 - accuracy: 0.7695 - val_loss: 1.2559 - val_accuracy: 0.7048 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9041 - accuracy: 0.7741\n",
      "Epoch 00114: val_accuracy did not improve from 0.70633\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9074 - accuracy: 0.7733 - val_loss: 1.2640 - val_accuracy: 0.7048 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8975 - accuracy: 0.7713\n",
      "Epoch 00115: val_accuracy did not improve from 0.70633\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9000 - accuracy: 0.7710 - val_loss: 1.2661 - val_accuracy: 0.7048 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9040 - accuracy: 0.7718\n",
      "Epoch 00116: val_accuracy did not improve from 0.70633\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9058 - accuracy: 0.7709 - val_loss: 1.2631 - val_accuracy: 0.7053 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9058 - accuracy: 0.7727\n",
      "Epoch 00117: val_accuracy did not improve from 0.70633\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9056 - accuracy: 0.7727 - val_loss: 1.2657 - val_accuracy: 0.7043 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9014 - accuracy: 0.7718\n",
      "Epoch 00118: val_accuracy did not improve from 0.70633\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9001 - accuracy: 0.7717 - val_loss: 1.2663 - val_accuracy: 0.7033 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9043 - accuracy: 0.7761\n",
      "Epoch 00119: val_accuracy did not improve from 0.70633\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9041 - accuracy: 0.7763 - val_loss: 1.2640 - val_accuracy: 0.7061 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8959 - accuracy: 0.7722\n",
      "Epoch 00120: val_accuracy did not improve from 0.70633\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.8989 - accuracy: 0.7715 - val_loss: 1.2694 - val_accuracy: 0.7033 - lr: 1.0000e-06\n",
      "epoch_number 106\n",
      "train accuracy and validation accuracy 0.767666757106781 0.7063349485397339\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.2629 - accuracy: 0.7063\n",
      "test_accuracy 0.7063349485397339\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087, 0.7748140692710876, 0.7055655121803284, 0.7168504595756531, 0.6958194375038147, 0.7476276159286499, 0.7235188484191895, 0.7981534004211426, 0.8017440438270569, 0.7732751965522766, 0.7940497398376465, 0.7437804341316223, 0.7063349485397339]\n",
      "0.751920777818431\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S24_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S24_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 5.0286 - accuracy: 0.0833\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.16953, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 3s 345ms/step - loss: 5.0217 - accuracy: 0.0835 - val_loss: 3.8248 - val_accuracy: 0.1695 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.4341 - accuracy: 0.1512\n",
      "Epoch 00002: val_accuracy improved from 0.16953 to 0.24878, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 3.4336 - accuracy: 0.1519 - val_loss: 3.0008 - val_accuracy: 0.2488 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.9824 - accuracy: 0.2540\n",
      "Epoch 00003: val_accuracy improved from 0.24878 to 0.34701, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 2.9796 - accuracy: 0.2548 - val_loss: 2.6819 - val_accuracy: 0.3470 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.6799 - accuracy: 0.3170\n",
      "Epoch 00004: val_accuracy improved from 0.34701 to 0.40241, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 2.6785 - accuracy: 0.3166 - val_loss: 2.4328 - val_accuracy: 0.4024 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4244 - accuracy: 0.3736\n",
      "Epoch 00005: val_accuracy improved from 0.40241 to 0.45781, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 2.4244 - accuracy: 0.3736 - val_loss: 2.2155 - val_accuracy: 0.4578 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2299 - accuracy: 0.4152\n",
      "Epoch 00006: val_accuracy improved from 0.45781 to 0.50090, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 2.2272 - accuracy: 0.4160 - val_loss: 2.0302 - val_accuracy: 0.5009 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0777 - accuracy: 0.4493\n",
      "Epoch 00007: val_accuracy improved from 0.50090 to 0.53193, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 2.0787 - accuracy: 0.4487 - val_loss: 1.8567 - val_accuracy: 0.5319 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9701 - accuracy: 0.4739\n",
      "Epoch 00008: val_accuracy improved from 0.53193 to 0.55527, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.9701 - accuracy: 0.4740 - val_loss: 1.7489 - val_accuracy: 0.5553 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8737 - accuracy: 0.5023\n",
      "Epoch 00009: val_accuracy improved from 0.55527 to 0.56604, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.8737 - accuracy: 0.5025 - val_loss: 1.6989 - val_accuracy: 0.5660 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7902 - accuracy: 0.5267\n",
      "Epoch 00010: val_accuracy improved from 0.56604 to 0.58784, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.7904 - accuracy: 0.5261 - val_loss: 1.6210 - val_accuracy: 0.5878 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7012 - accuracy: 0.5470\n",
      "Epoch 00011: val_accuracy improved from 0.58784 to 0.59810, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.7001 - accuracy: 0.5474 - val_loss: 1.5494 - val_accuracy: 0.5981 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6374 - accuracy: 0.5647\n",
      "Epoch 00012: val_accuracy improved from 0.59810 to 0.61811, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 1.6366 - accuracy: 0.5650 - val_loss: 1.4901 - val_accuracy: 0.6181 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5596 - accuracy: 0.5809\n",
      "Epoch 00013: val_accuracy did not improve from 0.61811\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.5608 - accuracy: 0.5809 - val_loss: 1.4671 - val_accuracy: 0.6173 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5115 - accuracy: 0.5991\n",
      "Epoch 00014: val_accuracy improved from 0.61811 to 0.63632, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1.5120 - accuracy: 0.5990 - val_loss: 1.4143 - val_accuracy: 0.6363 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4750 - accuracy: 0.6069\n",
      "Epoch 00015: val_accuracy did not improve from 0.63632\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.4735 - accuracy: 0.6079 - val_loss: 1.4000 - val_accuracy: 0.6355 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4284 - accuracy: 0.6188\n",
      "Epoch 00016: val_accuracy improved from 0.63632 to 0.64196, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.4285 - accuracy: 0.6186 - val_loss: 1.3599 - val_accuracy: 0.6420 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3951 - accuracy: 0.6248\n",
      "Epoch 00017: val_accuracy improved from 0.64196 to 0.65453, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.3979 - accuracy: 0.6248 - val_loss: 1.3400 - val_accuracy: 0.6545 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3767 - accuracy: 0.6332\n",
      "Epoch 00018: val_accuracy did not improve from 0.65453\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.3763 - accuracy: 0.6336 - val_loss: 1.3403 - val_accuracy: 0.6486 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3563 - accuracy: 0.6349\n",
      "Epoch 00019: val_accuracy improved from 0.65453 to 0.66043, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 1.3555 - accuracy: 0.6355 - val_loss: 1.3125 - val_accuracy: 0.6604 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3225 - accuracy: 0.6428\n",
      "Epoch 00020: val_accuracy did not improve from 0.66043\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.3231 - accuracy: 0.6429 - val_loss: 1.3188 - val_accuracy: 0.6602 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2688 - accuracy: 0.6622\n",
      "Epoch 00021: val_accuracy improved from 0.66043 to 0.66684, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 1.2702 - accuracy: 0.6622 - val_loss: 1.2769 - val_accuracy: 0.6668 - lr: 0.0010\n",
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2510 - accuracy: 0.6650\n",
      "Epoch 00022: val_accuracy did not improve from 0.66684\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.2513 - accuracy: 0.6652 - val_loss: 1.2958 - val_accuracy: 0.6609 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2257 - accuracy: 0.6767\n",
      "Epoch 00023: val_accuracy improved from 0.66684 to 0.67607, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.2283 - accuracy: 0.6753 - val_loss: 1.2710 - val_accuracy: 0.6761 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2156 - accuracy: 0.6788\n",
      "Epoch 00024: val_accuracy improved from 0.67607 to 0.67710, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.2138 - accuracy: 0.6794 - val_loss: 1.2557 - val_accuracy: 0.6771 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2081 - accuracy: 0.6808\n",
      "Epoch 00025: val_accuracy did not improve from 0.67710\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.2090 - accuracy: 0.6811 - val_loss: 1.2575 - val_accuracy: 0.6743 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1781 - accuracy: 0.6896\n",
      "Epoch 00026: val_accuracy improved from 0.67710 to 0.68274, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.1777 - accuracy: 0.6899 - val_loss: 1.2325 - val_accuracy: 0.6827 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1520 - accuracy: 0.6931\n",
      "Epoch 00027: val_accuracy did not improve from 0.68274\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1510 - accuracy: 0.6936 - val_loss: 1.2521 - val_accuracy: 0.6822 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1412 - accuracy: 0.6941\n",
      "Epoch 00028: val_accuracy did not improve from 0.68274\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1420 - accuracy: 0.6937 - val_loss: 1.2359 - val_accuracy: 0.6784 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1235 - accuracy: 0.7056\n",
      "Epoch 00029: val_accuracy improved from 0.68274 to 0.69761, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.1235 - accuracy: 0.7056 - val_loss: 1.1970 - val_accuracy: 0.6976 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1043 - accuracy: 0.7053\n",
      "Epoch 00030: val_accuracy did not improve from 0.69761\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1042 - accuracy: 0.7054 - val_loss: 1.2197 - val_accuracy: 0.6886 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1053 - accuracy: 0.7124\n",
      "Epoch 00031: val_accuracy did not improve from 0.69761\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1036 - accuracy: 0.7128 - val_loss: 1.2019 - val_accuracy: 0.6948 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0801 - accuracy: 0.7197\n",
      "Epoch 00032: val_accuracy did not improve from 0.69761\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0811 - accuracy: 0.7193 - val_loss: 1.1829 - val_accuracy: 0.6953 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0640 - accuracy: 0.7213\n",
      "Epoch 00033: val_accuracy improved from 0.69761 to 0.70787, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.0658 - accuracy: 0.7206 - val_loss: 1.1756 - val_accuracy: 0.7079 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0403 - accuracy: 0.7249\n",
      "Epoch 00034: val_accuracy did not improve from 0.70787\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.0409 - accuracy: 0.7247 - val_loss: 1.1882 - val_accuracy: 0.6935 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0356 - accuracy: 0.7334\n",
      "Epoch 00035: val_accuracy did not improve from 0.70787\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.0373 - accuracy: 0.7334 - val_loss: 1.2194 - val_accuracy: 0.6935 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0586 - accuracy: 0.7201\n",
      "Epoch 00036: val_accuracy did not improve from 0.70787\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.0585 - accuracy: 0.7201 - val_loss: 1.2052 - val_accuracy: 0.6922 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0380 - accuracy: 0.7259\n",
      "Epoch 00037: val_accuracy did not improve from 0.70787\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0362 - accuracy: 0.7265 - val_loss: 1.1748 - val_accuracy: 0.7012 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0199 - accuracy: 0.7346\n",
      "Epoch 00038: val_accuracy improved from 0.70787 to 0.71300, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.0207 - accuracy: 0.7344 - val_loss: 1.1531 - val_accuracy: 0.7130 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9980 - accuracy: 0.7361\n",
      "Epoch 00039: val_accuracy did not improve from 0.71300\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9971 - accuracy: 0.7360 - val_loss: 1.1467 - val_accuracy: 0.7122 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9837 - accuracy: 0.7489\n",
      "Epoch 00040: val_accuracy improved from 0.71300 to 0.71711, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 0.9857 - accuracy: 0.7484 - val_loss: 1.1226 - val_accuracy: 0.7171 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9522 - accuracy: 0.7608\n",
      "Epoch 00041: val_accuracy improved from 0.71711 to 0.72070, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 0.9523 - accuracy: 0.7609 - val_loss: 1.1161 - val_accuracy: 0.7207 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9382 - accuracy: 0.7572\n",
      "Epoch 00042: val_accuracy improved from 0.72070 to 0.72403, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.9410 - accuracy: 0.7562 - val_loss: 1.1108 - val_accuracy: 0.7240 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9303 - accuracy: 0.7589\n",
      "Epoch 00043: val_accuracy did not improve from 0.72403\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.9307 - accuracy: 0.7590 - val_loss: 1.1039 - val_accuracy: 0.7210 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9046 - accuracy: 0.7644\n",
      "Epoch 00044: val_accuracy did not improve from 0.72403\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9060 - accuracy: 0.7636 - val_loss: 1.1082 - val_accuracy: 0.7207 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9179 - accuracy: 0.7654\n",
      "Epoch 00045: val_accuracy did not improve from 0.72403\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9174 - accuracy: 0.7655 - val_loss: 1.1037 - val_accuracy: 0.7230 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8942 - accuracy: 0.7651\n",
      "Epoch 00046: val_accuracy did not improve from 0.72403\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8949 - accuracy: 0.7650 - val_loss: 1.1016 - val_accuracy: 0.7240 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8919 - accuracy: 0.7701\n",
      "Epoch 00047: val_accuracy improved from 0.72403 to 0.72583, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 89ms/step - loss: 0.8942 - accuracy: 0.7694 - val_loss: 1.1027 - val_accuracy: 0.7258 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8842 - accuracy: 0.7782\n",
      "Epoch 00048: val_accuracy did not improve from 0.72583\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.8852 - accuracy: 0.7778 - val_loss: 1.1060 - val_accuracy: 0.7240 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8823 - accuracy: 0.7721\n",
      "Epoch 00049: val_accuracy did not improve from 0.72583\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8846 - accuracy: 0.7711 - val_loss: 1.1039 - val_accuracy: 0.7245 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8845 - accuracy: 0.7692\n",
      "Epoch 00050: val_accuracy improved from 0.72583 to 0.72660, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 0.8855 - accuracy: 0.7689 - val_loss: 1.0974 - val_accuracy: 0.7266 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8760 - accuracy: 0.7754\n",
      "Epoch 00051: val_accuracy did not improve from 0.72660\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.8787 - accuracy: 0.7746 - val_loss: 1.1027 - val_accuracy: 0.7256 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8715 - accuracy: 0.7688\n",
      "Epoch 00052: val_accuracy improved from 0.72660 to 0.72788, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.8713 - accuracy: 0.7687 - val_loss: 1.0987 - val_accuracy: 0.7279 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8718 - accuracy: 0.7772\n",
      "Epoch 00053: val_accuracy did not improve from 0.72788\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8710 - accuracy: 0.7771 - val_loss: 1.0973 - val_accuracy: 0.7276 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8689 - accuracy: 0.7776\n",
      "Epoch 00054: val_accuracy did not improve from 0.72788\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8702 - accuracy: 0.7772 - val_loss: 1.0979 - val_accuracy: 0.7261 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8618 - accuracy: 0.7797\n",
      "Epoch 00055: val_accuracy did not improve from 0.72788\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8640 - accuracy: 0.7792 - val_loss: 1.0976 - val_accuracy: 0.7269 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8615 - accuracy: 0.7809\n",
      "Epoch 00056: val_accuracy improved from 0.72788 to 0.72890, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 0.8610 - accuracy: 0.7804 - val_loss: 1.0965 - val_accuracy: 0.7289 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8613 - accuracy: 0.7730\n",
      "Epoch 00057: val_accuracy improved from 0.72890 to 0.72993, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.8628 - accuracy: 0.7728 - val_loss: 1.0972 - val_accuracy: 0.7299 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8507 - accuracy: 0.7776\n",
      "Epoch 00058: val_accuracy improved from 0.72993 to 0.73121, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 0.8503 - accuracy: 0.7776 - val_loss: 1.0988 - val_accuracy: 0.7312 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8445 - accuracy: 0.7837\n",
      "Epoch 00059: val_accuracy did not improve from 0.73121\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8469 - accuracy: 0.7829 - val_loss: 1.0925 - val_accuracy: 0.7294 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8659 - accuracy: 0.7732\n",
      "Epoch 00060: val_accuracy did not improve from 0.73121\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8636 - accuracy: 0.7740 - val_loss: 1.0909 - val_accuracy: 0.7302 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8618 - accuracy: 0.7762\n",
      "Epoch 00061: val_accuracy improved from 0.73121 to 0.73198, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 0.8635 - accuracy: 0.7755 - val_loss: 1.0876 - val_accuracy: 0.7320 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8462 - accuracy: 0.7848\n",
      "Epoch 00062: val_accuracy did not improve from 0.73198\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8472 - accuracy: 0.7842 - val_loss: 1.0860 - val_accuracy: 0.7299 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8314 - accuracy: 0.7821\n",
      "Epoch 00063: val_accuracy improved from 0.73198 to 0.73301, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.8323 - accuracy: 0.7820 - val_loss: 1.0833 - val_accuracy: 0.7330 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8481 - accuracy: 0.7817\n",
      "Epoch 00064: val_accuracy improved from 0.73301 to 0.73429, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 0.8497 - accuracy: 0.7814 - val_loss: 1.0822 - val_accuracy: 0.7343 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8336 - accuracy: 0.7879\n",
      "Epoch 00065: val_accuracy did not improve from 0.73429\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8329 - accuracy: 0.7877 - val_loss: 1.0924 - val_accuracy: 0.7299 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8378 - accuracy: 0.7832\n",
      "Epoch 00066: val_accuracy did not improve from 0.73429\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8420 - accuracy: 0.7821 - val_loss: 1.0958 - val_accuracy: 0.7266 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8161 - accuracy: 0.7899\n",
      "Epoch 00067: val_accuracy did not improve from 0.73429\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8181 - accuracy: 0.7889 - val_loss: 1.0973 - val_accuracy: 0.7281 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8345 - accuracy: 0.7853\n",
      "Epoch 00068: val_accuracy did not improve from 0.73429\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8379 - accuracy: 0.7846 - val_loss: 1.0936 - val_accuracy: 0.7289 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8287 - accuracy: 0.7892\n",
      "Epoch 00069: val_accuracy did not improve from 0.73429\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8297 - accuracy: 0.7887 - val_loss: 1.0936 - val_accuracy: 0.7325 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8244 - accuracy: 0.7859\n",
      "Epoch 00070: val_accuracy improved from 0.73429 to 0.73737, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.8242 - accuracy: 0.7857 - val_loss: 1.0907 - val_accuracy: 0.7374 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8334 - accuracy: 0.7852\n",
      "Epoch 00071: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8324 - accuracy: 0.7854 - val_loss: 1.0881 - val_accuracy: 0.7338 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8161 - accuracy: 0.7907\n",
      "Epoch 00072: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8205 - accuracy: 0.7893 - val_loss: 1.0857 - val_accuracy: 0.7338 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8253 - accuracy: 0.7852\n",
      "Epoch 00073: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8268 - accuracy: 0.7849 - val_loss: 1.0878 - val_accuracy: 0.7330 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8179 - accuracy: 0.7909\n",
      "Epoch 00074: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8185 - accuracy: 0.7910 - val_loss: 1.0826 - val_accuracy: 0.7333 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8172 - accuracy: 0.7913\n",
      "Epoch 00075: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8181 - accuracy: 0.7906 - val_loss: 1.0855 - val_accuracy: 0.7315 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8227 - accuracy: 0.7874\n",
      "Epoch 00076: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.8243 - accuracy: 0.7867 - val_loss: 1.0816 - val_accuracy: 0.7315 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8247 - accuracy: 0.7878\n",
      "Epoch 00077: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8274 - accuracy: 0.7866 - val_loss: 1.0871 - val_accuracy: 0.7345 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8199 - accuracy: 0.7860\n",
      "Epoch 00078: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.8216 - accuracy: 0.7854 - val_loss: 1.0779 - val_accuracy: 0.7361 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8128 - accuracy: 0.7932\n",
      "Epoch 00079: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8183 - accuracy: 0.7920 - val_loss: 1.0803 - val_accuracy: 0.7366 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8131 - accuracy: 0.7946\n",
      "Epoch 00080: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8148 - accuracy: 0.7935 - val_loss: 1.0792 - val_accuracy: 0.7374 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8123 - accuracy: 0.7891\n",
      "Epoch 00081: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.8123 - accuracy: 0.7894 - val_loss: 1.0752 - val_accuracy: 0.7345 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7958 - accuracy: 0.7932\n",
      "Epoch 00082: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.7975 - accuracy: 0.7925 - val_loss: 1.0779 - val_accuracy: 0.7343 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7989 - accuracy: 0.7972\n",
      "Epoch 00083: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.7996 - accuracy: 0.7969 - val_loss: 1.0809 - val_accuracy: 0.7328 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7989 - accuracy: 0.7977\n",
      "Epoch 00084: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7992 - accuracy: 0.7975 - val_loss: 1.0794 - val_accuracy: 0.7343 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8164 - accuracy: 0.7900\n",
      "Epoch 00085: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8181 - accuracy: 0.7892 - val_loss: 1.0788 - val_accuracy: 0.7338 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8008 - accuracy: 0.7947\n",
      "Epoch 00086: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8016 - accuracy: 0.7945 - val_loss: 1.0792 - val_accuracy: 0.7363 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8083 - accuracy: 0.7877\n",
      "Epoch 00087: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8097 - accuracy: 0.7871 - val_loss: 1.0815 - val_accuracy: 0.7335 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8026 - accuracy: 0.7960\n",
      "Epoch 00088: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8029 - accuracy: 0.7960 - val_loss: 1.0814 - val_accuracy: 0.7328 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8065 - accuracy: 0.7922\n",
      "Epoch 00089: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8071 - accuracy: 0.7917 - val_loss: 1.0787 - val_accuracy: 0.7363 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7970 - accuracy: 0.7983\n",
      "Epoch 00090: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.7988 - accuracy: 0.7973 - val_loss: 1.0813 - val_accuracy: 0.7358 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7962 - accuracy: 0.7952\n",
      "Epoch 00091: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7992 - accuracy: 0.7944 - val_loss: 1.0789 - val_accuracy: 0.7371 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7936 - accuracy: 0.7988\n",
      "Epoch 00092: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7948 - accuracy: 0.7980 - val_loss: 1.0769 - val_accuracy: 0.7345 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8004 - accuracy: 0.7916\n",
      "Epoch 00093: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 0.8021 - accuracy: 0.7910 - val_loss: 1.0756 - val_accuracy: 0.7366 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8096 - accuracy: 0.7916\n",
      "Epoch 00094: val_accuracy did not improve from 0.73737\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.8080 - accuracy: 0.7918 - val_loss: 1.0728 - val_accuracy: 0.7358 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8007 - accuracy: 0.7991\n",
      "Epoch 00095: val_accuracy improved from 0.73737 to 0.73814, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 0.8005 - accuracy: 0.7990 - val_loss: 1.0759 - val_accuracy: 0.7381 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8026 - accuracy: 0.7900\n",
      "Epoch 00096: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.8028 - accuracy: 0.7898 - val_loss: 1.0770 - val_accuracy: 0.7351 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7862 - accuracy: 0.7994\n",
      "Epoch 00097: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7894 - accuracy: 0.7984 - val_loss: 1.0742 - val_accuracy: 0.7328 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8089 - accuracy: 0.7924\n",
      "Epoch 00098: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8110 - accuracy: 0.7920 - val_loss: 1.0777 - val_accuracy: 0.7330 - lr: 1.0000e-05\n",
      "Epoch 99/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8034 - accuracy: 0.7941\n",
      "Epoch 00099: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8051 - accuracy: 0.7937 - val_loss: 1.0795 - val_accuracy: 0.7340 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8007 - accuracy: 0.7977\n",
      "Epoch 00100: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8023 - accuracy: 0.7973 - val_loss: 1.0765 - val_accuracy: 0.7356 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7921 - accuracy: 0.7992\n",
      "Epoch 00101: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7922 - accuracy: 0.7994 - val_loss: 1.0724 - val_accuracy: 0.7345 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7985 - accuracy: 0.7950\n",
      "Epoch 00102: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.7989 - accuracy: 0.7948 - val_loss: 1.0775 - val_accuracy: 0.7358 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8039 - accuracy: 0.7990\n",
      "Epoch 00103: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.8047 - accuracy: 0.7990 - val_loss: 1.0800 - val_accuracy: 0.7358 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7965 - accuracy: 0.7972\n",
      "Epoch 00104: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7984 - accuracy: 0.7965 - val_loss: 1.0777 - val_accuracy: 0.7369 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8073 - accuracy: 0.7896\n",
      "Epoch 00105: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8076 - accuracy: 0.7896 - val_loss: 1.0758 - val_accuracy: 0.7356 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7930 - accuracy: 0.7943\n",
      "Epoch 00106: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7925 - accuracy: 0.7948 - val_loss: 1.0763 - val_accuracy: 0.7361 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8069 - accuracy: 0.7934\n",
      "Epoch 00107: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8085 - accuracy: 0.7937 - val_loss: 1.0760 - val_accuracy: 0.7356 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8019 - accuracy: 0.7951\n",
      "Epoch 00108: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8034 - accuracy: 0.7949 - val_loss: 1.0744 - val_accuracy: 0.7374 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7893 - accuracy: 0.8027\n",
      "Epoch 00109: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.7931 - accuracy: 0.8015 - val_loss: 1.0733 - val_accuracy: 0.7363 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7930 - accuracy: 0.7974\n",
      "Epoch 00110: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7926 - accuracy: 0.7972 - val_loss: 1.0759 - val_accuracy: 0.7379 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7963 - accuracy: 0.7926\n",
      "Epoch 00111: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8038 - accuracy: 0.7914 - val_loss: 1.0760 - val_accuracy: 0.7333 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7871 - accuracy: 0.7971\n",
      "Epoch 00112: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7897 - accuracy: 0.7961 - val_loss: 1.0784 - val_accuracy: 0.7351 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8011 - accuracy: 0.7914\n",
      "Epoch 00113: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8012 - accuracy: 0.7914 - val_loss: 1.0751 - val_accuracy: 0.7361 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7903 - accuracy: 0.7980\n",
      "Epoch 00114: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.7900 - accuracy: 0.7978 - val_loss: 1.0722 - val_accuracy: 0.7366 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7871 - accuracy: 0.7960\n",
      "Epoch 00115: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.7876 - accuracy: 0.7958 - val_loss: 1.0756 - val_accuracy: 0.7363 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8000 - accuracy: 0.7901\n",
      "Epoch 00116: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8024 - accuracy: 0.7898 - val_loss: 1.0773 - val_accuracy: 0.7379 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7921 - accuracy: 0.7962\n",
      "Epoch 00117: val_accuracy did not improve from 0.73814\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7946 - accuracy: 0.7957 - val_loss: 1.0725 - val_accuracy: 0.7351 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7895 - accuracy: 0.7983\n",
      "Epoch 00118: val_accuracy improved from 0.73814 to 0.73839, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.7909 - accuracy: 0.7981 - val_loss: 1.0744 - val_accuracy: 0.7384 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8013 - accuracy: 0.7928\n",
      "Epoch 00119: val_accuracy did not improve from 0.73839\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.8054 - accuracy: 0.7917 - val_loss: 1.0775 - val_accuracy: 0.7343 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.7875 - accuracy: 0.8024\n",
      "Epoch 00120: val_accuracy did not improve from 0.73839\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.7896 - accuracy: 0.8018 - val_loss: 1.0774 - val_accuracy: 0.7379 - lr: 1.0000e-06\n",
      "epoch_number 118\n",
      "train accuracy and validation accuracy 0.7981097102165222 0.7383944392204285\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.0744 - accuracy: 0.7384\n",
      "test_accuracy 0.7383944392204285\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087, 0.7748140692710876, 0.7055655121803284, 0.7168504595756531, 0.6958194375038147, 0.7476276159286499, 0.7235188484191895, 0.7981534004211426, 0.8017440438270569, 0.7732751965522766, 0.7940497398376465, 0.7437804341316223, 0.7063349485397339, 0.7383944392204285]\n",
      "0.7513571803768476\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S25_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S25_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.9247 - accuracy: 0.1338\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.19210, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 3s 346ms/step - loss: 4.9105 - accuracy: 0.1339 - val_loss: 4.2413 - val_accuracy: 0.1921 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.2617 - accuracy: 0.2004\n",
      "Epoch 00002: val_accuracy improved from 0.19210 to 0.27674, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 3.2654 - accuracy: 0.2004 - val_loss: 2.8523 - val_accuracy: 0.2767 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.8549 - accuracy: 0.2717\n",
      "Epoch 00003: val_accuracy improved from 0.27674 to 0.33624, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 2.8557 - accuracy: 0.2711 - val_loss: 2.5606 - val_accuracy: 0.3362 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.5740 - accuracy: 0.3356\n",
      "Epoch 00004: val_accuracy improved from 0.33624 to 0.39343, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 2.5739 - accuracy: 0.3354 - val_loss: 2.3374 - val_accuracy: 0.3934 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4096 - accuracy: 0.3719\n",
      "Epoch 00005: val_accuracy improved from 0.39343 to 0.43986, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 2.4136 - accuracy: 0.3715 - val_loss: 2.2098 - val_accuracy: 0.4399 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2852 - accuracy: 0.4064\n",
      "Epoch 00006: val_accuracy improved from 0.43986 to 0.47474, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 2.2869 - accuracy: 0.4061 - val_loss: 2.0742 - val_accuracy: 0.4747 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1531 - accuracy: 0.4422\n",
      "Epoch 00007: val_accuracy improved from 0.47474 to 0.50218, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 2.1551 - accuracy: 0.4425 - val_loss: 1.9619 - val_accuracy: 0.5022 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0452 - accuracy: 0.4672\n",
      "Epoch 00008: val_accuracy improved from 0.50218 to 0.52578, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 2.0440 - accuracy: 0.4675 - val_loss: 1.9231 - val_accuracy: 0.5258 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9358 - accuracy: 0.4949\n",
      "Epoch 00009: val_accuracy improved from 0.52578 to 0.53757, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 1.9346 - accuracy: 0.4953 - val_loss: 1.8275 - val_accuracy: 0.5376 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8737 - accuracy: 0.5143\n",
      "Epoch 00010: val_accuracy improved from 0.53757 to 0.55989, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.8778 - accuracy: 0.5138 - val_loss: 1.7542 - val_accuracy: 0.5599 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8244 - accuracy: 0.5201\n",
      "Epoch 00011: val_accuracy did not improve from 0.55989\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 1.8247 - accuracy: 0.5202 - val_loss: 1.7499 - val_accuracy: 0.5589 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7504 - accuracy: 0.5436\n",
      "Epoch 00012: val_accuracy improved from 0.55989 to 0.57348, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.7488 - accuracy: 0.5438 - val_loss: 1.6963 - val_accuracy: 0.5735 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7116 - accuracy: 0.5618\n",
      "Epoch 00013: val_accuracy improved from 0.57348 to 0.57451, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.7138 - accuracy: 0.5609 - val_loss: 1.6926 - val_accuracy: 0.5745 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6765 - accuracy: 0.5672\n",
      "Epoch 00014: val_accuracy improved from 0.57451 to 0.58682, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 1.6767 - accuracy: 0.5669 - val_loss: 1.6474 - val_accuracy: 0.5868 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6286 - accuracy: 0.5817\n",
      "Epoch 00015: val_accuracy did not improve from 0.58682\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.6301 - accuracy: 0.5814 - val_loss: 1.6116 - val_accuracy: 0.5868 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5886 - accuracy: 0.5941\n",
      "Epoch 00016: val_accuracy improved from 0.58682 to 0.60195, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.5882 - accuracy: 0.5948 - val_loss: 1.5984 - val_accuracy: 0.6019 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5687 - accuracy: 0.5964\n",
      "Epoch 00017: val_accuracy improved from 0.60195 to 0.60605, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1.5685 - accuracy: 0.5962 - val_loss: 1.5738 - val_accuracy: 0.6061 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5327 - accuracy: 0.6014\n",
      "Epoch 00018: val_accuracy improved from 0.60605 to 0.60887, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.5365 - accuracy: 0.6005 - val_loss: 1.5452 - val_accuracy: 0.6089 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4829 - accuracy: 0.6109\n",
      "Epoch 00019: val_accuracy did not improve from 0.60887\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.4857 - accuracy: 0.6108 - val_loss: 1.5989 - val_accuracy: 0.5989 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4891 - accuracy: 0.6147\n",
      "Epoch 00020: val_accuracy did not improve from 0.60887\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.4878 - accuracy: 0.6150 - val_loss: 1.5528 - val_accuracy: 0.6068 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4396 - accuracy: 0.6257\n",
      "Epoch 00021: val_accuracy improved from 0.60887 to 0.62785, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.4436 - accuracy: 0.6242 - val_loss: 1.5119 - val_accuracy: 0.6279 - lr: 0.0010\n",
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4255 - accuracy: 0.6364\n",
      "Epoch 00022: val_accuracy did not improve from 0.62785\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 1.4262 - accuracy: 0.6363 - val_loss: 1.5700 - val_accuracy: 0.6155 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4045 - accuracy: 0.6399\n",
      "Epoch 00023: val_accuracy improved from 0.62785 to 0.62888, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1.4085 - accuracy: 0.6385 - val_loss: 1.5139 - val_accuracy: 0.6289 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3749 - accuracy: 0.6451\n",
      "Epoch 00024: val_accuracy improved from 0.62888 to 0.63273, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.3763 - accuracy: 0.6452 - val_loss: 1.4922 - val_accuracy: 0.6327 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3710 - accuracy: 0.6464\n",
      "Epoch 00025: val_accuracy did not improve from 0.63273\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.3726 - accuracy: 0.6458 - val_loss: 1.4820 - val_accuracy: 0.6307 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3410 - accuracy: 0.6571\n",
      "Epoch 00026: val_accuracy did not improve from 0.63273\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.3439 - accuracy: 0.6561 - val_loss: 1.5021 - val_accuracy: 0.6317 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3508 - accuracy: 0.6510\n",
      "Epoch 00027: val_accuracy improved from 0.63273 to 0.64170, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.3506 - accuracy: 0.6506 - val_loss: 1.4699 - val_accuracy: 0.6417 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2959 - accuracy: 0.6620\n",
      "Epoch 00028: val_accuracy did not improve from 0.64170\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.2976 - accuracy: 0.6616 - val_loss: 1.4530 - val_accuracy: 0.6343 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2831 - accuracy: 0.6729\n",
      "Epoch 00029: val_accuracy did not improve from 0.64170\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.2845 - accuracy: 0.6719 - val_loss: 1.4768 - val_accuracy: 0.6414 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2674 - accuracy: 0.6747\n",
      "Epoch 00030: val_accuracy did not improve from 0.64170\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.2677 - accuracy: 0.6747 - val_loss: 1.5051 - val_accuracy: 0.6302 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2564 - accuracy: 0.6702\n",
      "Epoch 00031: val_accuracy improved from 0.64170 to 0.65376, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.2551 - accuracy: 0.6707 - val_loss: 1.4323 - val_accuracy: 0.6538 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2254 - accuracy: 0.6834\n",
      "Epoch 00032: val_accuracy did not improve from 0.65376\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.2245 - accuracy: 0.6837 - val_loss: 1.4481 - val_accuracy: 0.6450 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2087 - accuracy: 0.6916\n",
      "Epoch 00033: val_accuracy improved from 0.65376 to 0.65632, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.2115 - accuracy: 0.6913 - val_loss: 1.4056 - val_accuracy: 0.6563 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1976 - accuracy: 0.6924\n",
      "Epoch 00034: val_accuracy did not improve from 0.65632\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1983 - accuracy: 0.6930 - val_loss: 1.4556 - val_accuracy: 0.6425 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1764 - accuracy: 0.7000\n",
      "Epoch 00035: val_accuracy did not improve from 0.65632\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1766 - accuracy: 0.6997 - val_loss: 1.4411 - val_accuracy: 0.6502 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1766 - accuracy: 0.7033\n",
      "Epoch 00036: val_accuracy improved from 0.65632 to 0.65940, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 1.1765 - accuracy: 0.7032 - val_loss: 1.4182 - val_accuracy: 0.6594 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1524 - accuracy: 0.7073\n",
      "Epoch 00037: val_accuracy did not improve from 0.65940\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1538 - accuracy: 0.7065 - val_loss: 1.4113 - val_accuracy: 0.6525 - lr: 0.0010\n",
      "Epoch 38/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.1453 - accuracy: 0.7091\n",
      "Epoch 00038: val_accuracy did not improve from 0.65940\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.1453 - accuracy: 0.7091 - val_loss: 1.4579 - val_accuracy: 0.6471 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1243 - accuracy: 0.7136\n",
      "Epoch 00039: val_accuracy did not improve from 0.65940\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1254 - accuracy: 0.7130 - val_loss: 1.4246 - val_accuracy: 0.6514 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0864 - accuracy: 0.7243\n",
      "Epoch 00040: val_accuracy improved from 0.65940 to 0.66273, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 1.0878 - accuracy: 0.7243 - val_loss: 1.4041 - val_accuracy: 0.6627 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0673 - accuracy: 0.7268\n",
      "Epoch 00041: val_accuracy did not improve from 0.66273\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.0713 - accuracy: 0.7256 - val_loss: 1.3900 - val_accuracy: 0.6602 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0573 - accuracy: 0.7317\n",
      "Epoch 00042: val_accuracy improved from 0.66273 to 0.66812, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.0602 - accuracy: 0.7313 - val_loss: 1.3772 - val_accuracy: 0.6681 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0468 - accuracy: 0.7330\n",
      "Epoch 00043: val_accuracy did not improve from 0.66812\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.0455 - accuracy: 0.7338 - val_loss: 1.3741 - val_accuracy: 0.6638 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0391 - accuracy: 0.7382\n",
      "Epoch 00044: val_accuracy did not improve from 0.66812\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0391 - accuracy: 0.7381 - val_loss: 1.3725 - val_accuracy: 0.6663 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0232 - accuracy: 0.7482\n",
      "Epoch 00045: val_accuracy did not improve from 0.66812\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.0250 - accuracy: 0.7479 - val_loss: 1.3659 - val_accuracy: 0.6679 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0267 - accuracy: 0.7432\n",
      "Epoch 00046: val_accuracy improved from 0.66812 to 0.66992, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1.0266 - accuracy: 0.7432 - val_loss: 1.3675 - val_accuracy: 0.6699 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0231 - accuracy: 0.7462\n",
      "Epoch 00047: val_accuracy did not improve from 0.66992\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.0256 - accuracy: 0.7457 - val_loss: 1.3700 - val_accuracy: 0.6653 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0315 - accuracy: 0.7407\n",
      "Epoch 00048: val_accuracy did not improve from 0.66992\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.0316 - accuracy: 0.7406 - val_loss: 1.3765 - val_accuracy: 0.6620 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0139 - accuracy: 0.7454\n",
      "Epoch 00049: val_accuracy did not improve from 0.66992\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.0138 - accuracy: 0.7456 - val_loss: 1.3683 - val_accuracy: 0.6689 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0322 - accuracy: 0.7411\n",
      "Epoch 00050: val_accuracy did not improve from 0.66992\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 1.0305 - accuracy: 0.7417 - val_loss: 1.3665 - val_accuracy: 0.6679 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0189 - accuracy: 0.7444\n",
      "Epoch 00051: val_accuracy improved from 0.66992 to 0.67274, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.0177 - accuracy: 0.7446 - val_loss: 1.3563 - val_accuracy: 0.6727 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9989 - accuracy: 0.7473\n",
      "Epoch 00052: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.9992 - accuracy: 0.7470 - val_loss: 1.3516 - val_accuracy: 0.6699 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9907 - accuracy: 0.7513\n",
      "Epoch 00053: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9896 - accuracy: 0.7517 - val_loss: 1.3552 - val_accuracy: 0.6699 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9853 - accuracy: 0.7551\n",
      "Epoch 00054: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9843 - accuracy: 0.7554 - val_loss: 1.3566 - val_accuracy: 0.6671 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9946 - accuracy: 0.7501\n",
      "Epoch 00055: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9949 - accuracy: 0.7495 - val_loss: 1.3542 - val_accuracy: 0.6681 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9906 - accuracy: 0.7486\n",
      "Epoch 00056: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9904 - accuracy: 0.7488 - val_loss: 1.3544 - val_accuracy: 0.6717 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9807 - accuracy: 0.7512\n",
      "Epoch 00057: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9858 - accuracy: 0.7496 - val_loss: 1.3576 - val_accuracy: 0.6727 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9796 - accuracy: 0.7569\n",
      "Epoch 00058: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.9807 - accuracy: 0.7563 - val_loss: 1.3657 - val_accuracy: 0.6676 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9815 - accuracy: 0.7573\n",
      "Epoch 00059: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9828 - accuracy: 0.7568 - val_loss: 1.3640 - val_accuracy: 0.6707 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9703 - accuracy: 0.7524\n",
      "Epoch 00060: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9725 - accuracy: 0.7514 - val_loss: 1.3647 - val_accuracy: 0.6671 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9757 - accuracy: 0.7553\n",
      "Epoch 00061: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9757 - accuracy: 0.7549 - val_loss: 1.3695 - val_accuracy: 0.6684 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9774 - accuracy: 0.7557\n",
      "Epoch 00062: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.9772 - accuracy: 0.7570 - val_loss: 1.3667 - val_accuracy: 0.6679 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9832 - accuracy: 0.7506\n",
      "Epoch 00063: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9825 - accuracy: 0.7512 - val_loss: 1.3674 - val_accuracy: 0.6668 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9767 - accuracy: 0.7547\n",
      "Epoch 00064: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9791 - accuracy: 0.7541 - val_loss: 1.3697 - val_accuracy: 0.6679 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9560 - accuracy: 0.7623\n",
      "Epoch 00065: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9554 - accuracy: 0.7623 - val_loss: 1.3661 - val_accuracy: 0.6679 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9604 - accuracy: 0.7639\n",
      "Epoch 00066: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 0.9613 - accuracy: 0.7638 - val_loss: 1.3599 - val_accuracy: 0.6681 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9530 - accuracy: 0.7612\n",
      "Epoch 00067: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9529 - accuracy: 0.7610 - val_loss: 1.3596 - val_accuracy: 0.6717 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9561 - accuracy: 0.7636\n",
      "Epoch 00068: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.9582 - accuracy: 0.7627 - val_loss: 1.3625 - val_accuracy: 0.6704 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9553 - accuracy: 0.7561\n",
      "Epoch 00069: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9553 - accuracy: 0.7557 - val_loss: 1.3604 - val_accuracy: 0.6720 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9519 - accuracy: 0.7611\n",
      "Epoch 00070: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9516 - accuracy: 0.7611 - val_loss: 1.3594 - val_accuracy: 0.6681 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9496 - accuracy: 0.7652\n",
      "Epoch 00071: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9523 - accuracy: 0.7643 - val_loss: 1.3541 - val_accuracy: 0.6694 - lr: 1.0000e-04\n",
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9484 - accuracy: 0.7642\n",
      "Epoch 00072: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9505 - accuracy: 0.7633 - val_loss: 1.3518 - val_accuracy: 0.6674 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9467 - accuracy: 0.7628\n",
      "Epoch 00073: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9486 - accuracy: 0.7625 - val_loss: 1.3599 - val_accuracy: 0.6684 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9494 - accuracy: 0.7687\n",
      "Epoch 00074: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9482 - accuracy: 0.7687 - val_loss: 1.3539 - val_accuracy: 0.6681 - lr: 1.0000e-04\n",
      "Epoch 75/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9247 - accuracy: 0.7696\n",
      "Epoch 00075: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9295 - accuracy: 0.7684 - val_loss: 1.3591 - val_accuracy: 0.6676 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9334 - accuracy: 0.7670\n",
      "Epoch 00076: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.9336 - accuracy: 0.7676 - val_loss: 1.3632 - val_accuracy: 0.6725 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9351 - accuracy: 0.7679\n",
      "Epoch 00077: val_accuracy did not improve from 0.67274\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9382 - accuracy: 0.7666 - val_loss: 1.3553 - val_accuracy: 0.6702 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9243 - accuracy: 0.7656\n",
      "Epoch 00078: val_accuracy improved from 0.67274 to 0.67428, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 0.9250 - accuracy: 0.7656 - val_loss: 1.3505 - val_accuracy: 0.6743 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9296 - accuracy: 0.7679\n",
      "Epoch 00079: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9321 - accuracy: 0.7677 - val_loss: 1.3595 - val_accuracy: 0.6717 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9207 - accuracy: 0.7644\n",
      "Epoch 00080: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9225 - accuracy: 0.7639 - val_loss: 1.3608 - val_accuracy: 0.6707 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9170 - accuracy: 0.7698\n",
      "Epoch 00081: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9203 - accuracy: 0.7693 - val_loss: 1.3535 - val_accuracy: 0.6704 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9251 - accuracy: 0.7691\n",
      "Epoch 00082: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9259 - accuracy: 0.7689 - val_loss: 1.3547 - val_accuracy: 0.6709 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9133 - accuracy: 0.7714\n",
      "Epoch 00083: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.9190 - accuracy: 0.7694 - val_loss: 1.3520 - val_accuracy: 0.6704 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9155 - accuracy: 0.7697\n",
      "Epoch 00084: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9174 - accuracy: 0.7692 - val_loss: 1.3579 - val_accuracy: 0.6720 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9268 - accuracy: 0.7727\n",
      "Epoch 00085: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9287 - accuracy: 0.7723 - val_loss: 1.3605 - val_accuracy: 0.6727 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9041 - accuracy: 0.7750\n",
      "Epoch 00086: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9049 - accuracy: 0.7747 - val_loss: 1.3555 - val_accuracy: 0.6699 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9095 - accuracy: 0.7747\n",
      "Epoch 00087: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9123 - accuracy: 0.7736 - val_loss: 1.3540 - val_accuracy: 0.6725 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9143 - accuracy: 0.7767\n",
      "Epoch 00088: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9163 - accuracy: 0.7762 - val_loss: 1.3512 - val_accuracy: 0.6715 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9063 - accuracy: 0.7762\n",
      "Epoch 00089: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9075 - accuracy: 0.7757 - val_loss: 1.3537 - val_accuracy: 0.6727 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9064 - accuracy: 0.7754\n",
      "Epoch 00090: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9085 - accuracy: 0.7748 - val_loss: 1.3541 - val_accuracy: 0.6720 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9117 - accuracy: 0.7733\n",
      "Epoch 00091: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9149 - accuracy: 0.7724 - val_loss: 1.3560 - val_accuracy: 0.6715 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9068 - accuracy: 0.7744\n",
      "Epoch 00092: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9096 - accuracy: 0.7739 - val_loss: 1.3539 - val_accuracy: 0.6732 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9317 - accuracy: 0.7662\n",
      "Epoch 00093: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9292 - accuracy: 0.7672 - val_loss: 1.3550 - val_accuracy: 0.6717 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9130 - accuracy: 0.7713\n",
      "Epoch 00094: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9143 - accuracy: 0.7710 - val_loss: 1.3563 - val_accuracy: 0.6709 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9148 - accuracy: 0.7730\n",
      "Epoch 00095: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9155 - accuracy: 0.7732 - val_loss: 1.3560 - val_accuracy: 0.6717 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9197 - accuracy: 0.7690\n",
      "Epoch 00096: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9222 - accuracy: 0.7679 - val_loss: 1.3540 - val_accuracy: 0.6722 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9121 - accuracy: 0.7761\n",
      "Epoch 00097: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9115 - accuracy: 0.7761 - val_loss: 1.3547 - val_accuracy: 0.6722 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9155 - accuracy: 0.7719\n",
      "Epoch 00098: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9156 - accuracy: 0.7723 - val_loss: 1.3582 - val_accuracy: 0.6727 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9159 - accuracy: 0.7703\n",
      "Epoch 00099: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9148 - accuracy: 0.7703 - val_loss: 1.3569 - val_accuracy: 0.6699 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9136 - accuracy: 0.7722\n",
      "Epoch 00100: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9171 - accuracy: 0.7713 - val_loss: 1.3568 - val_accuracy: 0.6722 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9219 - accuracy: 0.7719\n",
      "Epoch 00101: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9213 - accuracy: 0.7723 - val_loss: 1.3599 - val_accuracy: 0.6732 - lr: 1.0000e-05\n",
      "Epoch 102/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9110 - accuracy: 0.7736\n",
      "Epoch 00102: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.9125 - accuracy: 0.7728 - val_loss: 1.3579 - val_accuracy: 0.6715 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9064 - accuracy: 0.7749\n",
      "Epoch 00103: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9058 - accuracy: 0.7744 - val_loss: 1.3575 - val_accuracy: 0.6732 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9213 - accuracy: 0.7698\n",
      "Epoch 00104: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9230 - accuracy: 0.7694 - val_loss: 1.3506 - val_accuracy: 0.6717 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9086 - accuracy: 0.7749\n",
      "Epoch 00105: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9093 - accuracy: 0.7745 - val_loss: 1.3528 - val_accuracy: 0.6720 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9086 - accuracy: 0.7749\n",
      "Epoch 00106: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9107 - accuracy: 0.7742 - val_loss: 1.3530 - val_accuracy: 0.6722 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9195 - accuracy: 0.7697\n",
      "Epoch 00107: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9208 - accuracy: 0.7692 - val_loss: 1.3536 - val_accuracy: 0.6727 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8926 - accuracy: 0.7799\n",
      "Epoch 00108: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8939 - accuracy: 0.7792 - val_loss: 1.3539 - val_accuracy: 0.6712 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9004 - accuracy: 0.7788\n",
      "Epoch 00109: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9000 - accuracy: 0.7788 - val_loss: 1.3510 - val_accuracy: 0.6730 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9019 - accuracy: 0.7800\n",
      "Epoch 00110: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9016 - accuracy: 0.7799 - val_loss: 1.3559 - val_accuracy: 0.6727 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9069 - accuracy: 0.7764\n",
      "Epoch 00111: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9082 - accuracy: 0.7763 - val_loss: 1.3534 - val_accuracy: 0.6712 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8967 - accuracy: 0.7779\n",
      "Epoch 00112: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.8984 - accuracy: 0.7773 - val_loss: 1.3524 - val_accuracy: 0.6732 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9051 - accuracy: 0.7749\n",
      "Epoch 00113: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9047 - accuracy: 0.7747 - val_loss: 1.3520 - val_accuracy: 0.6732 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8969 - accuracy: 0.7807\n",
      "Epoch 00114: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.8967 - accuracy: 0.7806 - val_loss: 1.3480 - val_accuracy: 0.6732 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9124 - accuracy: 0.7741\n",
      "Epoch 00115: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 0.9130 - accuracy: 0.7737 - val_loss: 1.3501 - val_accuracy: 0.6725 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9001 - accuracy: 0.7790\n",
      "Epoch 00116: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9024 - accuracy: 0.7787 - val_loss: 1.3524 - val_accuracy: 0.6717 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9074 - accuracy: 0.7674\n",
      "Epoch 00117: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9077 - accuracy: 0.7671 - val_loss: 1.3536 - val_accuracy: 0.6727 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9144 - accuracy: 0.7754\n",
      "Epoch 00118: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9167 - accuracy: 0.7748 - val_loss: 1.3569 - val_accuracy: 0.6707 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.8988 - accuracy: 0.7788\n",
      "Epoch 00119: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9017 - accuracy: 0.7779 - val_loss: 1.3572 - val_accuracy: 0.6717 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9099 - accuracy: 0.7713\n",
      "Epoch 00120: val_accuracy did not improve from 0.67428\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9103 - accuracy: 0.7711 - val_loss: 1.3546 - val_accuracy: 0.6712 - lr: 1.0000e-06\n",
      "epoch_number 78\n",
      "train accuracy and validation accuracy 0.7655786275863647 0.6742754578590393\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.3505 - accuracy: 0.6743\n",
      "test_accuracy 0.6742754578590393\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087, 0.7748140692710876, 0.7055655121803284, 0.7168504595756531, 0.6958194375038147, 0.7476276159286499, 0.7235188484191895, 0.7981534004211426, 0.8017440438270569, 0.7732751965522766, 0.7940497398376465, 0.7437804341316223, 0.7063349485397339, 0.7383944392204285, 0.6742754578590393]\n",
      "0.7482739114761352\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S26_tr.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S26_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.0605 - accuracy: 0.1164\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.20903, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 3s 345ms/step - loss: 4.0548 - accuracy: 0.1172 - val_loss: 3.4091 - val_accuracy: 0.2090 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.1648 - accuracy: 0.2200\n",
      "Epoch 00002: val_accuracy improved from 0.20903 to 0.29803, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 103ms/step - loss: 3.1659 - accuracy: 0.2204 - val_loss: 2.7694 - val_accuracy: 0.2980 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.8213 - accuracy: 0.2814\n",
      "Epoch 00003: val_accuracy improved from 0.29803 to 0.36804, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 2.8204 - accuracy: 0.2818 - val_loss: 2.5152 - val_accuracy: 0.3680 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.6043 - accuracy: 0.3326\n",
      "Epoch 00004: val_accuracy improved from 0.36804 to 0.41498, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 2.6061 - accuracy: 0.3326 - val_loss: 2.3554 - val_accuracy: 0.4150 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4502 - accuracy: 0.3696\n",
      "Epoch 00005: val_accuracy improved from 0.41498 to 0.44140, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 2.4483 - accuracy: 0.3708 - val_loss: 2.2277 - val_accuracy: 0.4414 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.3036 - accuracy: 0.4047\n",
      "Epoch 00006: val_accuracy improved from 0.44140 to 0.47140, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 2.3023 - accuracy: 0.4050 - val_loss: 2.1328 - val_accuracy: 0.4714 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2255 - accuracy: 0.4254\n",
      "Epoch 00007: val_accuracy improved from 0.47140 to 0.47807, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 2.2246 - accuracy: 0.4255 - val_loss: 2.0731 - val_accuracy: 0.4781 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1210 - accuracy: 0.4538\n",
      "Epoch 00008: val_accuracy improved from 0.47807 to 0.49705, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 2.1211 - accuracy: 0.4531 - val_loss: 2.0003 - val_accuracy: 0.4971 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0628 - accuracy: 0.4659\n",
      "Epoch 00009: val_accuracy improved from 0.49705 to 0.51064, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 2.0640 - accuracy: 0.4658 - val_loss: 1.9529 - val_accuracy: 0.5106 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0103 - accuracy: 0.4778\n",
      "Epoch 00010: val_accuracy improved from 0.51064 to 0.51629, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 2.0171 - accuracy: 0.4764 - val_loss: 1.9163 - val_accuracy: 0.5163 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9495 - accuracy: 0.4971\n",
      "Epoch 00011: val_accuracy improved from 0.51629 to 0.51859, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.9507 - accuracy: 0.4970 - val_loss: 1.8824 - val_accuracy: 0.5186 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9014 - accuracy: 0.5107\n",
      "Epoch 00012: val_accuracy improved from 0.51859 to 0.51911, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.9022 - accuracy: 0.5102 - val_loss: 1.8716 - val_accuracy: 0.5191 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8604 - accuracy: 0.5162\n",
      "Epoch 00013: val_accuracy improved from 0.51911 to 0.52911, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.8605 - accuracy: 0.5157 - val_loss: 1.8384 - val_accuracy: 0.5291 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8134 - accuracy: 0.5279\n",
      "Epoch 00014: val_accuracy improved from 0.52911 to 0.53424, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.8180 - accuracy: 0.5270 - val_loss: 1.8063 - val_accuracy: 0.5342 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7896 - accuracy: 0.5331\n",
      "Epoch 00015: val_accuracy improved from 0.53424 to 0.54937, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.7882 - accuracy: 0.5338 - val_loss: 1.7761 - val_accuracy: 0.5494 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7330 - accuracy: 0.5491\n",
      "Epoch 00016: val_accuracy improved from 0.54937 to 0.55424, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.7332 - accuracy: 0.5493 - val_loss: 1.7621 - val_accuracy: 0.5542 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7343 - accuracy: 0.5452\n",
      "Epoch 00017: val_accuracy did not improve from 0.55424\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.7359 - accuracy: 0.5450 - val_loss: 1.7430 - val_accuracy: 0.5540 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6925 - accuracy: 0.5614\n",
      "Epoch 00018: val_accuracy improved from 0.55424 to 0.56732, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.6921 - accuracy: 0.5617 - val_loss: 1.7111 - val_accuracy: 0.5673 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6751 - accuracy: 0.5618\n",
      "Epoch 00019: val_accuracy did not improve from 0.56732\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.6808 - accuracy: 0.5604 - val_loss: 1.7041 - val_accuracy: 0.5591 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6243 - accuracy: 0.5757\n",
      "Epoch 00020: val_accuracy improved from 0.56732 to 0.56963, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.6248 - accuracy: 0.5753 - val_loss: 1.6913 - val_accuracy: 0.5696 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6192 - accuracy: 0.5773\n",
      "Epoch 00021: val_accuracy did not improve from 0.56963\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.6179 - accuracy: 0.5774 - val_loss: 1.6888 - val_accuracy: 0.5658 - lr: 0.0010\n",
      "Epoch 22/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6027 - accuracy: 0.5877\n",
      "Epoch 00022: val_accuracy improved from 0.56963 to 0.57220, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.6031 - accuracy: 0.5870 - val_loss: 1.6769 - val_accuracy: 0.5722 - lr: 0.0010\n",
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5693 - accuracy: 0.5898\n",
      "Epoch 00023: val_accuracy improved from 0.57220 to 0.57810, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.5704 - accuracy: 0.5893 - val_loss: 1.6734 - val_accuracy: 0.5781 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5335 - accuracy: 0.6023\n",
      "Epoch 00024: val_accuracy improved from 0.57810 to 0.57938, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.5316 - accuracy: 0.6030 - val_loss: 1.6387 - val_accuracy: 0.5794 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5107 - accuracy: 0.6096\n",
      "Epoch 00025: val_accuracy did not improve from 0.57938\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 1.5101 - accuracy: 0.6098 - val_loss: 1.6487 - val_accuracy: 0.5776 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4735 - accuracy: 0.6144\n",
      "Epoch 00026: val_accuracy improved from 0.57938 to 0.58502, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1.4750 - accuracy: 0.6142 - val_loss: 1.6430 - val_accuracy: 0.5850 - lr: 0.0010\n",
      "Epoch 27/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.4733 - accuracy: 0.6136\n",
      "Epoch 00027: val_accuracy improved from 0.58502 to 0.58913, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.4733 - accuracy: 0.6136 - val_loss: 1.6022 - val_accuracy: 0.5891 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4298 - accuracy: 0.6268\n",
      "Epoch 00028: val_accuracy did not improve from 0.58913\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.4330 - accuracy: 0.6262 - val_loss: 1.6139 - val_accuracy: 0.5868 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4315 - accuracy: 0.6260\n",
      "Epoch 00029: val_accuracy did not improve from 0.58913\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.4334 - accuracy: 0.6252 - val_loss: 1.6684 - val_accuracy: 0.5845 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4285 - accuracy: 0.6336\n",
      "Epoch 00030: val_accuracy improved from 0.58913 to 0.59990, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 1.4308 - accuracy: 0.6329 - val_loss: 1.6285 - val_accuracy: 0.5999 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4326 - accuracy: 0.6212\n",
      "Epoch 00031: val_accuracy did not improve from 0.59990\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.4337 - accuracy: 0.6204 - val_loss: 1.6163 - val_accuracy: 0.5943 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4036 - accuracy: 0.6383\n",
      "Epoch 00032: val_accuracy did not improve from 0.59990\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.4042 - accuracy: 0.6378 - val_loss: 1.5903 - val_accuracy: 0.5948 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3859 - accuracy: 0.6339\n",
      "Epoch 00033: val_accuracy did not improve from 0.59990\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.3884 - accuracy: 0.6330 - val_loss: 1.6132 - val_accuracy: 0.5940 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3844 - accuracy: 0.6377\n",
      "Epoch 00034: val_accuracy did not improve from 0.59990\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.3838 - accuracy: 0.6380 - val_loss: 1.6231 - val_accuracy: 0.5899 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3466 - accuracy: 0.6501\n",
      "Epoch 00035: val_accuracy improved from 0.59990 to 0.60554, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1.3491 - accuracy: 0.6494 - val_loss: 1.5988 - val_accuracy: 0.6055 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3502 - accuracy: 0.6518\n",
      "Epoch 00036: val_accuracy improved from 0.60554 to 0.60580, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1.3502 - accuracy: 0.6517 - val_loss: 1.6188 - val_accuracy: 0.6058 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3160 - accuracy: 0.6621\n",
      "Epoch 00037: val_accuracy improved from 0.60580 to 0.61041, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.3211 - accuracy: 0.6610 - val_loss: 1.5704 - val_accuracy: 0.6104 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3300 - accuracy: 0.6544\n",
      "Epoch 00038: val_accuracy did not improve from 0.61041\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.3314 - accuracy: 0.6547 - val_loss: 1.5619 - val_accuracy: 0.6066 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3228 - accuracy: 0.6587\n",
      "Epoch 00039: val_accuracy did not improve from 0.61041\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.3265 - accuracy: 0.6574 - val_loss: 1.5687 - val_accuracy: 0.6050 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2772 - accuracy: 0.6687\n",
      "Epoch 00040: val_accuracy improved from 0.61041 to 0.61221, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.2769 - accuracy: 0.6682 - val_loss: 1.5452 - val_accuracy: 0.6122 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2634 - accuracy: 0.6791\n",
      "Epoch 00041: val_accuracy improved from 0.61221 to 0.61580, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.2634 - accuracy: 0.6789 - val_loss: 1.5364 - val_accuracy: 0.6158 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2508 - accuracy: 0.6779\n",
      "Epoch 00042: val_accuracy did not improve from 0.61580\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.2533 - accuracy: 0.6771 - val_loss: 1.5345 - val_accuracy: 0.6132 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2417 - accuracy: 0.6754\n",
      "Epoch 00043: val_accuracy improved from 0.61580 to 0.62042, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.2424 - accuracy: 0.6758 - val_loss: 1.5301 - val_accuracy: 0.6204 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2278 - accuracy: 0.6822\n",
      "Epoch 00044: val_accuracy did not improve from 0.62042\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.2286 - accuracy: 0.6818 - val_loss: 1.5301 - val_accuracy: 0.6150 - lr: 1.0000e-04\n",
      "Epoch 45/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2162 - accuracy: 0.6882\n",
      "Epoch 00045: val_accuracy did not improve from 0.62042\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.2169 - accuracy: 0.6878 - val_loss: 1.5292 - val_accuracy: 0.6161 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2242 - accuracy: 0.6833\n",
      "Epoch 00046: val_accuracy did not improve from 0.62042\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.2263 - accuracy: 0.6827 - val_loss: 1.5276 - val_accuracy: 0.6186 - lr: 1.0000e-04\n",
      "Epoch 47/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2013 - accuracy: 0.6899\n",
      "Epoch 00047: val_accuracy improved from 0.62042 to 0.62093, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.2031 - accuracy: 0.6890 - val_loss: 1.5267 - val_accuracy: 0.6209 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1962 - accuracy: 0.6903\n",
      "Epoch 00048: val_accuracy did not improve from 0.62093\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1980 - accuracy: 0.6901 - val_loss: 1.5306 - val_accuracy: 0.6209 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2061 - accuracy: 0.6877\n",
      "Epoch 00049: val_accuracy improved from 0.62093 to 0.62272, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.2061 - accuracy: 0.6878 - val_loss: 1.5239 - val_accuracy: 0.6227 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1742 - accuracy: 0.6978\n",
      "Epoch 00050: val_accuracy improved from 0.62272 to 0.62401, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.1774 - accuracy: 0.6970 - val_loss: 1.5221 - val_accuracy: 0.6240 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1939 - accuracy: 0.6948\n",
      "Epoch 00051: val_accuracy did not improve from 0.62401\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.1976 - accuracy: 0.6939 - val_loss: 1.5184 - val_accuracy: 0.6214 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1806 - accuracy: 0.6957\n",
      "Epoch 00052: val_accuracy did not improve from 0.62401\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1849 - accuracy: 0.6943 - val_loss: 1.5191 - val_accuracy: 0.6232 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1758 - accuracy: 0.6984\n",
      "Epoch 00053: val_accuracy did not improve from 0.62401\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 1.1749 - accuracy: 0.6986 - val_loss: 1.5194 - val_accuracy: 0.6225 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1716 - accuracy: 0.6994\n",
      "Epoch 00054: val_accuracy did not improve from 0.62401\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1766 - accuracy: 0.6981 - val_loss: 1.5211 - val_accuracy: 0.6181 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1701 - accuracy: 0.7023\n",
      "Epoch 00055: val_accuracy did not improve from 0.62401\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.1710 - accuracy: 0.7019 - val_loss: 1.5153 - val_accuracy: 0.6214 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1607 - accuracy: 0.7024\n",
      "Epoch 00056: val_accuracy did not improve from 0.62401\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.1648 - accuracy: 0.7014 - val_loss: 1.5145 - val_accuracy: 0.6230 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1650 - accuracy: 0.6997\n",
      "Epoch 00057: val_accuracy did not improve from 0.62401\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1648 - accuracy: 0.7001 - val_loss: 1.5191 - val_accuracy: 0.6214 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1646 - accuracy: 0.6979\n",
      "Epoch 00058: val_accuracy improved from 0.62401 to 0.62914, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.1672 - accuracy: 0.6972 - val_loss: 1.5101 - val_accuracy: 0.6291 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1535 - accuracy: 0.7037\n",
      "Epoch 00059: val_accuracy did not improve from 0.62914\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1560 - accuracy: 0.7027 - val_loss: 1.5113 - val_accuracy: 0.6261 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1580 - accuracy: 0.6996\n",
      "Epoch 00060: val_accuracy did not improve from 0.62914\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1594 - accuracy: 0.6993 - val_loss: 1.5169 - val_accuracy: 0.6276 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1525 - accuracy: 0.7009\n",
      "Epoch 00061: val_accuracy did not improve from 0.62914\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.1529 - accuracy: 0.7011 - val_loss: 1.5091 - val_accuracy: 0.6276 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1564 - accuracy: 0.7004\n",
      "Epoch 00062: val_accuracy did not improve from 0.62914\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 1.1595 - accuracy: 0.6996 - val_loss: 1.5020 - val_accuracy: 0.6284 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1549 - accuracy: 0.7064\n",
      "Epoch 00063: val_accuracy did not improve from 0.62914\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 1.1553 - accuracy: 0.7066 - val_loss: 1.5048 - val_accuracy: 0.6271 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1520 - accuracy: 0.7017\n",
      "Epoch 00064: val_accuracy did not improve from 0.62914\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1564 - accuracy: 0.7006 - val_loss: 1.5052 - val_accuracy: 0.6284 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1497 - accuracy: 0.7079\n",
      "Epoch 00065: val_accuracy did not improve from 0.62914\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1524 - accuracy: 0.7072 - val_loss: 1.5166 - val_accuracy: 0.6258 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1557 - accuracy: 0.7047\n",
      "Epoch 00066: val_accuracy did not improve from 0.62914\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1602 - accuracy: 0.7030 - val_loss: 1.5183 - val_accuracy: 0.6235 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1489 - accuracy: 0.7042\n",
      "Epoch 00067: val_accuracy did not improve from 0.62914\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.1539 - accuracy: 0.7037 - val_loss: 1.5246 - val_accuracy: 0.6173 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1509 - accuracy: 0.7054\n",
      "Epoch 00068: val_accuracy did not improve from 0.62914\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1511 - accuracy: 0.7052 - val_loss: 1.5140 - val_accuracy: 0.6212 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1291 - accuracy: 0.7084\n",
      "Epoch 00069: val_accuracy did not improve from 0.62914\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1318 - accuracy: 0.7077 - val_loss: 1.5122 - val_accuracy: 0.6250 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1416 - accuracy: 0.7049\n",
      "Epoch 00070: val_accuracy did not improve from 0.62914\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1419 - accuracy: 0.7051 - val_loss: 1.5095 - val_accuracy: 0.6281 - lr: 1.0000e-04\n",
      "Epoch 71/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1297 - accuracy: 0.7051\n",
      "Epoch 00071: val_accuracy improved from 0.62914 to 0.62991, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1.1301 - accuracy: 0.7055 - val_loss: 1.5052 - val_accuracy: 0.6299 - lr: 1.0000e-04\n",
      "Epoch 72/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1419 - accuracy: 0.7046\n",
      "Epoch 00072: val_accuracy did not improve from 0.62991\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1421 - accuracy: 0.7049 - val_loss: 1.5127 - val_accuracy: 0.6273 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1357 - accuracy: 0.7108\n",
      "Epoch 00073: val_accuracy did not improve from 0.62991\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1419 - accuracy: 0.7094 - val_loss: 1.5129 - val_accuracy: 0.6276 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1307 - accuracy: 0.7124\n",
      "Epoch 00074: val_accuracy did not improve from 0.62991\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1315 - accuracy: 0.7123 - val_loss: 1.5059 - val_accuracy: 0.6299 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1305 - accuracy: 0.7038\n",
      "Epoch 00075: val_accuracy improved from 0.62991 to 0.63016, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1.1312 - accuracy: 0.7038 - val_loss: 1.5080 - val_accuracy: 0.6302 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1078 - accuracy: 0.7170\n",
      "Epoch 00076: val_accuracy improved from 0.63016 to 0.63221, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.1118 - accuracy: 0.7161 - val_loss: 1.5018 - val_accuracy: 0.6322 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1304 - accuracy: 0.7116\n",
      "Epoch 00077: val_accuracy improved from 0.63221 to 0.63247, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.1325 - accuracy: 0.7113 - val_loss: 1.5055 - val_accuracy: 0.6325 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1128 - accuracy: 0.7102\n",
      "Epoch 00078: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1155 - accuracy: 0.7099 - val_loss: 1.5182 - val_accuracy: 0.6240 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1081 - accuracy: 0.7122\n",
      "Epoch 00079: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1096 - accuracy: 0.7121 - val_loss: 1.5135 - val_accuracy: 0.6281 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1107 - accuracy: 0.7169\n",
      "Epoch 00080: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1114 - accuracy: 0.7170 - val_loss: 1.5093 - val_accuracy: 0.6230 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1073 - accuracy: 0.7139\n",
      "Epoch 00081: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1088 - accuracy: 0.7137 - val_loss: 1.5079 - val_accuracy: 0.6291 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1011 - accuracy: 0.7224\n",
      "Epoch 00082: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1006 - accuracy: 0.7229 - val_loss: 1.5137 - val_accuracy: 0.6258 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1143 - accuracy: 0.7084\n",
      "Epoch 00083: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1146 - accuracy: 0.7085 - val_loss: 1.5125 - val_accuracy: 0.6281 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0985 - accuracy: 0.7222\n",
      "Epoch 00084: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.0984 - accuracy: 0.7218 - val_loss: 1.5074 - val_accuracy: 0.6258 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1055 - accuracy: 0.7136\n",
      "Epoch 00085: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1054 - accuracy: 0.7138 - val_loss: 1.5102 - val_accuracy: 0.6255 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1222 - accuracy: 0.7074\n",
      "Epoch 00086: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1193 - accuracy: 0.7084 - val_loss: 1.5094 - val_accuracy: 0.6312 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1077 - accuracy: 0.7144\n",
      "Epoch 00087: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1085 - accuracy: 0.7140 - val_loss: 1.5081 - val_accuracy: 0.6245 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1085 - accuracy: 0.7177\n",
      "Epoch 00088: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1122 - accuracy: 0.7169 - val_loss: 1.5119 - val_accuracy: 0.6263 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1015 - accuracy: 0.7163\n",
      "Epoch 00089: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1024 - accuracy: 0.7160 - val_loss: 1.5132 - val_accuracy: 0.6271 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1020 - accuracy: 0.7177\n",
      "Epoch 00090: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1014 - accuracy: 0.7174 - val_loss: 1.5101 - val_accuracy: 0.6299 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1044 - accuracy: 0.7177\n",
      "Epoch 00091: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1049 - accuracy: 0.7172 - val_loss: 1.5091 - val_accuracy: 0.6304 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1048 - accuracy: 0.7194\n",
      "Epoch 00092: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1051 - accuracy: 0.7193 - val_loss: 1.5132 - val_accuracy: 0.6284 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1087 - accuracy: 0.7150\n",
      "Epoch 00093: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1104 - accuracy: 0.7144 - val_loss: 1.5192 - val_accuracy: 0.6304 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0972 - accuracy: 0.7148\n",
      "Epoch 00094: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.0976 - accuracy: 0.7144 - val_loss: 1.5065 - val_accuracy: 0.6309 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0888 - accuracy: 0.7191\n",
      "Epoch 00095: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.0893 - accuracy: 0.7188 - val_loss: 1.5057 - val_accuracy: 0.6273 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1056 - accuracy: 0.7158\n",
      "Epoch 00096: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1072 - accuracy: 0.7156 - val_loss: 1.5036 - val_accuracy: 0.6304 - lr: 1.0000e-05\n",
      "Epoch 97/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0829 - accuracy: 0.7208\n",
      "Epoch 00097: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 1.0913 - accuracy: 0.7187 - val_loss: 1.5046 - val_accuracy: 0.6276 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0926 - accuracy: 0.7257\n",
      "Epoch 00098: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.0920 - accuracy: 0.7257 - val_loss: 1.5059 - val_accuracy: 0.6281 - lr: 1.0000e-05\n",
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1064 - accuracy: 0.7164\n",
      "Epoch 00099: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1042 - accuracy: 0.7170 - val_loss: 1.5052 - val_accuracy: 0.6296 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0917 - accuracy: 0.7202\n",
      "Epoch 00100: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 1.0942 - accuracy: 0.7194 - val_loss: 1.5044 - val_accuracy: 0.6304 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1002 - accuracy: 0.7180\n",
      "Epoch 00101: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1010 - accuracy: 0.7179 - val_loss: 1.5032 - val_accuracy: 0.6299 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0872 - accuracy: 0.7228\n",
      "Epoch 00102: val_accuracy did not improve from 0.63247\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.0863 - accuracy: 0.7229 - val_loss: 1.5031 - val_accuracy: 0.6314 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0931 - accuracy: 0.7221\n",
      "Epoch 00103: val_accuracy improved from 0.63247 to 0.63273, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1.0919 - accuracy: 0.7223 - val_loss: 1.5083 - val_accuracy: 0.6327 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0995 - accuracy: 0.7178\n",
      "Epoch 00104: val_accuracy did not improve from 0.63273\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1001 - accuracy: 0.7177 - val_loss: 1.5069 - val_accuracy: 0.6312 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1138 - accuracy: 0.7154\n",
      "Epoch 00105: val_accuracy did not improve from 0.63273\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1167 - accuracy: 0.7149 - val_loss: 1.5027 - val_accuracy: 0.6327 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1039 - accuracy: 0.7162\n",
      "Epoch 00106: val_accuracy did not improve from 0.63273\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1048 - accuracy: 0.7159 - val_loss: 1.5058 - val_accuracy: 0.6320 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0945 - accuracy: 0.7188\n",
      "Epoch 00107: val_accuracy did not improve from 0.63273\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.0941 - accuracy: 0.7185 - val_loss: 1.5029 - val_accuracy: 0.6279 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1094 - accuracy: 0.7126\n",
      "Epoch 00108: val_accuracy did not improve from 0.63273\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.1082 - accuracy: 0.7125 - val_loss: 1.4991 - val_accuracy: 0.6289 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0955 - accuracy: 0.7174\n",
      "Epoch 00109: val_accuracy did not improve from 0.63273\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.0990 - accuracy: 0.7167 - val_loss: 1.5073 - val_accuracy: 0.6255 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0986 - accuracy: 0.7171\n",
      "Epoch 00110: val_accuracy improved from 0.63273 to 0.63324, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1.1022 - accuracy: 0.7162 - val_loss: 1.5014 - val_accuracy: 0.6332 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0920 - accuracy: 0.7217\n",
      "Epoch 00111: val_accuracy did not improve from 0.63324\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.0939 - accuracy: 0.7219 - val_loss: 1.5047 - val_accuracy: 0.6302 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1058 - accuracy: 0.7174\n",
      "Epoch 00112: val_accuracy did not improve from 0.63324\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.1077 - accuracy: 0.7176 - val_loss: 1.5006 - val_accuracy: 0.6299 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0802 - accuracy: 0.7232\n",
      "Epoch 00113: val_accuracy did not improve from 0.63324\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.0806 - accuracy: 0.7229 - val_loss: 1.5006 - val_accuracy: 0.6286 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1030 - accuracy: 0.7181\n",
      "Epoch 00114: val_accuracy did not improve from 0.63324\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.1024 - accuracy: 0.7183 - val_loss: 1.4961 - val_accuracy: 0.6291 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0900 - accuracy: 0.7200\n",
      "Epoch 00115: val_accuracy did not improve from 0.63324\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.0912 - accuracy: 0.7193 - val_loss: 1.5020 - val_accuracy: 0.6307 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0990 - accuracy: 0.7173\n",
      "Epoch 00116: val_accuracy did not improve from 0.63324\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1018 - accuracy: 0.7171 - val_loss: 1.4997 - val_accuracy: 0.6317 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0961 - accuracy: 0.7152\n",
      "Epoch 00117: val_accuracy did not improve from 0.63324\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.0965 - accuracy: 0.7149 - val_loss: 1.4996 - val_accuracy: 0.6314 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1049 - accuracy: 0.7152\n",
      "Epoch 00118: val_accuracy did not improve from 0.63324\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1065 - accuracy: 0.7150 - val_loss: 1.5020 - val_accuracy: 0.6289 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0936 - accuracy: 0.7233\n",
      "Epoch 00119: val_accuracy did not improve from 0.63324\n",
      "10/10 [==============================] - 1s 62ms/step - loss: 1.0923 - accuracy: 0.7238 - val_loss: 1.5049 - val_accuracy: 0.6299 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0892 - accuracy: 0.7206\n",
      "Epoch 00120: val_accuracy did not improve from 0.63324\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.0899 - accuracy: 0.7202 - val_loss: 1.4993 - val_accuracy: 0.6281 - lr: 1.0000e-06\n",
      "epoch_number 110\n",
      "train accuracy and validation accuracy 0.7162325382232666 0.6332392692565918\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.5014 - accuracy: 0.6332\n",
      "test_accuracy 0.6332392692565918\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087, 0.7748140692710876, 0.7055655121803284, 0.7168504595756531, 0.6958194375038147, 0.7476276159286499, 0.7235188484191895, 0.7981534004211426, 0.8017440438270569, 0.7732751965522766, 0.7940497398376465, 0.7437804341316223, 0.7063349485397339, 0.7383944392204285, 0.6742754578590393, 0.6332392692565918]\n",
      "0.743849502159999\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S27_tr.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 182000\n",
      "\n",
      "/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_naveen/sub_wise_process_TT/S27_tt.csv\n",
      "Number of columns in the dataframe: 11\n",
      "Number of rows in the dataframe: 78000\n",
      "\n",
      "x_train shape:  (9099, 20, 10)\n",
      "9099 training samples\n",
      "y_train shape:  (9099,)\n",
      "num_time_periods 20\n",
      "num_sensors 10\n",
      "class_list [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0, 22.0, 23.0, 24.0, 25.0, 26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 33.0, 34.0, 35.0, 36.0, 37.0, 38.0, 39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46.0, 47.0, 48.0, 49.0, 50.0, 51.0, 52.0]\n",
      "x_train shape: (20, 10)\n",
      "input_shape: (20, 10)\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "New y_train shape:  (9099, 52)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "3899 testing samples\n",
      "y_test shape:  (3899,)\n",
      "x_train shape:  (9099, 20, 10)\n",
      "x_test shape:  (3899, 20, 10)\n",
      "n_outputs 52\n",
      "Model: \"wei_mscnn_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main (InputLayer)               [(None, 20, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 20)]         0           main[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(2,)]               0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_2 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_3 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_4 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_5 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_6 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_7 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_8 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_16[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_9 (TensorFlow [(2,)]               0           tf_op_layer_strided_slice_18[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [()]                 0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [()]                 0           tf_op_layer_Shape_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [()]                 0           tf_op_layer_Shape_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [()]                 0           tf_op_layer_Shape_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [()]                 0           tf_op_layer_Shape_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [()]                 0           tf_op_layer_Shape_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [()]                 0           tf_op_layer_Shape_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [()]                 0           tf_op_layer_Shape_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [()]                 0           tf_op_layer_Shape_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape/shape (Tens [(3,)]               0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1/shape (Te [(3,)]               0           tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2/shape (Te [(3,)]               0           tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3/shape (Te [(3,)]               0           tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4/shape (Te [(3,)]               0           tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5/shape (Te [(3,)]               0           tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6/shape (Te [(3,)]               0           tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7/shape (Te [(3,)]               0           tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8/shape (Te [(3,)]               0           tf_op_layer_strided_slice_17[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9/shape (Te [(3,)]               0           tf_op_layer_strided_slice_19[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 20, 1)]      0           tf_op_layer_strided_slice[0][0]  \n",
      "                                                                 tf_op_layer_Reshape/shape[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_2[0][0]\n",
      "                                                                 tf_op_layer_Reshape_1/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_2 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_4[0][0]\n",
      "                                                                 tf_op_layer_Reshape_2/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_3 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_Reshape_3/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_4 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_8[0][0]\n",
      "                                                                 tf_op_layer_Reshape_4/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_5 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_10[0][0\n",
      "                                                                 tf_op_layer_Reshape_5/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_6 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_12[0][0\n",
      "                                                                 tf_op_layer_Reshape_6/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_7 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_14[0][0\n",
      "                                                                 tf_op_layer_Reshape_7/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_8 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_Reshape_8/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_9 (TensorFl [(None, 20, 1)]      0           tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_Reshape_9/shape[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 20, 64)       256         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 20, 64)       256         tf_op_layer_Reshape_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 20, 64)       256         tf_op_layer_Reshape_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20, 64)       256         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20, 64)       256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 64)       256         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 64)       256         conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 64)       256         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 64)       256         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 64)       256         conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 64)       256         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 20, 64)       256         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 20, 64)       256         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 20, 64)       0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 20, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 64)       0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 64)       0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 64)       0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 64)       0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 64)       0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 20, 64)       0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 20, 64)       0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       12352       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 20, 64)       12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 20, 64)       12352       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 20, 64)       12352       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 20, 64)       12352       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 20, 64)       12352       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 20, 64)       12352       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 20, 64)       12352       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 20, 64)       12352       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 20, 64)       12352       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 64)       256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 20, 64)       256         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 64)       256         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 64)       256         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 64)       256         conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 64)       256         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 64)       256         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 64)       256         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 20, 64)       256         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 20, 64)       256         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 20, 64)       0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 20, 64)       0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 64)       0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 64)       0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 64)       0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 64)       0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 64)       0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 20, 64)       0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 20, 64)       0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d (LocallyCon (None, 20, 64)       83200       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_2 (LocallyC (None, 20, 64)       83200       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_4 (LocallyC (None, 20, 64)       83200       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_6 (LocallyC (None, 20, 64)       83200       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_8 (LocallyC (None, 20, 64)       83200       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_10 (Locally (None, 20, 64)       83200       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_12 (Locally (None, 20, 64)       83200       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_14 (Locally (None, 20, 64)       83200       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_16 (Locally (None, 20, 64)       83200       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_18 (Locally (None, 20, 64)       83200       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 20, 64)       256         locally_connected1d[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 64)       256         locally_connected1d_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 64)       256         locally_connected1d_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 64)       256         locally_connected1d_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 64)       256         locally_connected1d_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 64)       256         locally_connected1d_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 64)       256         locally_connected1d_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 20, 64)       256         locally_connected1d_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 20, 64)       256         locally_connected1d_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 20, 64)       256         locally_connected1d_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 20, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 64)       0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 64)       0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 64)       0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 64)       0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 64)       0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 64)       0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 20, 64)       0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 20, 64)       0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 20, 64)       0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_1 (LocallyC (None, 20, 64)       83200       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_3 (LocallyC (None, 20, 64)       83200       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_5 (LocallyC (None, 20, 64)       83200       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_7 (LocallyC (None, 20, 64)       83200       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_9 (LocallyC (None, 20, 64)       83200       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_11 (Locally (None, 20, 64)       83200       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_13 (Locally (None, 20, 64)       83200       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_15 (Locally (None, 20, 64)       83200       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_17 (Locally (None, 20, 64)       83200       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "locally_connected1d_19 (Locally (None, 20, 64)       83200       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 20, 64)       256         locally_connected1d_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 64)       256         locally_connected1d_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 64)       256         locally_connected1d_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 64)       256         locally_connected1d_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 64)       256         locally_connected1d_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 64)       256         locally_connected1d_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 20, 64)       256         locally_connected1d_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 20, 64)       256         locally_connected1d_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 20, 64)       256         locally_connected1d_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 20, 64)       256         locally_connected1d_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 20, 64)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 64)       0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 64)       0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 64)       0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 64)       0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 20, 64)       0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 20, 64)       0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 20, 64)       0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 20, 64)       0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 20, 64)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 20, 64)       0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 20, 64)       0           activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20, 64)       0           activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20, 64)       0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 20, 64)       0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 20, 64)       0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 20, 64)       0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 20, 64)       0           activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 20, 64)       0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 20, 10, 64)] 0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "                                                                 dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "                                                                 dropout_5[0][0]                  \n",
      "                                                                 dropout_6[0][0]                  \n",
      "                                                                 dropout_7[0][0]                  \n",
      "                                                                 dropout_8[0][0]                  \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 12800)        0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          6554112     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 512)          2048        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 512)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 512)          0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 512)          0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 512)          0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 128)          65664       dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 52)           6708        activation_42[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 8,694,068\n",
      "Trainable params: 8,686,644\n",
      "Non-trainable params: 7,424\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 4.0828 - accuracy: 0.1346\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.23750, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 3s 345ms/step - loss: 4.0747 - accuracy: 0.1355 - val_loss: 3.3163 - val_accuracy: 0.2375 - lr: 0.0010\n",
      "Epoch 2/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 3.0639 - accuracy: 0.2398\n",
      "Epoch 00002: val_accuracy improved from 0.23750 to 0.30546, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 3.0617 - accuracy: 0.2400 - val_loss: 2.7581 - val_accuracy: 0.3055 - lr: 0.0010\n",
      "Epoch 3/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.7192 - accuracy: 0.3068\n",
      "Epoch 00003: val_accuracy improved from 0.30546 to 0.37728, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 2.7186 - accuracy: 0.3072 - val_loss: 2.4506 - val_accuracy: 0.3773 - lr: 0.0010\n",
      "Epoch 4/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.4806 - accuracy: 0.3533\n",
      "Epoch 00004: val_accuracy improved from 0.37728 to 0.43319, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 2.4787 - accuracy: 0.3536 - val_loss: 2.2587 - val_accuracy: 0.4332 - lr: 0.0010\n",
      "Epoch 5/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.2790 - accuracy: 0.4011\n",
      "Epoch 00005: val_accuracy improved from 0.43319 to 0.47653, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 2.2793 - accuracy: 0.4011 - val_loss: 2.1004 - val_accuracy: 0.4765 - lr: 0.0010\n",
      "Epoch 6/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.1283 - accuracy: 0.4447\n",
      "Epoch 00006: val_accuracy improved from 0.47653 to 0.51423, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 2.1291 - accuracy: 0.4441 - val_loss: 1.9577 - val_accuracy: 0.5142 - lr: 0.0010\n",
      "Epoch 7/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 2.0143 - accuracy: 0.4681\n",
      "Epoch 00007: val_accuracy improved from 0.51423 to 0.53834, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 2.0148 - accuracy: 0.4675 - val_loss: 1.8436 - val_accuracy: 0.5383 - lr: 0.0010\n",
      "Epoch 8/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.9107 - accuracy: 0.4991\n",
      "Epoch 00008: val_accuracy improved from 0.53834 to 0.54886, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.9117 - accuracy: 0.4994 - val_loss: 1.7660 - val_accuracy: 0.5489 - lr: 0.0010\n",
      "Epoch 9/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8823 - accuracy: 0.5067\n",
      "Epoch 00009: val_accuracy improved from 0.54886 to 0.56117, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1.8815 - accuracy: 0.5071 - val_loss: 1.7500 - val_accuracy: 0.5612 - lr: 0.0010\n",
      "Epoch 10/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.8055 - accuracy: 0.5300\n",
      "Epoch 00010: val_accuracy improved from 0.56117 to 0.57425, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.8051 - accuracy: 0.5299 - val_loss: 1.6877 - val_accuracy: 0.5742 - lr: 0.0010\n",
      "Epoch 11/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7439 - accuracy: 0.5428\n",
      "Epoch 00011: val_accuracy did not improve from 0.57425\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.7430 - accuracy: 0.5430 - val_loss: 1.6384 - val_accuracy: 0.5737 - lr: 0.0010\n",
      "Epoch 12/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.7022 - accuracy: 0.5516\n",
      "Epoch 00012: val_accuracy improved from 0.57425 to 0.59169, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 1.7045 - accuracy: 0.5507 - val_loss: 1.6069 - val_accuracy: 0.5917 - lr: 0.0010\n",
      "Epoch 13/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.6334 - accuracy: 0.5722\n",
      "Epoch 00013: val_accuracy improved from 0.59169 to 0.60015, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.6359 - accuracy: 0.5716 - val_loss: 1.5614 - val_accuracy: 0.6002 - lr: 0.0010\n",
      "Epoch 14/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5974 - accuracy: 0.5877\n",
      "Epoch 00014: val_accuracy improved from 0.60015 to 0.60990, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.5991 - accuracy: 0.5876 - val_loss: 1.5407 - val_accuracy: 0.6099 - lr: 0.0010\n",
      "Epoch 15/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5637 - accuracy: 0.5920\n",
      "Epoch 00015: val_accuracy improved from 0.60990 to 0.62349, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.5665 - accuracy: 0.5914 - val_loss: 1.5171 - val_accuracy: 0.6235 - lr: 0.0010\n",
      "Epoch 16/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5329 - accuracy: 0.6000\n",
      "Epoch 00016: val_accuracy did not improve from 0.62349\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 1.5335 - accuracy: 0.6000 - val_loss: 1.4989 - val_accuracy: 0.6166 - lr: 0.0010\n",
      "Epoch 17/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.5224 - accuracy: 0.5998\n",
      "Epoch 00017: val_accuracy did not improve from 0.62349\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.5229 - accuracy: 0.5997 - val_loss: 1.4714 - val_accuracy: 0.6204 - lr: 0.0010\n",
      "Epoch 18/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4833 - accuracy: 0.6129\n",
      "Epoch 00018: val_accuracy improved from 0.62349 to 0.62657, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.4812 - accuracy: 0.6134 - val_loss: 1.4645 - val_accuracy: 0.6266 - lr: 0.0010\n",
      "Epoch 19/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4662 - accuracy: 0.6174\n",
      "Epoch 00019: val_accuracy did not improve from 0.62657\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.4678 - accuracy: 0.6174 - val_loss: 1.4745 - val_accuracy: 0.6248 - lr: 0.0010\n",
      "Epoch 20/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4241 - accuracy: 0.6304\n",
      "Epoch 00020: val_accuracy improved from 0.62657 to 0.63811, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.4272 - accuracy: 0.6301 - val_loss: 1.4292 - val_accuracy: 0.6381 - lr: 0.0010\n",
      "Epoch 21/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.4152 - accuracy: 0.6299\n",
      "Epoch 00021: val_accuracy did not improve from 0.63811\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.4160 - accuracy: 0.6296 - val_loss: 1.4362 - val_accuracy: 0.6279 - lr: 0.0010\n",
      "Epoch 22/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3947 - accuracy: 0.6377\n",
      "Epoch 00022: val_accuracy improved from 0.63811 to 0.64401, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.3952 - accuracy: 0.6379 - val_loss: 1.4101 - val_accuracy: 0.6440 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3565 - accuracy: 0.6476\n",
      "Epoch 00023: val_accuracy did not improve from 0.64401\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.3577 - accuracy: 0.6473 - val_loss: 1.4120 - val_accuracy: 0.6414 - lr: 0.0010\n",
      "Epoch 24/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3631 - accuracy: 0.6448\n",
      "Epoch 00024: val_accuracy improved from 0.64401 to 0.64837, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.3649 - accuracy: 0.6449 - val_loss: 1.3893 - val_accuracy: 0.6484 - lr: 0.0010\n",
      "Epoch 25/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3203 - accuracy: 0.6589\n",
      "Epoch 00025: val_accuracy did not improve from 0.64837\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.3205 - accuracy: 0.6582 - val_loss: 1.4913 - val_accuracy: 0.6325 - lr: 0.0010\n",
      "Epoch 26/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3247 - accuracy: 0.6544\n",
      "Epoch 00026: val_accuracy did not improve from 0.64837\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.3281 - accuracy: 0.6533 - val_loss: 1.3930 - val_accuracy: 0.6458 - lr: 0.0010\n",
      "Epoch 27/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.3141 - accuracy: 0.6638\n",
      "Epoch 00027: val_accuracy improved from 0.64837 to 0.65632, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.3133 - accuracy: 0.6639 - val_loss: 1.3837 - val_accuracy: 0.6563 - lr: 0.0010\n",
      "Epoch 28/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2788 - accuracy: 0.6741\n",
      "Epoch 00028: val_accuracy improved from 0.65632 to 0.66350, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.2812 - accuracy: 0.6736 - val_loss: 1.3541 - val_accuracy: 0.6635 - lr: 0.0010\n",
      "Epoch 29/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2481 - accuracy: 0.6769\n",
      "Epoch 00029: val_accuracy improved from 0.66350 to 0.67068, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.2523 - accuracy: 0.6760 - val_loss: 1.3296 - val_accuracy: 0.6707 - lr: 0.0010\n",
      "Epoch 30/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2612 - accuracy: 0.6773\n",
      "Epoch 00030: val_accuracy did not improve from 0.67068\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.2618 - accuracy: 0.6768 - val_loss: 1.3451 - val_accuracy: 0.6679 - lr: 0.0010\n",
      "Epoch 31/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2503 - accuracy: 0.6707\n",
      "Epoch 00031: val_accuracy improved from 0.67068 to 0.67197, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1.2508 - accuracy: 0.6702 - val_loss: 1.3352 - val_accuracy: 0.6720 - lr: 0.0010\n",
      "Epoch 32/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2295 - accuracy: 0.6848\n",
      "Epoch 00032: val_accuracy did not improve from 0.67197\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.2294 - accuracy: 0.6845 - val_loss: 1.3426 - val_accuracy: 0.6674 - lr: 0.0010\n",
      "Epoch 33/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2023 - accuracy: 0.6900\n",
      "Epoch 00033: val_accuracy did not improve from 0.67197\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.2027 - accuracy: 0.6895 - val_loss: 1.3080 - val_accuracy: 0.6720 - lr: 0.0010\n",
      "Epoch 34/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.2098 - accuracy: 0.6858\n",
      "Epoch 00034: val_accuracy did not improve from 0.67197\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.2099 - accuracy: 0.6857 - val_loss: 1.3306 - val_accuracy: 0.6653 - lr: 0.0010\n",
      "Epoch 35/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1876 - accuracy: 0.6981\n",
      "Epoch 00035: val_accuracy did not improve from 0.67197\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.1899 - accuracy: 0.6968 - val_loss: 1.3317 - val_accuracy: 0.6615 - lr: 0.0010\n",
      "Epoch 36/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1755 - accuracy: 0.6961\n",
      "Epoch 00036: val_accuracy improved from 0.67197 to 0.67325, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.1758 - accuracy: 0.6966 - val_loss: 1.2987 - val_accuracy: 0.6732 - lr: 0.0010\n",
      "Epoch 37/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1791 - accuracy: 0.7004\n",
      "Epoch 00037: val_accuracy did not improve from 0.67325\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.1811 - accuracy: 0.7001 - val_loss: 1.3229 - val_accuracy: 0.6694 - lr: 0.0010\n",
      "Epoch 38/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1719 - accuracy: 0.6953\n",
      "Epoch 00038: val_accuracy improved from 0.67325 to 0.68120, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1.1716 - accuracy: 0.6952 - val_loss: 1.2935 - val_accuracy: 0.6812 - lr: 0.0010\n",
      "Epoch 39/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1462 - accuracy: 0.7054\n",
      "Epoch 00039: val_accuracy did not improve from 0.68120\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.1454 - accuracy: 0.7058 - val_loss: 1.2904 - val_accuracy: 0.6750 - lr: 0.0010\n",
      "Epoch 40/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.1030 - accuracy: 0.7180\n",
      "Epoch 00040: val_accuracy improved from 0.68120 to 0.69095, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.1043 - accuracy: 0.7179 - val_loss: 1.2652 - val_accuracy: 0.6909 - lr: 1.0000e-04\n",
      "Epoch 41/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0807 - accuracy: 0.7192\n",
      "Epoch 00041: val_accuracy did not improve from 0.69095\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 1.0818 - accuracy: 0.7192 - val_loss: 1.2630 - val_accuracy: 0.6889 - lr: 1.0000e-04\n",
      "Epoch 42/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0668 - accuracy: 0.7224\n",
      "Epoch 00042: val_accuracy did not improve from 0.69095\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 1.0690 - accuracy: 0.7223 - val_loss: 1.2605 - val_accuracy: 0.6871 - lr: 1.0000e-04\n",
      "Epoch 43/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0672 - accuracy: 0.7288\n",
      "Epoch 00043: val_accuracy improved from 0.69095 to 0.69172, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1.0711 - accuracy: 0.7278 - val_loss: 1.2523 - val_accuracy: 0.6917 - lr: 1.0000e-04\n",
      "Epoch 44/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0504 - accuracy: 0.7284\n",
      "Epoch 00044: val_accuracy improved from 0.69172 to 0.69300, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.0525 - accuracy: 0.7285 - val_loss: 1.2464 - val_accuracy: 0.6930 - lr: 1.0000e-04\n",
      "Epoch 45/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0560 - accuracy: 0.7237\n",
      "Epoch 00045: val_accuracy did not improve from 0.69300\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.0561 - accuracy: 0.7237 - val_loss: 1.2520 - val_accuracy: 0.6894 - lr: 1.0000e-04\n",
      "Epoch 46/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0525 - accuracy: 0.7296\n",
      "Epoch 00046: val_accuracy improved from 0.69300 to 0.69505, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1.0525 - accuracy: 0.7298 - val_loss: 1.2438 - val_accuracy: 0.6951 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/120\n",
      "10/10 [==============================] - ETA: 0s - loss: 1.0482 - accuracy: 0.7285\n",
      "Epoch 00047: val_accuracy did not improve from 0.69505\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 1.0482 - accuracy: 0.7285 - val_loss: 1.2785 - val_accuracy: 0.6848 - lr: 1.0000e-04\n",
      "Epoch 48/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0415 - accuracy: 0.7292\n",
      "Epoch 00048: val_accuracy did not improve from 0.69505\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.0463 - accuracy: 0.7278 - val_loss: 1.2916 - val_accuracy: 0.6809 - lr: 1.0000e-04\n",
      "Epoch 49/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0222 - accuracy: 0.7442\n",
      "Epoch 00049: val_accuracy did not improve from 0.69505\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 1.0257 - accuracy: 0.7432 - val_loss: 1.2609 - val_accuracy: 0.6879 - lr: 1.0000e-04\n",
      "Epoch 50/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0291 - accuracy: 0.7302\n",
      "Epoch 00050: val_accuracy improved from 0.69505 to 0.69967, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1.0284 - accuracy: 0.7310 - val_loss: 1.2368 - val_accuracy: 0.6997 - lr: 1.0000e-04\n",
      "Epoch 51/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0190 - accuracy: 0.7446\n",
      "Epoch 00051: val_accuracy improved from 0.69967 to 0.70018, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1.0184 - accuracy: 0.7446 - val_loss: 1.2319 - val_accuracy: 0.7002 - lr: 1.0000e-04\n",
      "Epoch 52/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0271 - accuracy: 0.7328\n",
      "Epoch 00052: val_accuracy did not improve from 0.70018\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 1.0291 - accuracy: 0.7325 - val_loss: 1.2272 - val_accuracy: 0.6984 - lr: 1.0000e-04\n",
      "Epoch 53/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0161 - accuracy: 0.7401\n",
      "Epoch 00053: val_accuracy did not improve from 0.70018\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 1.0177 - accuracy: 0.7394 - val_loss: 1.2283 - val_accuracy: 0.6968 - lr: 1.0000e-04\n",
      "Epoch 54/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0105 - accuracy: 0.7372\n",
      "Epoch 00054: val_accuracy improved from 0.70018 to 0.70044, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1.0130 - accuracy: 0.7369 - val_loss: 1.2280 - val_accuracy: 0.7004 - lr: 1.0000e-04\n",
      "Epoch 55/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0138 - accuracy: 0.7424\n",
      "Epoch 00055: val_accuracy did not improve from 0.70044\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.0164 - accuracy: 0.7417 - val_loss: 1.2302 - val_accuracy: 0.6979 - lr: 1.0000e-04\n",
      "Epoch 56/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0156 - accuracy: 0.7407\n",
      "Epoch 00056: val_accuracy improved from 0.70044 to 0.70069, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 1.0158 - accuracy: 0.7404 - val_loss: 1.2306 - val_accuracy: 0.7007 - lr: 1.0000e-04\n",
      "Epoch 57/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0060 - accuracy: 0.7408\n",
      "Epoch 00057: val_accuracy did not improve from 0.70069\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 1.0065 - accuracy: 0.7404 - val_loss: 1.2291 - val_accuracy: 0.6963 - lr: 1.0000e-04\n",
      "Epoch 58/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0127 - accuracy: 0.7411\n",
      "Epoch 00058: val_accuracy did not improve from 0.70069\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 1.0130 - accuracy: 0.7412 - val_loss: 1.2237 - val_accuracy: 0.6971 - lr: 1.0000e-04\n",
      "Epoch 59/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9972 - accuracy: 0.7476\n",
      "Epoch 00059: val_accuracy did not improve from 0.70069\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9970 - accuracy: 0.7473 - val_loss: 1.2230 - val_accuracy: 0.6989 - lr: 1.0000e-04\n",
      "Epoch 60/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9997 - accuracy: 0.7466\n",
      "Epoch 00060: val_accuracy did not improve from 0.70069\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9991 - accuracy: 0.7469 - val_loss: 1.2254 - val_accuracy: 0.6986 - lr: 1.0000e-04\n",
      "Epoch 61/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 1.0057 - accuracy: 0.7404\n",
      "Epoch 00061: val_accuracy did not improve from 0.70069\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1.0044 - accuracy: 0.7409 - val_loss: 1.2193 - val_accuracy: 0.6979 - lr: 1.0000e-04\n",
      "Epoch 62/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9832 - accuracy: 0.7496\n",
      "Epoch 00062: val_accuracy did not improve from 0.70069\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9835 - accuracy: 0.7499 - val_loss: 1.2161 - val_accuracy: 0.6981 - lr: 1.0000e-04\n",
      "Epoch 63/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9899 - accuracy: 0.7439\n",
      "Epoch 00063: val_accuracy did not improve from 0.70069\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9925 - accuracy: 0.7430 - val_loss: 1.2176 - val_accuracy: 0.6999 - lr: 1.0000e-04\n",
      "Epoch 64/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9775 - accuracy: 0.7530\n",
      "Epoch 00064: val_accuracy did not improve from 0.70069\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9774 - accuracy: 0.7529 - val_loss: 1.2268 - val_accuracy: 0.6912 - lr: 1.0000e-04\n",
      "Epoch 65/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9728 - accuracy: 0.7554\n",
      "Epoch 00065: val_accuracy did not improve from 0.70069\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9761 - accuracy: 0.7546 - val_loss: 1.2174 - val_accuracy: 0.6984 - lr: 1.0000e-04\n",
      "Epoch 66/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9903 - accuracy: 0.7482\n",
      "Epoch 00066: val_accuracy did not improve from 0.70069\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9921 - accuracy: 0.7482 - val_loss: 1.2197 - val_accuracy: 0.6986 - lr: 1.0000e-04\n",
      "Epoch 67/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9792 - accuracy: 0.7492\n",
      "Epoch 00067: val_accuracy improved from 0.70069 to 0.70121, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 0.9790 - accuracy: 0.7490 - val_loss: 1.2167 - val_accuracy: 0.7012 - lr: 1.0000e-04\n",
      "Epoch 68/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9748 - accuracy: 0.7543\n",
      "Epoch 00068: val_accuracy did not improve from 0.70121\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9757 - accuracy: 0.7544 - val_loss: 1.2180 - val_accuracy: 0.6999 - lr: 1.0000e-04\n",
      "Epoch 69/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9822 - accuracy: 0.7492\n",
      "Epoch 00069: val_accuracy did not improve from 0.70121\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9826 - accuracy: 0.7491 - val_loss: 1.2176 - val_accuracy: 0.6984 - lr: 1.0000e-04\n",
      "Epoch 70/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9572 - accuracy: 0.7589\n",
      "Epoch 00070: val_accuracy did not improve from 0.70121\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9613 - accuracy: 0.7580 - val_loss: 1.2159 - val_accuracy: 0.7009 - lr: 1.0000e-04\n",
      "Epoch 71/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9495 - accuracy: 0.7587\n",
      "Epoch 00071: val_accuracy improved from 0.70121 to 0.70197, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 0.9532 - accuracy: 0.7579 - val_loss: 1.2147 - val_accuracy: 0.7020 - lr: 1.0000e-04\n",
      "Epoch 72/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9741 - accuracy: 0.7500\n",
      "Epoch 00072: val_accuracy did not improve from 0.70197\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9753 - accuracy: 0.7496 - val_loss: 1.2268 - val_accuracy: 0.6953 - lr: 1.0000e-04\n",
      "Epoch 73/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9750 - accuracy: 0.7479\n",
      "Epoch 00073: val_accuracy improved from 0.70197 to 0.70223, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 0.9773 - accuracy: 0.7470 - val_loss: 1.2214 - val_accuracy: 0.7022 - lr: 1.0000e-04\n",
      "Epoch 74/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9678 - accuracy: 0.7551\n",
      "Epoch 00074: val_accuracy did not improve from 0.70223\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9698 - accuracy: 0.7548 - val_loss: 1.2326 - val_accuracy: 0.6948 - lr: 1.0000e-04\n",
      "Epoch 75/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9601 - accuracy: 0.7497\n",
      "Epoch 00075: val_accuracy did not improve from 0.70223\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9617 - accuracy: 0.7492 - val_loss: 1.2161 - val_accuracy: 0.7022 - lr: 1.0000e-04\n",
      "Epoch 76/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9448 - accuracy: 0.7619\n",
      "Epoch 00076: val_accuracy improved from 0.70223 to 0.70557, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 0.9443 - accuracy: 0.7624 - val_loss: 1.2094 - val_accuracy: 0.7056 - lr: 1.0000e-04\n",
      "Epoch 77/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9539 - accuracy: 0.7571\n",
      "Epoch 00077: val_accuracy improved from 0.70557 to 0.70813, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 0.9547 - accuracy: 0.7568 - val_loss: 1.2090 - val_accuracy: 0.7081 - lr: 1.0000e-04\n",
      "Epoch 78/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9550 - accuracy: 0.7547\n",
      "Epoch 00078: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9588 - accuracy: 0.7537 - val_loss: 1.2130 - val_accuracy: 0.7033 - lr: 1.0000e-04\n",
      "Epoch 79/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9659 - accuracy: 0.7537\n",
      "Epoch 00079: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.9665 - accuracy: 0.7539 - val_loss: 1.2074 - val_accuracy: 0.7058 - lr: 1.0000e-04\n",
      "Epoch 80/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9445 - accuracy: 0.7611\n",
      "Epoch 00080: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9478 - accuracy: 0.7604 - val_loss: 1.2100 - val_accuracy: 0.7040 - lr: 1.0000e-05\n",
      "Epoch 81/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9534 - accuracy: 0.7592\n",
      "Epoch 00081: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9534 - accuracy: 0.7588 - val_loss: 1.2158 - val_accuracy: 0.7025 - lr: 1.0000e-05\n",
      "Epoch 82/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9581 - accuracy: 0.7527\n",
      "Epoch 00082: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9581 - accuracy: 0.7529 - val_loss: 1.2105 - val_accuracy: 0.7007 - lr: 1.0000e-05\n",
      "Epoch 83/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9546 - accuracy: 0.7524\n",
      "Epoch 00083: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.9544 - accuracy: 0.7521 - val_loss: 1.2052 - val_accuracy: 0.7040 - lr: 1.0000e-05\n",
      "Epoch 84/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9604 - accuracy: 0.7544\n",
      "Epoch 00084: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.9616 - accuracy: 0.7539 - val_loss: 1.2061 - val_accuracy: 0.7063 - lr: 1.0000e-05\n",
      "Epoch 85/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9463 - accuracy: 0.7613\n",
      "Epoch 00085: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.9452 - accuracy: 0.7620 - val_loss: 1.2029 - val_accuracy: 0.7068 - lr: 1.0000e-05\n",
      "Epoch 86/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9469 - accuracy: 0.7603\n",
      "Epoch 00086: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.9478 - accuracy: 0.7601 - val_loss: 1.2061 - val_accuracy: 0.7027 - lr: 1.0000e-05\n",
      "Epoch 87/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9487 - accuracy: 0.7592\n",
      "Epoch 00087: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9503 - accuracy: 0.7587 - val_loss: 1.2078 - val_accuracy: 0.7048 - lr: 1.0000e-05\n",
      "Epoch 88/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9526 - accuracy: 0.7546\n",
      "Epoch 00088: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.9533 - accuracy: 0.7541 - val_loss: 1.2093 - val_accuracy: 0.7038 - lr: 1.0000e-05\n",
      "Epoch 89/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9483 - accuracy: 0.7553\n",
      "Epoch 00089: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9473 - accuracy: 0.7557 - val_loss: 1.2083 - val_accuracy: 0.7030 - lr: 1.0000e-05\n",
      "Epoch 90/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9406 - accuracy: 0.7602\n",
      "Epoch 00090: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9400 - accuracy: 0.7602 - val_loss: 1.2074 - val_accuracy: 0.7045 - lr: 1.0000e-05\n",
      "Epoch 91/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9382 - accuracy: 0.7610\n",
      "Epoch 00091: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.9411 - accuracy: 0.7600 - val_loss: 1.2029 - val_accuracy: 0.7038 - lr: 1.0000e-05\n",
      "Epoch 92/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9555 - accuracy: 0.7563\n",
      "Epoch 00092: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9570 - accuracy: 0.7559 - val_loss: 1.2056 - val_accuracy: 0.7051 - lr: 1.0000e-05\n",
      "Epoch 93/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9476 - accuracy: 0.7541\n",
      "Epoch 00093: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9486 - accuracy: 0.7537 - val_loss: 1.2109 - val_accuracy: 0.7022 - lr: 1.0000e-05\n",
      "Epoch 94/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9445 - accuracy: 0.7564\n",
      "Epoch 00094: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9461 - accuracy: 0.7558 - val_loss: 1.2179 - val_accuracy: 0.6989 - lr: 1.0000e-05\n",
      "Epoch 95/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9355 - accuracy: 0.7618\n",
      "Epoch 00095: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9375 - accuracy: 0.7611 - val_loss: 1.2084 - val_accuracy: 0.7025 - lr: 1.0000e-05\n",
      "Epoch 96/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9426 - accuracy: 0.7571\n",
      "Epoch 00096: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9469 - accuracy: 0.7565 - val_loss: 1.2285 - val_accuracy: 0.6935 - lr: 1.0000e-05\n",
      "Epoch 97/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9390 - accuracy: 0.7608\n",
      "Epoch 00097: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9398 - accuracy: 0.7605 - val_loss: 1.2038 - val_accuracy: 0.7040 - lr: 1.0000e-05\n",
      "Epoch 98/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9402 - accuracy: 0.7597\n",
      "Epoch 00098: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9416 - accuracy: 0.7598 - val_loss: 1.2467 - val_accuracy: 0.6886 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9369 - accuracy: 0.7623\n",
      "Epoch 00099: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9383 - accuracy: 0.7621 - val_loss: 1.2189 - val_accuracy: 0.6976 - lr: 1.0000e-05\n",
      "Epoch 100/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9322 - accuracy: 0.7616\n",
      "Epoch 00100: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9315 - accuracy: 0.7617 - val_loss: 1.2098 - val_accuracy: 0.7009 - lr: 1.0000e-05\n",
      "Epoch 101/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9322 - accuracy: 0.7629\n",
      "Epoch 00101: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 67ms/step - loss: 0.9348 - accuracy: 0.7622 - val_loss: 1.2048 - val_accuracy: 0.7071 - lr: 1.0000e-05\n",
      "Epoch 102/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9419 - accuracy: 0.7584\n",
      "Epoch 00102: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9433 - accuracy: 0.7583 - val_loss: 1.2009 - val_accuracy: 0.7056 - lr: 1.0000e-05\n",
      "Epoch 103/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9465 - accuracy: 0.7573\n",
      "Epoch 00103: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 0.9464 - accuracy: 0.7572 - val_loss: 1.1995 - val_accuracy: 0.7066 - lr: 1.0000e-05\n",
      "Epoch 104/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9507 - accuracy: 0.7571\n",
      "Epoch 00104: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9534 - accuracy: 0.7558 - val_loss: 1.2015 - val_accuracy: 0.7053 - lr: 1.0000e-05\n",
      "Epoch 105/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9496 - accuracy: 0.7590\n",
      "Epoch 00105: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9490 - accuracy: 0.7590 - val_loss: 1.1994 - val_accuracy: 0.7061 - lr: 1.0000e-05\n",
      "Epoch 106/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9423 - accuracy: 0.7568\n",
      "Epoch 00106: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9434 - accuracy: 0.7570 - val_loss: 1.2012 - val_accuracy: 0.7053 - lr: 1.0000e-05\n",
      "Epoch 107/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9310 - accuracy: 0.7606\n",
      "Epoch 00107: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 63ms/step - loss: 0.9321 - accuracy: 0.7602 - val_loss: 1.2000 - val_accuracy: 0.7071 - lr: 1.0000e-05\n",
      "Epoch 108/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9408 - accuracy: 0.7598\n",
      "Epoch 00108: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9434 - accuracy: 0.7588 - val_loss: 1.2049 - val_accuracy: 0.7058 - lr: 1.0000e-05\n",
      "Epoch 109/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9421 - accuracy: 0.7684\n",
      "Epoch 00109: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9451 - accuracy: 0.7673 - val_loss: 1.2035 - val_accuracy: 0.7068 - lr: 1.0000e-05\n",
      "Epoch 110/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9405 - accuracy: 0.7617\n",
      "Epoch 00110: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9450 - accuracy: 0.7604 - val_loss: 1.3029 - val_accuracy: 0.6732 - lr: 1.0000e-05\n",
      "Epoch 111/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9401 - accuracy: 0.7624\n",
      "Epoch 00111: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9407 - accuracy: 0.7622 - val_loss: 1.2224 - val_accuracy: 0.6966 - lr: 1.0000e-05\n",
      "Epoch 112/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9358 - accuracy: 0.7660\n",
      "Epoch 00112: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9362 - accuracy: 0.7659 - val_loss: 1.2069 - val_accuracy: 0.7009 - lr: 1.0000e-05\n",
      "Epoch 113/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9379 - accuracy: 0.7610\n",
      "Epoch 00113: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9388 - accuracy: 0.7601 - val_loss: 1.2013 - val_accuracy: 0.7043 - lr: 1.0000e-05\n",
      "Epoch 114/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9337 - accuracy: 0.7597\n",
      "Epoch 00114: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 64ms/step - loss: 0.9359 - accuracy: 0.7590 - val_loss: 1.2003 - val_accuracy: 0.7066 - lr: 1.0000e-05\n",
      "Epoch 115/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9332 - accuracy: 0.7578\n",
      "Epoch 00115: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9363 - accuracy: 0.7567 - val_loss: 1.2013 - val_accuracy: 0.7053 - lr: 1.0000e-05\n",
      "Epoch 116/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9441 - accuracy: 0.7602\n",
      "Epoch 00116: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.9455 - accuracy: 0.7595 - val_loss: 1.1991 - val_accuracy: 0.7068 - lr: 1.0000e-05\n",
      "Epoch 117/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9443 - accuracy: 0.7588\n",
      "Epoch 00117: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 66ms/step - loss: 0.9443 - accuracy: 0.7591 - val_loss: 1.2001 - val_accuracy: 0.7081 - lr: 1.0000e-05\n",
      "Epoch 118/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9302 - accuracy: 0.7588\n",
      "Epoch 00118: val_accuracy did not improve from 0.70813\n",
      "10/10 [==============================] - 1s 65ms/step - loss: 0.9298 - accuracy: 0.7589 - val_loss: 1.2019 - val_accuracy: 0.7066 - lr: 1.0000e-05\n",
      "Epoch 119/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9428 - accuracy: 0.7571\n",
      "Epoch 00119: val_accuracy improved from 0.70813 to 0.70916, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 0.9464 - accuracy: 0.7568 - val_loss: 1.1983 - val_accuracy: 0.7092 - lr: 1.0000e-05\n",
      "Epoch 120/120\n",
      " 9/10 [==========================>...] - ETA: 0s - loss: 0.9520 - accuracy: 0.7559\n",
      "Epoch 00120: val_accuracy improved from 0.70916 to 0.70941, saving model to /media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 0.9545 - accuracy: 0.7552 - val_loss: 1.1981 - val_accuracy: 0.7094 - lr: 1.0000e-06\n",
      "epoch_number 120\n",
      "train accuracy and validation accuracy 0.7552478313446045 0.709412693977356\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.1981 - accuracy: 0.7094\n",
      "test_accuracy 0.709412693977356\n",
      "[0.7863554954528809, 0.6014362573623657, 0.7224929332733154, 0.7276224493980408, 0.7583996057510376, 0.8317517042160034, 0.8776609301567078, 0.686073362827301, 0.7327519655227661, 0.8212361931800842, 0.7668632864952087, 0.7748140692710876, 0.7055655121803284, 0.7168504595756531, 0.6958194375038147, 0.7476276159286499, 0.7235188484191895, 0.7981534004211426, 0.8017440438270569, 0.7732751965522766, 0.7940497398376465, 0.7437804341316223, 0.7063349485397339, 0.7383944392204285, 0.6742754578590393, 0.6332392692565918, 0.709412693977356]\n",
      "0.7425740648199011\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,28):\n",
    "    file_path_train=path+'S'+str(i)+'_'+'tr'+'.'+'csv'\n",
    "    print(file_path_train)\n",
    "    df_Train=read_data_Train(file_path_train)\n",
    "    show_basic_dataframe_info(df_Train)\n",
    "    df_Train.head(5)\n",
    "    file_path_test=path+'S'+str(i)+'_'+'tt'+'.'+'csv'\n",
    "    print(file_path_test)\n",
    "    df_Test=read_data_Test(file_path_test)\n",
    "    show_basic_dataframe_info(df_Test)\n",
    "    df_Test.head(5)\n",
    "#     feature standardization\n",
    "    scaler = preprocessing.StandardScaler().fit(df_Train.iloc[:,0:N_FEATURES])\n",
    "    df_Train.iloc[:,0:N_FEATURES]=scaler.transform(df_Train.iloc[:,0:N_FEATURES])\n",
    "    df_Test.iloc[:,0:N_FEATURES]=scaler.transform(df_Test.iloc[:,0:N_FEATURES])\n",
    "    \n",
    "    LABEL = 'ActivityEncoded'\n",
    "    # Transform the labels from String to Integer via LabelEncoder\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    # Add a new column to the existing DataFrame with the encoded values\n",
    "    df_Train[LABEL] = le.fit_transform(df_Train['Class_label'].values.ravel())\n",
    "    # df_Valid[LABEL] = le.fit_transform(df_Valid['Class_label'].values.ravel())\n",
    "    df_Test[LABEL] = le.fit_transform(df_Test['Class_label'].values.ravel())\n",
    "    x_train, y_train = create_segments_and_labels(df_Train,TIME_PERIODS,STEP_DISTANCE,N_FEATURES,LABEL)\n",
    "    print('x_train shape: ', x_train.shape)\n",
    "    # print(x_train)\n",
    "    print(x_train.shape[0], 'training samples')\n",
    "    print('y_train shape: ', y_train.shape)\n",
    "    # Set input & output dimensions\n",
    "    num_time_periods, num_sensors = x_train.shape[1], x_train.shape[2]\n",
    "    print('num_time_periods',num_time_periods)\n",
    "    print('num_sensors',num_sensors)\n",
    "    num_classes = le.classes_.size\n",
    "    print('class_list',list(le.classes_))\n",
    "    # input_shape = (num_time_periods,num_sensors)\n",
    "    # print(input_shape)\n",
    "    input_shape = (num_time_periods,num_sensors)\n",
    "    #x_train = x_train.reshape(x_train.shape[0], input_shape)\n",
    "    print('x_train shape:', x_train[0].shape)\n",
    "    print('input_shape:', input_shape)\n",
    "    x_train = x_train.astype('float32')\n",
    "    # x_train = [torch.tensor(arr, dtype=torch.float32) for arr in x_train]\n",
    "    # y_train = y_train.astype('float32')\n",
    "    # print(y_train)\n",
    "    y_train_hot = np_utils.to_categorical(y_train, num_classes)\n",
    "    print(y_train_hot)\n",
    "    # y_train_hot= [torch.tensor(arr, dtype=torch.uint8) for arr in y_train_hot]\n",
    "    print('New y_train shape: ', y_train_hot.shape)\n",
    "    x_test, y_test = create_segments_and_labels(df_Test,TIME_PERIODS,STEP_DISTANCE,N_FEATURES,LABEL)\n",
    "    print('x_test shape: ', x_test.shape)\n",
    "    # print(x_train)\n",
    "    print(x_test.shape[0], 'testing samples')\n",
    "    print('y_test shape: ', y_test.shape)\n",
    "    # Set input_shape / reshape for Keras\n",
    "    #x_test = x_test.reshape(x_test.shape[0], input_shape)\n",
    "    x_test = x_test.astype('float32')\n",
    "    y_test = y_test.astype('float32')\n",
    "    y_test_hot = np_utils.to_categorical(y_test, num_classes)\n",
    "    # n_steps, n_length = 20, 25\n",
    "    # n_steps, n_length= 10, 50\n",
    "    # n_steps, n_length= 16, 32\n",
    "    n_length =  20\n",
    "    n_depth=10\n",
    "    n_channel=10\n",
    "    x_train = x_train.reshape(x_train.shape[0], n_length,n_depth)\n",
    "    print('x_train shape: ', x_train.shape)\n",
    "    # x_valid = x_valid.reshape(x_valid.shape[0], n_steps, n_length, n_depth)\n",
    "    # print('x_valid shape: ', x_valid.shape)\n",
    "    x_test = x_test.reshape(x_test.shape[0],  n_length,n_depth)\n",
    "    print('x_test shape: ', x_test.shape)\n",
    "    n_outputs = y_train_hot.shape[1]\n",
    "    print('n_outputs',n_outputs)\n",
    "    adam=optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "#     sgd=optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=False, name='SGD')\n",
    "    checkpoint_filepath = '/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/checkpoint.hdf5'\n",
    "#     model.load_weights(checkpoint_filepath) \n",
    "    checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath,verbose=1, monitor='val_accuracy',save_weights_only=True,save_best_only=True)\n",
    "    early = EarlyStopping(monitor='val_loss', min_delta=0, patience=100, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n",
    "    model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "#     tf.keras.utils.plot_model(model, to_file='/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/olsson/CNN20X10/Model1.png',show_shapes=True,show_layer_names=True,dpi=96)\n",
    "    csv_logger = CSVLogger('/media/naveen/nav/mat_codes/nina_DB1_codes/nina_prep_python/wei_sub_20X10/LSTM_nina_20X100.csv', append=True, separator=';')\n",
    "    history = model.fit(x_train, y_train_hot, epochs=epochs, batch_size=batch_size, callbacks=[csv_logger,checkpoint_callback,lrate,early],validation_data=(x_test, y_test_hot), verbose=1)\n",
    "    best_index = history.history['val_accuracy'].index(max(history.history['val_accuracy']))\n",
    "    print('epoch_number',best_index+1)\n",
    "    print('train accuracy and validation accuracy', history.history['accuracy'][best_index], history.history['val_accuracy'][best_index])\n",
    "    model.load_weights(checkpoint_filepath) \n",
    "    _, testaccuracy = model.evaluate(x_test, y_test_hot, batch_size=batch_size, verbose=1)\n",
    "    print('test_accuracy',testaccuracy)\n",
    "    test_acc.append(testaccuracy)\n",
    "    print(test_acc)\n",
    "    test_mean=statistics.mean(test_acc)\n",
    "    print('test_mean for %d subjects:',i)\n",
    "    print(test_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7425740648199011"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
